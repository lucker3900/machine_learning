{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNo8Ydmz2JpzKH8y95XXGKH"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Ul2zWH6BsJTo"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#题目描述：\n",
        "\n",
        "#假设我们有一个简单的两层全连接神经网络，用于回归任务。网络结构如下：\n",
        "\n",
        "#输入层：2个神经元\n",
        "#隐藏层：3个神经元，使用 ReLU 激活函数\n",
        "#输出层：1个神经元，线性输出"
      ],
      "metadata": {
        "id": "YJz7DCutuIXn"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#relu函数\n",
        "def relu(a):\n",
        "  return np.maximum(0, a)"
      ],
      "metadata": {
        "id": "LRwXgHb7ty83"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#前向传播，求y_pred\n",
        "\n",
        "def forward(W1, W2, X, b1, b2):\n",
        "  Z = np.dot(W1, X.T).mean(axis=1, keepdims=True) + b1\n",
        "  H = relu(Z)\n",
        "\n",
        "  y_pred = np.dot(W2, H) + b2\n",
        "\n",
        "  return Z, H, y_pred"
      ],
      "metadata": {
        "id": "P614PX_0uCWn"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#求损失函数loss的值\n",
        "\n",
        "def compute_loss(y_pred, y_true):\n",
        "  loss = (y_pred.T - y_true)**2 / 2\n",
        "  return loss"
      ],
      "metadata": {
        "id": "AYZukD83vEcI"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#反向传播,梯度计算\n",
        "def backward(y_pred, y_true, W1, H, Z, X):\n",
        "  b2_grad = np.mean((y_pred.T - y_true), axis=0, keepdims=True)\n",
        "  W2_grad = np.outer(b2_grad.flatten(), H.flatten())\n",
        "\n",
        "  print(b2_grad.shape)\n",
        "\n",
        "  b1_grad = np.dot(W2, (y_pred-y_true)) * (Z>0)\n",
        "  W1_grad = np.outer(b1_grad.flatten(), np.mean(X, axis=0, keepdims=True).flatten())\n",
        "\n",
        "  return W1_grad, W2_grad, b1_grad, b2_grad"
      ],
      "metadata": {
        "id": "vMwEhQvivpn5"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_params(W1, W2, b1, b2, W1_grad, W2_grad, b1_grad, b2_grad, learning_rate):\n",
        "  W1 -= learning_rate * W1_grad\n",
        "  W2 -= learning_rate * W2_grad\n",
        "\n",
        "  b1 -= learning_rate * b1_grad\n",
        "  b2 -= learning_rate * b2_grad\n",
        "  return W1, W2, b1, b2"
      ],
      "metadata": {
        "id": "B0nekePvThK5"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 数据\n",
        "X = np.array([[0.5, 0.3],\n",
        "        [0.8, 0.6],\n",
        "        [0.2, 0.9]])\n",
        "\n",
        "y_true = np.array([[0.8], [1.2], [0.5]])\n",
        "\n",
        "# 初始参数\n",
        "W1 = np.array([[0.1, 0.2],\n",
        "        [0.3, 0.4],\n",
        "        [0.5, 0.6]])\n",
        "\n",
        "b1 = np.array([[0.1], [0.2], [0.3]])\n",
        "W2 = np.array([[0.7, 0.8, 0.9]])\n",
        "b2 = np.array([[0.4]])\n",
        "\n",
        "# 学习率\n",
        "learning_rate = 0.1\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  Z, H, y_pred = forward(W1, W2, X, b1, b2)\n",
        "  loss = compute_loss(y_pred, y_true)\n",
        "  W1_grad, W2_grad, b1_grad, b2_grad = backward(y_pred, y_true, W1, H, Z, X)\n",
        "  W1, W2, b1, b2 = update_params(W1, W2, b1, b2, W1_grad, W2_grad, b1_grad, b2_grad, learning_rate)\n",
        "\n",
        "  print(\n",
        "    \"Epoch:\", epoch + 1,\n",
        "    \"loss\", np.round(loss, 4),\n",
        "    \"W1_grad\", np.round(W1_grad.flatten(), 2),\n",
        "    \"W2_grad\", np.round(W2_grad.flatten(), 2),\n",
        "    \"b1_grad\", np.round(b1_grad.flatten(), 2),\n",
        "    \"b2_grad\", np.round(b2_grad.flatten(), 2)\n",
        "  )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wn8WtT9zsPWA",
        "outputId": "2b5ddf7b-4f79-4660-99c8-0910b7e560fc"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 1)\n",
            "Epoch: 1 loss [[0.5832]\n",
            " [0.2312]\n",
            " [0.9522]] W1_grad [1.27 1.53 1.27 1.53 1.27 1.53] W2_grad [0.28 0.62 0.95] b1_grad [2.54 2.54 2.54] b2_grad [1.05]\n",
            "(1, 1)\n",
            "Epoch: 2 loss [[0.0005]\n",
            " [0.0678]\n",
            " [0.055 ]] W1_grad [0.   0.   0.01 0.01 0.01 0.01] W2_grad [-0. -0. -0.] b1_grad [0.   0.02 0.02] b2_grad [-0.]\n",
            "(1, 1)\n",
            "Epoch: 3 loss [[0.0004]\n",
            " [0.0692]\n",
            " [0.0537]] W1_grad [0. 0. 0. 0. 0. 0.] W2_grad [-0. -0. -0.] b1_grad [0.   0.01 0.01] b2_grad [-0.01]\n",
            "(1, 1)\n",
            "Epoch: 4 loss [[0.0004]\n",
            " [0.0697]\n",
            " [0.0533]] W1_grad [0. 0. 0. 0. 0. 0.] W2_grad [-0. -0. -0.] b1_grad [0.   0.01 0.01] b2_grad [-0.01]\n",
            "(1, 1)\n",
            "Epoch: 5 loss [[0.0003]\n",
            " [0.0699]\n",
            " [0.0532]] W1_grad [0. 0. 0. 0. 0. 0.] W2_grad [-0. -0. -0.] b1_grad [0. 0. 0.] b2_grad [-0.01]\n",
            "(1, 1)\n",
            "Epoch: 6 loss [[0.0003]\n",
            " [0.0699]\n",
            " [0.0532]] W1_grad [0. 0. 0. 0. 0. 0.] W2_grad [-0. -0. -0.] b1_grad [0. 0. 0.] b2_grad [-0.01]\n",
            "(1, 1)\n",
            "Epoch: 7 loss [[0.0003]\n",
            " [0.0699]\n",
            " [0.0531]] W1_grad [0. 0. 0. 0. 0. 0.] W2_grad [-0. -0. -0.] b1_grad [0. 0. 0.] b2_grad [-0.01]\n",
            "(1, 1)\n",
            "Epoch: 8 loss [[0.0003]\n",
            " [0.07  ]\n",
            " [0.0531]] W1_grad [0. 0. 0. 0. 0. 0.] W2_grad [-0. -0. -0.] b1_grad [0. 0. 0.] b2_grad [-0.01]\n",
            "(1, 1)\n",
            "Epoch: 9 loss [[0.0003]\n",
            " [0.07  ]\n",
            " [0.0531]] W1_grad [0. 0. 0. 0. 0. 0.] W2_grad [-0. -0. -0.] b1_grad [0. 0. 0.] b2_grad [-0.01]\n",
            "(1, 1)\n",
            "Epoch: 10 loss [[0.0003]\n",
            " [0.07  ]\n",
            " [0.0531]] W1_grad [0. 0. 0. 0. 0. 0.] W2_grad [-0. -0. -0.] b1_grad [0. 0. 0.] b2_grad [-0.01]\n"
          ]
        }
      ]
    }
  ]
}