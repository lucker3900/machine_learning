{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWhLjetUaCcNjdCM2lsiPF"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#题目描述：\n",
        "\n",
        "#假设我们有一个简单的三层全连接神经网络，用于分类任务。网络结构如下：\n",
        "\n",
        "#输入层：3个神经元\n",
        "#隐藏层1：4个神经元，使用 ReLU 激活函数\n",
        "#隐藏层2：2个神经元，使用 ReLU 激活函数\n",
        "#输出层：3个神经元，使用 Softmax 激活函数"
      ],
      "metadata": {
        "id": "qTU5fJTrv3PA"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "P-d3b9tGnaCa"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#relu激活函数\n",
        "def relu(a):\n",
        "  return np.maximum(0, a)"
      ],
      "metadata": {
        "id": "3xkOb3MNvYeo"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#softmax激活函数\n",
        "def softmax(b):\n",
        "  z = np.exp(b - np.max(b, axis=1, keepdims=True))\n",
        "  return z / np.sum(z, axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "IGwRBf86v0wJ"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#前向传播求输出y_pred\n",
        "def forward(W1, W2, W3, b1, b2, b3, X):\n",
        "  Z1 = np.dot(X, W1.T) + b1.T # ndarray(2,4)\n",
        "  H1 = relu(Z1) #ndarray(2,4)\n",
        "\n",
        "  Z2 = np.dot(H1, W2.T) + b2 # ndarray(2,2)\n",
        "  H2 = relu(Z2) #ndarray(2,2)\n",
        "\n",
        "  Z3 = np.dot(H2, W3.T) + b3.T\n",
        "  y_pred = softmax(Z3) #ndarray(2,3)\n",
        "\n",
        "  return Z1, H1, Z2, H2, Z3, y_pred"
      ],
      "metadata": {
        "id": "iMUuPSDlXruR"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#求损失loss函数\n",
        "def compute_loss(y_pred, y_true):\n",
        "  #loss = (y_pred - y_true)**2 / 2 #adarray(2,3)\n",
        "  loss = -np.sum(y_true * np.log(y_pred + 1e-10)) / y_true.shape[0] #ndarray(1,1)\n",
        "  return loss"
      ],
      "metadata": {
        "id": "ypYraWLaejX6"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#反向传播，求参数导数\n",
        "def backward(y_pred, y_true, H1, H2, Z1, Z2, Z3, W2, W3, X):\n",
        "  y_diff = y_pred - y_true #ndarray (2,3)\n",
        "\n",
        "  dz3 = y_diff #ndarray(2,3)\n",
        "  b3_grad = dz3.mean(axis=0, keepdims=True).T #abarray(3,1)\n",
        "  W3_grad = np.dot(dz3.T, H2) #ndarray(3,2)\n",
        "\n",
        "  dh2 = np.dot(dz3, W3) #shape(2,2)\n",
        "  dz2 = dh2 * (Z2>0) #shape(2,2)\n",
        "  b2_grad = np.mean(dz2, axis=0, keepdims=True).T #shape(2,1)\n",
        "  W2_grad = np.dot(dz2.T, H1)\n",
        "\n",
        "  dh1 = np.dot(dz2.T, W2) #shape(2,4)\n",
        "  dz1 = dh1 * (Z1>0) #shape(2,4)\n",
        "  b1_grad = np.mean(dz1, axis=0, keepdims=True).T #shape(4,1)\n",
        "  W1_grad = np.dot(dz1.T, X) #shape(4,3)\n",
        "\n",
        "  return W1_grad, W2_grad, W3_grad, b1_grad, b2_grad, b3_grad"
      ],
      "metadata": {
        "id": "0dtgKVUmf7j0"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#更新参数weight, bias\n",
        "def update_params(W1, W2, W3, b1, b2, b3, learning_rate):\n",
        "  W1 -= learning_rate * W1_grad\n",
        "  W2 -= learning_rate * W2_grad\n",
        "  W3 -= learning_rate * W3_grad\n",
        "\n",
        "  b1 -= learning_rate * b1_grad\n",
        "  b2 -= learning_rate * b2_grad\n",
        "  b3 -= learning_rate * b3_grad\n",
        "  return W1, W2, W3, b1, b2, b3"
      ],
      "metadata": {
        "id": "RQiS_wmx_SUL"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([[0.2, 0.4, 0.1],\n",
        "        [0.7, 0.3, 0.8]])   #ndarray (2, 3)\n",
        "\n",
        "y_true = np.array([[1, 0, 0],\n",
        "          [0, 0, 1]])   # ndarray(2, 3)\n",
        "\n",
        "W1 = np.array([[0.1, 0.2, 0.3],\n",
        "        [0.4, 0.5, 0.6],\n",
        "        [0.7, 0.8, 0.9],    # ndarray(4, 3)\n",
        "        [0.2, 0.3, 0.1]])   #（输入层到隐藏层1）\n",
        "\n",
        "b1 = np.array([[0.1],\n",
        "        [0.2],\n",
        "        [0.3],  # ndarray(4, 1)\n",
        "        [0.1]]) #（隐藏层1 偏置）\n",
        "\n",
        "W2 = np.array([[0.5, 0.4, 0.3, 0.2], # ndarray(2, 4)\n",
        "        [0.1, 0.6, 0.7, 0.8]]) #（隐藏层1 到隐藏层2）\n",
        "\n",
        "b2 = np.array([[0.2],  # ndarray(2, 1)\n",
        "        [0.3]])  #（隐藏层2 偏置）\n",
        "\n",
        "W3 = np.array([[0.9, 0.8],\n",
        "        [0.7, 0.6],   # ndarray(3, 2)\n",
        "        [0.5, 0.4]])   #（隐藏层2 到输出层）\n",
        "\n",
        "b3 = np.array([[0.1],\n",
        "        [0.2],  # ndarray(3, 1)\n",
        "        [0.3]]) #（输出层偏置）\n",
        "\n",
        "learning_rate = 0.02\n",
        "\n",
        "epochs = 40\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  Z1, H1, Z2, H2, Z3, y_pred = forward(W1, W2, W3, b1, b2, b3, X)\n",
        "  loss = compute_loss(y_pred, y_true)\n",
        "\n",
        "  W1_grad, W2_grad, W3_grad, b1_grad, b2_grad, b3_grad = backward(y_pred, y_true, H1, H2, Z1, Z2, Z3, W2, W3, X)\n",
        "  W1, W2, W3, b1, b2, b3 = update_params(W1, W2, W3, b1, b2, b3, learning_rate)\n",
        "\n",
        "  print(\"epoch:\", epoch + 1,\n",
        "    \"loss\", np.round(loss.mean(), 4),\n",
        "    \"W1_grad\", np.round(W1_grad.flatten(), 2),\n",
        "    \"W2_grad\", np.round(W2_grad.flatten(), 2),\n",
        "    \"W3_grad\", np.round(W3_grad.flatten(), 2),\n",
        "    \"b1_grad\", np.round(b1_grad.flatten(), 2),\n",
        "    \"b2_grad\", np.round(b2_grad.flatten(), 2),\n",
        "    \"b3_grad\", np.round(b3_grad.flatten(), 2)\n",
        "  )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6H3k01mtvGx",
        "outputId": "692fa700-1bcc-4be2-faf3-b6d0d0464947"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1 loss 1.3983 W1_grad [-0.04 -0.03 -0.04  0.1   0.08  0.1   0.14  0.11  0.14  0.18  0.14  0.18] W2_grad [0.1  0.24 0.37 0.08 0.1  0.24 0.37 0.08] W3_grad [ 0.46  0.75  0.72  1.16 -1.18 -1.91] b1_grad [-0.05  0.11  0.16  0.2 ] b2_grad [0.07 0.07] b3_grad [ 0.02  0.3  -0.32]\n",
            "epoch: 2 loss 1.3147 W1_grad [-0.05 -0.04 -0.05  0.07  0.06  0.07  0.11  0.09  0.11  0.14  0.12  0.14] W2_grad [0.08 0.19 0.31 0.06 0.08 0.18 0.28 0.05] W3_grad [ 0.42  0.69  0.67  1.09 -1.1  -1.78] b1_grad [-0.05  0.08  0.12  0.16] b2_grad [0.05 0.04] b3_grad [ 0.    0.29 -0.29]\n",
            "epoch: 3 loss 1.2438 W1_grad [-0.05 -0.04 -0.05  0.05  0.04  0.05  0.08  0.07  0.08  0.11  0.09  0.11] W2_grad [0.07 0.16 0.25 0.05 0.05 0.13 0.2  0.03] W3_grad [ 0.39  0.63  0.63  1.03 -1.02 -1.66] b1_grad [-0.05  0.06  0.1   0.13] b2_grad [0.03 0.02] b3_grad [-0.01  0.28 -0.27]\n",
            "epoch: 4 loss 1.1833 W1_grad [-0.05 -0.04 -0.05  0.03  0.03  0.03  0.06  0.05  0.05  0.08  0.07  0.08] W2_grad [0.05 0.13 0.2  0.03 0.04 0.08 0.13 0.02] W3_grad [ 0.35  0.57  0.6   0.97 -0.95 -1.54] b1_grad [-0.06  0.04  0.07  0.1 ] b2_grad [0.02 0.  ] b3_grad [-0.02  0.26 -0.24]\n",
            "epoch: 5 loss 1.1314 W1_grad [-0.05 -0.04 -0.05  0.02  0.02  0.01  0.04  0.04  0.03  0.06  0.06  0.06] W2_grad [0.04 0.1  0.15 0.02 0.02 0.04 0.07 0.01] W3_grad [ 0.32  0.52  0.56  0.92 -0.88 -1.44] b1_grad [-0.06  0.02  0.05  0.08] b2_grad [ 0.01 -0.01] b3_grad [-0.03  0.25 -0.22]\n",
            "epoch: 6 loss 1.0868 W1_grad [-0.05 -0.04 -0.05  0.    0.01 -0.    0.02  0.03  0.02  0.04  0.04  0.03] W2_grad [ 0.03  0.07  0.12  0.02  0.01  0.01  0.02 -0.  ] W3_grad [ 0.28  0.47  0.53  0.87 -0.82 -1.34] b1_grad [-0.06  0.01  0.03  0.06] b2_grad [-0.   -0.02] b3_grad [-0.05  0.24 -0.19]\n",
            "epoch: 7 loss 1.0482 W1_grad [-0.05 -0.04 -0.05 -0.01  0.   -0.01  0.01  0.02  0.    0.02  0.03  0.02] W2_grad [ 0.02  0.05  0.08  0.01 -0.01 -0.01 -0.02 -0.01] W3_grad [ 0.25  0.42  0.5   0.82 -0.76 -1.24] b1_grad [-0.05 -0.    0.02  0.04] b2_grad [-0.01 -0.03] b3_grad [-0.06  0.23 -0.17]\n",
            "epoch: 8 loss 1.0146 W1_grad [-0.04 -0.04 -0.04 -0.02 -0.01 -0.02 -0.    0.01 -0.01  0.01  0.02  0.  ] W2_grad [ 0.02  0.04  0.06  0.   -0.02 -0.03 -0.05 -0.02] W3_grad [ 0.23  0.37  0.48  0.78 -0.7  -1.16] b1_grad [-0.05 -0.01  0.01  0.02] b2_grad [-0.02 -0.04] b3_grad [-0.07  0.22 -0.15]\n",
            "epoch: 9 loss 0.9854 W1_grad [-0.04 -0.04 -0.04 -0.02 -0.01 -0.03 -0.01  0.   -0.02 -0.01  0.01 -0.01] W2_grad [ 0.01  0.02  0.03 -0.   -0.02 -0.05 -0.08 -0.02] W3_grad [ 0.2   0.33  0.45  0.74 -0.65 -1.08] b1_grad [-0.05 -0.02 -0.    0.01] b2_grad [-0.02 -0.05] b3_grad [-0.08  0.21 -0.13]\n",
            "epoch: 10 loss 0.9599 W1_grad [-0.04 -0.03 -0.04 -0.03 -0.02 -0.03 -0.02 -0.   -0.03 -0.02  0.   -0.02] W2_grad [ 0.    0.01  0.01 -0.01 -0.03 -0.06 -0.1  -0.03] W3_grad [ 0.18  0.29  0.43  0.71 -0.61 -1.  ] b1_grad [-0.05 -0.02 -0.01  0.  ] b2_grad [-0.03 -0.05] b3_grad [-0.09  0.2  -0.11]\n",
            "epoch: 11 loss 0.9375 W1_grad [-0.04 -0.03 -0.04 -0.03 -0.02 -0.04 -0.03 -0.01 -0.03 -0.02 -0.   -0.03] W2_grad [-0.   -0.   -0.   -0.01 -0.03 -0.07 -0.12 -0.03] W3_grad [ 0.15  0.26  0.41  0.67 -0.56 -0.93] b1_grad [-0.05 -0.03 -0.02 -0.01] b2_grad [-0.03 -0.05] b3_grad [-0.1   0.19 -0.09]\n",
            "epoch: 12 loss 0.9178 W1_grad [-0.03 -0.03 -0.03 -0.04 -0.02 -0.04 -0.03 -0.01 -0.04 -0.03 -0.01 -0.04] W2_grad [-0.   -0.01 -0.01 -0.01 -0.04 -0.08 -0.13 -0.03] W3_grad [ 0.13  0.22  0.39  0.64 -0.52 -0.86] b1_grad [-0.04 -0.03 -0.02 -0.02] b2_grad [-0.03 -0.05] b3_grad [-0.1   0.18 -0.08]\n",
            "epoch: 13 loss 0.9005 W1_grad [-0.03 -0.03 -0.03 -0.04 -0.02 -0.04 -0.04 -0.02 -0.04 -0.04 -0.01 -0.04] W2_grad [-0.01 -0.02 -0.03 -0.01 -0.04 -0.09 -0.14 -0.03] W3_grad [ 0.11  0.19  0.37  0.61 -0.48 -0.8 ] b1_grad [-0.04 -0.04 -0.03 -0.02] b2_grad [-0.04 -0.05] b3_grad [-0.11  0.17 -0.06]\n",
            "epoch: 14 loss 0.8851 W1_grad [-0.03 -0.03 -0.03 -0.04 -0.02 -0.05 -0.04 -0.02 -0.05 -0.04 -0.02 -0.05] W2_grad [-0.01 -0.02 -0.03 -0.01 -0.04 -0.09 -0.15 -0.03] W3_grad [ 0.09  0.16  0.35  0.58 -0.45 -0.74] b1_grad [-0.04 -0.04 -0.03 -0.03] b2_grad [-0.04 -0.05] b3_grad [-0.12  0.16 -0.04]\n",
            "epoch: 15 loss 0.8714 W1_grad [-0.03 -0.03 -0.02 -0.04 -0.03 -0.05 -0.04 -0.02 -0.05 -0.04 -0.02 -0.05] W2_grad [-0.01 -0.03 -0.04 -0.02 -0.04 -0.1  -0.15 -0.03] W3_grad [ 0.08  0.13  0.34  0.56 -0.41 -0.69] b1_grad [-0.04 -0.04 -0.03 -0.03] b2_grad [-0.04 -0.05] b3_grad [-0.13  0.16 -0.03]\n",
            "epoch: 16 loss 0.8593 W1_grad [-0.02 -0.02 -0.02 -0.04 -0.03 -0.05 -0.04 -0.02 -0.05 -0.05 -0.02 -0.05] W2_grad [-0.01 -0.03 -0.05 -0.02 -0.04 -0.1  -0.15 -0.04] W3_grad [ 0.06  0.11  0.32  0.53 -0.38 -0.64] b1_grad [-0.03 -0.04 -0.04 -0.03] b2_grad [-0.04 -0.05] b3_grad [-0.13  0.15 -0.02]\n",
            "epoch: 17 loss 0.8484 W1_grad [-0.02 -0.02 -0.02 -0.04 -0.03 -0.05 -0.05 -0.02 -0.05 -0.05 -0.02 -0.06] W2_grad [-0.02 -0.03 -0.05 -0.02 -0.05 -0.1  -0.15 -0.04] W3_grad [ 0.05  0.09  0.31  0.51 -0.36 -0.6 ] b1_grad [-0.03 -0.04 -0.04 -0.04] b2_grad [-0.04 -0.05] b3_grad [-0.14  0.14 -0.01]\n",
            "epoch: 18 loss 0.8386 W1_grad [-0.02 -0.02 -0.01 -0.04 -0.03 -0.05 -0.05 -0.03 -0.05 -0.05 -0.02 -0.06] W2_grad [-0.02 -0.04 -0.06 -0.02 -0.05 -0.1  -0.16 -0.03] W3_grad [ 0.03  0.06  0.3   0.49 -0.33 -0.56] b1_grad [-0.03 -0.04 -0.04 -0.04] b2_grad [-0.04 -0.05] b3_grad [-0.14  0.14  0.  ]\n",
            "epoch: 19 loss 0.8297 W1_grad [-0.01 -0.02 -0.01 -0.04 -0.03 -0.05 -0.05 -0.03 -0.05 -0.05 -0.03 -0.06] W2_grad [-0.02 -0.04 -0.06 -0.02 -0.05 -0.1  -0.15 -0.03] W3_grad [ 0.02  0.04  0.28  0.47 -0.31 -0.52] b1_grad [-0.03 -0.04 -0.04 -0.04] b2_grad [-0.04 -0.04] b3_grad [-0.15  0.13  0.01]\n",
            "epoch: 20 loss 0.8217 W1_grad [-0.01 -0.02 -0.01 -0.04 -0.03 -0.05 -0.05 -0.03 -0.05 -0.05 -0.03 -0.06] W2_grad [-0.02 -0.04 -0.06 -0.02 -0.04 -0.1  -0.15 -0.03] W3_grad [ 0.01  0.03  0.27  0.45 -0.28 -0.48] b1_grad [-0.02 -0.04 -0.04 -0.04] b2_grad [-0.04 -0.04] b3_grad [-0.15  0.13  0.02]\n",
            "epoch: 21 loss 0.8144 W1_grad [-0.01 -0.02 -0.01 -0.04 -0.03 -0.04 -0.05 -0.03 -0.05 -0.05 -0.03 -0.06] W2_grad [-0.02 -0.04 -0.06 -0.02 -0.04 -0.1  -0.15 -0.03] W3_grad [ 0.    0.01  0.26  0.44 -0.26 -0.45] b1_grad [-0.02 -0.04 -0.04 -0.04] b2_grad [-0.04 -0.04] b3_grad [-0.15  0.12  0.03]\n",
            "epoch: 22 loss 0.8076 W1_grad [-0.01 -0.02 -0.   -0.04 -0.03 -0.04 -0.05 -0.03 -0.05 -0.05 -0.03 -0.06] W2_grad [-0.02 -0.04 -0.06 -0.02 -0.04 -0.1  -0.15 -0.03] W3_grad [-0.01 -0.    0.25  0.42 -0.25 -0.42] b1_grad [-0.02 -0.04 -0.04 -0.04] b2_grad [-0.04 -0.04] b3_grad [-0.16  0.12  0.04]\n",
            "epoch: 23 loss 0.8015 W1_grad [-0.01 -0.01 -0.   -0.04 -0.03 -0.04 -0.05 -0.03 -0.05 -0.05 -0.03 -0.06] W2_grad [-0.02 -0.04 -0.06 -0.02 -0.04 -0.09 -0.15 -0.03] W3_grad [-0.01 -0.01  0.24  0.41 -0.23 -0.39] b1_grad [-0.02 -0.04 -0.04 -0.04] b2_grad [-0.04 -0.04] b3_grad [-0.16  0.11  0.05]\n",
            "epoch: 24 loss 0.7958 W1_grad [-0.   -0.01 -0.   -0.04 -0.03 -0.04 -0.05 -0.03 -0.05 -0.05 -0.03 -0.06] W2_grad [-0.02 -0.04 -0.06 -0.02 -0.04 -0.09 -0.15 -0.03] W3_grad [-0.02 -0.03  0.23  0.39 -0.21 -0.37] b1_grad [-0.02 -0.04 -0.04 -0.04] b2_grad [-0.04 -0.03] b3_grad [-0.16  0.11  0.05]\n",
            "epoch: 25 loss 0.7904 W1_grad [-0.   -0.01  0.   -0.04 -0.03 -0.04 -0.05 -0.03 -0.05 -0.05 -0.03 -0.06] W2_grad [-0.02 -0.04 -0.06 -0.02 -0.04 -0.09 -0.14 -0.03] W3_grad [-0.03 -0.04  0.23  0.38 -0.2  -0.34] b1_grad [-0.01 -0.04 -0.04 -0.04] b2_grad [-0.04 -0.03] b3_grad [-0.16  0.11  0.06]\n",
            "epoch: 26 loss 0.7855 W1_grad [-0.   -0.01  0.   -0.04 -0.03 -0.04 -0.05 -0.03 -0.05 -0.05 -0.03 -0.06] W2_grad [-0.02 -0.04 -0.06 -0.02 -0.04 -0.09 -0.14 -0.03] W3_grad [-0.03 -0.04  0.22  0.37 -0.19 -0.32] b1_grad [-0.01 -0.04 -0.04 -0.04] b2_grad [-0.04 -0.03] b3_grad [-0.16  0.1   0.06]\n",
            "epoch: 27 loss 0.7808 W1_grad [ 0.   -0.01  0.   -0.04 -0.02 -0.04 -0.05 -0.03 -0.05 -0.05 -0.03 -0.06] W2_grad [-0.02 -0.04 -0.06 -0.02 -0.04 -0.09 -0.14 -0.03] W3_grad [-0.04 -0.05  0.21  0.36 -0.17 -0.3 ] b1_grad [-0.01 -0.04 -0.04 -0.04] b2_grad [-0.04 -0.03] b3_grad [-0.17  0.1   0.07]\n",
            "epoch: 28 loss 0.7765 W1_grad [ 0.   -0.01  0.01 -0.04 -0.02 -0.04 -0.05 -0.03 -0.05 -0.05 -0.03 -0.06] W2_grad [-0.02 -0.04 -0.06 -0.02 -0.04 -0.09 -0.13 -0.03] W3_grad [-0.04 -0.06  0.21  0.34 -0.16 -0.29] b1_grad [-0.01 -0.04 -0.04 -0.04] b2_grad [-0.04 -0.03] b3_grad [-0.17  0.1   0.07]\n",
            "epoch: 29 loss 0.7723 W1_grad [ 0.   -0.01  0.01 -0.04 -0.02 -0.04 -0.05 -0.03 -0.05 -0.05 -0.03 -0.06] W2_grad [-0.02 -0.04 -0.06 -0.02 -0.04 -0.09 -0.13 -0.03] W3_grad [-0.04 -0.06  0.2   0.33 -0.15 -0.27] b1_grad [-0.01 -0.04 -0.04 -0.04] b2_grad [-0.03 -0.02] b3_grad [-0.17  0.09  0.08]\n",
            "epoch: 30 loss 0.7684 W1_grad [ 0.   -0.01  0.01 -0.03 -0.02 -0.04 -0.04 -0.03 -0.05 -0.05 -0.03 -0.06] W2_grad [-0.02 -0.04 -0.06 -0.02 -0.04 -0.08 -0.13 -0.03] W3_grad [-0.05 -0.07  0.19  0.32 -0.15 -0.26] b1_grad [-0.01 -0.03 -0.04 -0.04] b2_grad [-0.03 -0.02] b3_grad [-0.17  0.09  0.08]\n",
            "epoch: 31 loss 0.7647 W1_grad [ 0.   -0.01  0.01 -0.03 -0.02 -0.03 -0.04 -0.03 -0.05 -0.05 -0.03 -0.06] W2_grad [-0.02 -0.04 -0.06 -0.02 -0.04 -0.08 -0.13 -0.03] W3_grad [-0.05 -0.07  0.19  0.31 -0.14 -0.24] b1_grad [-0.01 -0.03 -0.04 -0.04] b2_grad [-0.03 -0.02] b3_grad [-0.17  0.09  0.08]\n",
            "epoch: 32 loss 0.7611 W1_grad [ 0.01 -0.01  0.01 -0.03 -0.02 -0.03 -0.04 -0.03 -0.05 -0.05 -0.03 -0.06] W2_grad [-0.02 -0.04 -0.06 -0.02 -0.04 -0.08 -0.13 -0.03] W3_grad [-0.05 -0.08  0.18  0.31 -0.13 -0.23] b1_grad [-0.01 -0.03 -0.04 -0.04] b2_grad [-0.03 -0.02] b3_grad [-0.17  0.08  0.08]\n",
            "epoch: 33 loss 0.7577 W1_grad [ 0.01 -0.01  0.01 -0.03 -0.02 -0.03 -0.04 -0.03 -0.05 -0.05 -0.03 -0.06] W2_grad [-0.02 -0.04 -0.06 -0.02 -0.04 -0.08 -0.12 -0.03] W3_grad [-0.05 -0.08  0.18  0.3  -0.12 -0.22] b1_grad [-0.01 -0.03 -0.04 -0.04] b2_grad [-0.03 -0.02] b3_grad [-0.17  0.08  0.09]\n",
            "epoch: 34 loss 0.7545 W1_grad [ 0.01 -0.01  0.01 -0.03 -0.02 -0.03 -0.04 -0.02 -0.05 -0.05 -0.03 -0.06] W2_grad [-0.02 -0.04 -0.06 -0.02 -0.03 -0.08 -0.12 -0.03] W3_grad [-0.06 -0.08  0.17  0.29 -0.12 -0.21] b1_grad [-0.01 -0.03 -0.04 -0.04] b2_grad [-0.03 -0.02] b3_grad [-0.17  0.08  0.09]\n",
            "epoch: 35 loss 0.7514 W1_grad [ 0.01 -0.01  0.01 -0.03 -0.02 -0.03 -0.04 -0.02 -0.05 -0.05 -0.03 -0.05] W2_grad [-0.02 -0.04 -0.06 -0.02 -0.03 -0.08 -0.12 -0.02] W3_grad [-0.06 -0.08  0.17  0.28 -0.11 -0.2 ] b1_grad [-0.   -0.03 -0.04 -0.04] b2_grad [-0.03 -0.02] b3_grad [-0.17  0.08  0.09]\n",
            "epoch: 36 loss 0.7483 W1_grad [ 0.01 -0.01  0.01 -0.03 -0.02 -0.03 -0.04 -0.02 -0.05 -0.05 -0.03 -0.05] W2_grad [-0.02 -0.04 -0.06 -0.02 -0.03 -0.08 -0.12 -0.02] W3_grad [-0.06 -0.08  0.16  0.28 -0.11 -0.19] b1_grad [-0.   -0.03 -0.04 -0.04] b2_grad [-0.03 -0.02] b3_grad [-0.17  0.08  0.09]\n",
            "epoch: 37 loss 0.7454 W1_grad [ 0.01 -0.    0.02 -0.03 -0.02 -0.03 -0.04 -0.02 -0.04 -0.05 -0.03 -0.05] W2_grad [-0.02 -0.04 -0.06 -0.02 -0.03 -0.07 -0.12 -0.02] W3_grad [-0.06 -0.08  0.16  0.27 -0.1  -0.18] b1_grad [-0.   -0.03 -0.04 -0.04] b2_grad [-0.03 -0.01] b3_grad [-0.17  0.07  0.1 ]\n",
            "epoch: 38 loss 0.7426 W1_grad [ 0.01 -0.    0.02 -0.03 -0.02 -0.03 -0.04 -0.02 -0.04 -0.05 -0.03 -0.05] W2_grad [-0.02 -0.04 -0.06 -0.02 -0.03 -0.07 -0.11 -0.02] W3_grad [-0.06 -0.08  0.15  0.26 -0.1  -0.18] b1_grad [-0.   -0.03 -0.04 -0.04] b2_grad [-0.03 -0.01] b3_grad [-0.17  0.07  0.1 ]\n",
            "epoch: 39 loss 0.7399 W1_grad [ 0.01 -0.    0.02 -0.03 -0.02 -0.03 -0.04 -0.02 -0.04 -0.05 -0.03 -0.05] W2_grad [-0.02 -0.04 -0.06 -0.02 -0.03 -0.07 -0.11 -0.02] W3_grad [-0.06 -0.08  0.15  0.26 -0.09 -0.17] b1_grad [-0.   -0.03 -0.04 -0.04] b2_grad [-0.03 -0.01] b3_grad [-0.17  0.07  0.1 ]\n",
            "epoch: 40 loss 0.7373 W1_grad [ 0.01 -0.    0.02 -0.03 -0.02 -0.03 -0.04 -0.02 -0.04 -0.05 -0.03 -0.05] W2_grad [-0.02 -0.04 -0.05 -0.02 -0.03 -0.07 -0.11 -0.02] W3_grad [-0.06 -0.08  0.15  0.25 -0.09 -0.17] b1_grad [-0.   -0.03 -0.04 -0.04] b2_grad [-0.03 -0.01] b3_grad [-0.17  0.07  0.1 ]\n"
          ]
        }
      ]
    }
  ]
}