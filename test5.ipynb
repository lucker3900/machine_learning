{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCjHVXKUaIVaB6tsMncXL4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "x2jxbi3OnkHC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#定义relu函数\n",
        "def relu(a):\n",
        "  np.maximum(0, a)\n",
        "  return a"
      ],
      "metadata": {
        "id": "4vFdgSK_nozS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#前向传播,计算输出y_pred\n",
        "def forward(W1, W2, W3, B1, B2, B3, X):\n",
        "  Z1 = np.dot(W1, X) + B1\n",
        "  H1 = relu(Z1)\n",
        "\n",
        "  Z2 = np.dot(W2, H1) + B2\n",
        "  H2 = relu(Z2)\n",
        "\n",
        "  y_pred = np.dot(W3, H2) + B3\n",
        "  return Z1, Z2, H1, H2, y_pred"
      ],
      "metadata": {
        "id": "P6QgJK0Cn8Wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#计算损失函数loss\n",
        "def compute_loss(y_pred, y_yrue):\n",
        "  loss = (y_pred - y_true)**2 / 2\n",
        "  return loss"
      ],
      "metadata": {
        "id": "YEy2TDN9pu6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#反向传播,计算w,b梯度\n",
        "def backward(y_pred, y_true,H1, H2, Z1, Z2, W1, W2, W3):\n",
        "\n",
        "  B3_grad = y_pred - y_true\n",
        "  W3_grad = B3_grad * H2\n",
        "\n",
        "  B2_grad = B3_grad * W3.flatten() * (Z2>0)\n",
        "  W2_grad = np.outer(B2_grad, H1)\n",
        "\n",
        "  B1_grad = np.dot(B2_grad, W2) * (Z1>0)\n",
        "  W1_grad = np.outer(B1_grad, X)\n",
        "\n",
        "  return W1_grad, W2_grad, W3_grad, B1_grad, B2_grad, B3_grad"
      ],
      "metadata": {
        "id": "kfdYpf1trgI4"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#更新参数w,b\n",
        "def update_param(learning_rate, W1, W2, W3, B1, B2, B3, W1_grad, W2_grad, W3_grad, B1_grad, B2_grad, B3_grad):\n",
        "   W1 -= learning_rate * W1_grad\n",
        "   W2 -= learning_rate * W2_grad\n",
        "   W3 -= learning_rate * W3_grad\n",
        "   B1 -= learning_rate * B1_grad\n",
        "   B2 -= learning_rate * B2_grad\n",
        "   B3 -= learning_rate * B3_grad\n",
        "\n",
        "   return W1, W2, W3, B1, B2, B3"
      ],
      "metadata": {
        "id": "G_F7nEGzVbfs"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "wsUUNVZQnjPj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d841e271-a71c-4e1a-9668-924f50e982a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 loss 2.981 W1_grad [-1.36  2.72 -4.07  0.   -0.    0.   -0.    0.   -0.    0.   -0.    0.  ] W2_grad [-0.    0.    0.    0.    0.1  -0.02 -0.1  -0.02  7.64 -1.61 -7.98 -1.27] W3_grad [  0.72 -11.41  -2.66] B1_grad [-1.36  0.   -0.    0.  ] B2_grad [-0.    0.03  2.58] B3_grad [-2.44]\n",
            "Epoch 2 loss 1.1902 W1_grad [-1.22  2.43 -3.65  0.   -0.    0.   -0.    0.   -0.    0.   -0.    0.  ] W2_grad [-0.    0.    0.    0.   -0.49  0.1   0.48  0.08  5.03 -0.99 -4.91 -0.78] W3_grad [ 0.77 -7.66 -0.69] B1_grad [-1.22  0.   -0.    0.  ] B2_grad [-0.   -0.16  1.59] B3_grad [-1.54]\n",
            "Epoch 3 loss 0.4565 W1_grad [-0.25  0.5  -0.75  0.   -0.    0.   -0.    0.   -0.    0.   -0.    0.  ] W2_grad [-0.    0.    0.    0.   -0.57  0.11  0.52  0.08  0.   -0.   -0.   -0.  ] W3_grad [ 0.66 -5.03  0.01] B1_grad [-0.25  0.   -0.    0.  ] B2_grad [-0.   -0.17  0.  ] B3_grad [-0.96]\n",
            "Epoch 4 loss 0.2442 W1_grad [-0.23  0.47 -0.7   0.   -0.    0.   -0.    0.   -0.    0.   -0.    0.  ] W2_grad [-0.    0.    0.    0.   -0.54  0.1   0.49  0.08  0.   -0.   -0.   -0.  ] W3_grad [ 0.51 -3.75  0.02] B1_grad [-0.23  0.   -0.    0.  ] B2_grad [-0.   -0.16  0.  ] B3_grad [-0.7]\n",
            "Epoch 5 loss 0.1275 W1_grad [-0.2   0.4  -0.59  0.   -0.    0.   -0.    0.   -0.    0.   -0.    0.  ] W2_grad [-0.    0.    0.    0.   -0.46  0.08  0.41  0.07  0.   -0.   -0.   -0.  ] W3_grad [ 0.38 -2.75  0.03] B1_grad [-0.2  0.  -0.   0. ] B2_grad [-0.   -0.13  0.  ] B3_grad [-0.5]\n",
            "Epoch 6 loss 0.0647 W1_grad [-0.16  0.31 -0.47  0.   -0.    0.   -0.    0.   -0.    0.   -0.    0.  ] W2_grad [-0.    0.    0.    0.   -0.36  0.07  0.32  0.05  0.   -0.   -0.   -0.  ] W3_grad [ 0.28 -1.99  0.03] B1_grad [-0.16  0.   -0.    0.  ] B2_grad [-0.   -0.11  0.  ] B3_grad [-0.36]\n",
            "Epoch 7 loss 0.0319 W1_grad [-0.12  0.24 -0.35  0.   -0.    0.   -0.    0.   -0.    0.   -0.    0.  ] W2_grad [-0.    0.    0.    0.   -0.27  0.05  0.24  0.04  0.   -0.   -0.   -0.  ] W3_grad [ 0.21 -1.41  0.02] B1_grad [-0.12  0.   -0.    0.  ] B2_grad [-0.   -0.08  0.  ] B3_grad [-0.25]\n",
            "Epoch 8 loss 0.0154 W1_grad [-0.09  0.17 -0.26  0.   -0.    0.   -0.    0.   -0.    0.   -0.    0.  ] W2_grad [-0.    0.    0.    0.   -0.2   0.04  0.18  0.03  0.   -0.   -0.   -0.  ] W3_grad [ 0.15 -0.99  0.02] B1_grad [-0.09  0.   -0.    0.  ] B2_grad [-0.   -0.06  0.  ] B3_grad [-0.18]\n",
            "Epoch 9 loss 0.0073 W1_grad [-0.06  0.12 -0.18  0.   -0.    0.   -0.    0.   -0.    0.   -0.    0.  ] W2_grad [-0.    0.    0.    0.   -0.14  0.03  0.13  0.02  0.   -0.   -0.   -0.  ] W3_grad [ 0.1  -0.68  0.01] B1_grad [-0.06  0.   -0.    0.  ] B2_grad [-0.   -0.04  0.  ] B3_grad [-0.12]\n",
            "Epoch 10 loss 0.0034 W1_grad [-0.04  0.08 -0.13  0.   -0.    0.   -0.    0.   -0.    0.   -0.    0.  ] W2_grad [-0.    0.    0.    0.   -0.1   0.02  0.09  0.01  0.   -0.   -0.   -0.  ] W3_grad [ 0.07 -0.47  0.01] B1_grad [-0.04  0.   -0.    0.  ] B2_grad [-0.   -0.03  0.  ] B3_grad [-0.08]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-a5a898cf3d76>:34: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  \"loss\", np.round(float(loss), 4),\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(42)  # 固定随机种子，保证可复现\n",
        "\n",
        "# 输入特征\n",
        "X = np.array([1.0, -2.0, 3.0])  # (3,)\n",
        "\n",
        "# 隐藏层 1\n",
        "W1 = np.random.randn(4, 3)  # (4,3)\n",
        "B1 = np.random.randn(4)  # (4,)\n",
        "\n",
        "# 隐藏层 2\n",
        "W2 = np.random.randn(3, 4)  # (3,4)\n",
        "B2 = np.random.randn(3)  # (3,)\n",
        "\n",
        "# 输出层\n",
        "W3 = np.random.randn(1, 3)  # (1,3)\n",
        "B3 = np.random.randn(1)  # (1,)\n",
        "\n",
        "# 真实值\n",
        "y_true = np.array([1.5])\n",
        "\n",
        "# 学习率\n",
        "learning_rate = 0.01\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "   Z1, Z2, H1, H2, y_pred = forward(W1, W2, W3, B1, B2, B3, X)\n",
        "   loss = compute_loss(y_pred, y_true)\n",
        "   W1_grad, W2_grad, W3_grad, B1_grad, B2_grad, B3_grad = backward(y_pred, y_true,H1, H2, Z1, Z2, W1, W2, W3)\n",
        "   W1, W2, W3, B1, B2, B3 = update_param(learning_rate, W1, W2, W3, B1, B2, B3, W1_grad, W2_grad, W3_grad, B1_grad, B2_grad, B3_grad)\n",
        "\n",
        "   print(\n",
        "      \"Epoch\", epoch + 1,\n",
        "      \"loss\", np.round(float(loss), 4),\n",
        "      \"W1_grad\", np.round(W1_grad.flatten(), 2),\n",
        "      \"W2_grad\", np.round(W2_grad.flatten(), 2),\n",
        "      \"W3_grad\", np.round(W3_grad.flatten(), 2),\n",
        "      \"B1_grad\", np.round(B1_grad.flatten(), 2),\n",
        "      \"B2_grad\", np.round(B2_grad.flatten(), 2),\n",
        "      \"B3_grad\", np.round(B3_grad.flatten(), 2)\n",
        "   )"
      ]
    }
  ]
}