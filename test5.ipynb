{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhvnlknlzhTk5PLhZLxwHI"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "x2jxbi3OnkHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#定义relu函数\n",
        "def relu(a):\n",
        "  a = np.maximum(0, a)\n",
        "  return a"
      ],
      "metadata": {
        "id": "4vFdgSK_nozS"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#前向传播,计算输出y_pred\n",
        "def forward(W1, W2, W3, B1, B2, B3, X):\n",
        "  Z1 = np.dot(W1, X) + B1\n",
        "  H1 = relu(Z1)\n",
        "\n",
        "  Z2 = np.dot(W2, H1) + B2\n",
        "  H2 = relu(Z2)\n",
        "\n",
        "  y_pred = np.dot(W3, H2) + B3\n",
        "  return Z1, Z2, H1, H2, y_pred"
      ],
      "metadata": {
        "id": "P6QgJK0Cn8Wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#计算损失函数loss\n",
        "def compute_loss(y_pred, y_yrue):\n",
        "  loss = (y_pred - y_true)**2 / 2\n",
        "  return loss"
      ],
      "metadata": {
        "id": "YEy2TDN9pu6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#反向传播,计算w,b梯度\n",
        "def backward(y_pred, y_true,H1, H2, Z1, Z2, W1, W2, W3):\n",
        "\n",
        "  B3_grad = y_pred - y_true\n",
        "  W3_grad = B3_grad * H2\n",
        "\n",
        "  B2_grad = B3_grad * W3.flatten() * (Z2>0)\n",
        "  W2_grad = np.outer(B2_grad, H1)\n",
        "\n",
        "  B1_grad = np.dot(B2_grad, W2) * (Z1>0)\n",
        "  W1_grad = np.outer(B1_grad, X)\n",
        "\n",
        "  return W1_grad, W2_grad, W3_grad, B1_grad, B2_grad, B3_grad"
      ],
      "metadata": {
        "id": "kfdYpf1trgI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#更新参数w,b\n",
        "def update_param(learning_rate, W1, W2, W3, B1, B2, B3, W1_grad, W2_grad, W3_grad, B1_grad, B2_grad, B3_grad):\n",
        "   W1 -= learning_rate * W1_grad\n",
        "   W2 -= learning_rate * W2_grad\n",
        "   W3 -= learning_rate * W3_grad\n",
        "   B1 -= learning_rate * B1_grad\n",
        "   B2 -= learning_rate * B2_grad\n",
        "   B3 -= learning_rate * B3_grad\n",
        "\n",
        "   return W1, W2, W3, B1, B2, B3"
      ],
      "metadata": {
        "id": "G_F7nEGzVbfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "wsUUNVZQnjPj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fd5b04d-df33-4e16-89a5-e75bfae9a02e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 loss 0.2679 W1_grad [ 0.01 -0.03  0.04 -0.    0.   -0.    0.   -0.    0.   -0.    0.   -0.  ] W2_grad [-0.   -0.   -0.   -0.    0.03  0.    0.    0.    0.    0.    0.    0.  ] W3_grad [-0.   -2.96 -0.  ] B1_grad [ 0.01 -0.    0.   -0.  ] B2_grad [-0.    0.01  0.  ] B3_grad [-0.73]\n",
            "Epoch 2 loss 0.1831 W1_grad [-0.01  0.03 -0.04  0.   -0.    0.   -0.    0.   -0.    0.   -0.    0.  ] W2_grad [-0.   -0.   -0.   -0.   -0.03 -0.   -0.   -0.    0.    0.    0.    0.  ] W3_grad [-0.   -2.44 -0.  ] B1_grad [-0.01  0.   -0.    0.  ] B2_grad [-0.   -0.01  0.  ] B3_grad [-0.61]\n",
            "Epoch 3 loss 0.125 W1_grad [-0.03  0.06 -0.09  0.   -0.    0.   -0.    0.   -0.    0.   -0.    0.  ] W2_grad [-0.   -0.   -0.   -0.   -0.06 -0.   -0.   -0.    0.    0.    0.    0.  ] W3_grad [-0.   -2.02 -0.  ] B1_grad [-0.03  0.   -0.    0.  ] B2_grad [-0.   -0.02  0.  ] B3_grad [-0.5]\n",
            "Epoch 4 loss 0.0852 W1_grad [-0.04  0.07 -0.11  0.   -0.    0.   -0.    0.   -0.    0.   -0.    0.  ] W2_grad [-0.   -0.   -0.   -0.   -0.07 -0.   -0.   -0.    0.    0.    0.    0.  ] W3_grad [-0.   -1.67 -0.  ] B1_grad [-0.04  0.   -0.    0.  ] B2_grad [-0.   -0.03  0.  ] B3_grad [-0.41]\n",
            "Epoch 5 loss 0.0578 W1_grad [-0.04  0.08 -0.12  0.   -0.    0.   -0.    0.   -0.    0.   -0.    0.  ] W2_grad [-0.   -0.   -0.   -0.   -0.08 -0.   -0.   -0.    0.    0.    0.    0.  ] W3_grad [-0.   -1.38 -0.  ] B1_grad [-0.04  0.   -0.    0.  ] B2_grad [-0.   -0.03  0.  ] B3_grad [-0.34]\n",
            "Epoch 6 loss 0.0391 W1_grad [-0.04  0.07 -0.11  0.   -0.    0.   -0.    0.   -0.    0.   -0.    0.  ] W2_grad [-0.   -0.   -0.   -0.   -0.08 -0.   -0.   -0.    0.    0.    0.    0.  ] W3_grad [-0.   -1.14 -0.  ] B1_grad [-0.04  0.   -0.    0.  ] B2_grad [-0.   -0.03  0.  ] B3_grad [-0.28]\n",
            "Epoch 7 loss 0.0263 W1_grad [-0.03  0.07 -0.1   0.   -0.    0.   -0.    0.   -0.    0.   -0.    0.  ] W2_grad [-0.   -0.   -0.   -0.   -0.07 -0.   -0.   -0.    0.    0.    0.    0.  ] W3_grad [-0.   -0.94 -0.  ] B1_grad [-0.03  0.   -0.    0.  ] B2_grad [-0.   -0.02  0.  ] B3_grad [-0.23]\n",
            "Epoch 8 loss 0.0176 W1_grad [-0.03  0.06 -0.09  0.   -0.    0.   -0.    0.   -0.    0.   -0.    0.  ] W2_grad [-0.   -0.   -0.   -0.   -0.06 -0.   -0.   -0.    0.    0.    0.    0.  ] W3_grad [-0.   -0.77 -0.  ] B1_grad [-0.03  0.   -0.    0.  ] B2_grad [-0.   -0.02  0.  ] B3_grad [-0.19]\n",
            "Epoch 9 loss 0.0117 W1_grad [-0.03  0.05 -0.08  0.   -0.    0.   -0.    0.   -0.    0.   -0.    0.  ] W2_grad [-0.   -0.   -0.   -0.   -0.05 -0.   -0.   -0.    0.    0.    0.    0.  ] W3_grad [-0.   -0.63 -0.  ] B1_grad [-0.03  0.   -0.    0.  ] B2_grad [-0.   -0.02  0.  ] B3_grad [-0.15]\n",
            "Epoch 10 loss 0.0078 W1_grad [-0.02  0.05 -0.07  0.   -0.    0.   -0.    0.   -0.    0.   -0.    0.  ] W2_grad [-0.   -0.   -0.   -0.   -0.05 -0.   -0.   -0.    0.    0.    0.    0.  ] W3_grad [-0.   -0.51 -0.  ] B1_grad [-0.02  0.   -0.    0.  ] B2_grad [-0.   -0.02  0.  ] B3_grad [-0.12]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-38-a5a898cf3d76>:34: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  \"loss\", np.round(float(loss), 4),\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(42)  # 固定随机种子，保证可复现\n",
        "\n",
        "# 输入特征\n",
        "X = np.array([1.0, -2.0, 3.0])  # (3,)\n",
        "\n",
        "# 隐藏层 1\n",
        "W1 = np.random.randn(4, 3)  # (4,3)\n",
        "B1 = np.random.randn(4)  # (4,)\n",
        "\n",
        "# 隐藏层 2\n",
        "W2 = np.random.randn(3, 4)  # (3,4)\n",
        "B2 = np.random.randn(3)  # (3,)\n",
        "\n",
        "# 输出层\n",
        "W3 = np.random.randn(1, 3)  # (1,3)\n",
        "B3 = np.random.randn(1)  # (1,)\n",
        "\n",
        "# 真实值\n",
        "y_true = np.array([1.5])\n",
        "\n",
        "# 学习率\n",
        "learning_rate = 0.01\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "   Z1, Z2, H1, H2, y_pred = forward(W1, W2, W3, B1, B2, B3, X)\n",
        "   loss = compute_loss(y_pred, y_true)\n",
        "   W1_grad, W2_grad, W3_grad, B1_grad, B2_grad, B3_grad = backward(y_pred, y_true,H1, H2, Z1, Z2, W1, W2, W3)\n",
        "   W1, W2, W3, B1, B2, B3 = update_param(learning_rate, W1, W2, W3, B1, B2, B3, W1_grad, W2_grad, W3_grad, B1_grad, B2_grad, B3_grad)\n",
        "\n",
        "   print(\n",
        "      \"Epoch\", epoch + 1,\n",
        "      \"loss\", np.round(float(loss), 4),\n",
        "      \"W1_grad\", np.round(W1_grad.flatten(), 2),\n",
        "      \"W2_grad\", np.round(W2_grad.flatten(), 2),\n",
        "      \"W3_grad\", np.round(W3_grad.flatten(), 2),\n",
        "      \"B1_grad\", np.round(B1_grad.flatten(), 2),\n",
        "      \"B2_grad\", np.round(B2_grad.flatten(), 2),\n",
        "      \"B3_grad\", np.round(B3_grad.flatten(), 2)\n",
        "   )"
      ]
    }
  ]
}