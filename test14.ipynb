{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO92ERpsKmXwoYLnsLJx2NJ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tn5LIMf3MCZB"
      },
      "outputs": [],
      "source": [
        "#考虑一个单隐藏层的神经网络，包含以下结构：\n",
        "\n",
        "#输入层：3个神经元。\n",
        "#隐藏层：2个神经元，使用 ReLU 激活函数。\n",
        "#输出层：1个神经元，使用线性激活函数（即无激活函数）。\n",
        "#我们将使用均方误差（Mean Squared Error, MSE）作为损失函数。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "ZrG-rPAUM979"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#计算relu函数\n",
        "def relu(a):\n",
        "  return np.maximum(0, a)"
      ],
      "metadata": {
        "id": "a7xA_ANAQCe1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#计算前向传播,输出y_pred\n",
        "def forward(W1, W2, b1, b2, X):\n",
        "  Z = np.dot(X, W1) + b1\n",
        "  H = relu(Z)\n",
        "\n",
        "  y_pred = np.dot(W2, H) + b2\n",
        "  return Z, H, y_pred"
      ],
      "metadata": {
        "id": "iRHRfTvBQy6-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#求损失函数loss\n",
        "def compute_loss(y_pred, y_true):\n",
        "  loss = (y_pred - y_true)**2 / 2\n",
        "  return loss"
      ],
      "metadata": {
        "id": "VE_Ns1Z8SjZd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#反向传导，求梯度\n",
        "def backward(y_pred, y_true, W2, H, Z, X):\n",
        "  b2_grad = y_pred - y_true\n",
        "  W2_grad = b2_grad * H\n",
        "\n",
        "  bh = b2_grad * W2\n",
        "  b1_grad = bh * (Z>0)\n",
        "  W1_grad = np.outer(X, b1_grad)\n",
        "\n",
        "  return W1_grad, W2_grad, b1_grad, b2_grad"
      ],
      "metadata": {
        "id": "p1Yig-IAtdFy"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#更新参数，weight, bias\n",
        "def update_params(W1, W2, b1, b2, W1_grad, W2_grad, b1_grad, b2_grad, lr):\n",
        "  W1 -= lr * W1_grad\n",
        "  W2 -= lr * W2_grad\n",
        "  b1 -= lr * b1_grad\n",
        "  b2 -= lr * b2_grad\n",
        "  return W1, W2, b1, b2"
      ],
      "metadata": {
        "id": "CBZzfzClyB5f"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X= np.array([1.0, 0.5, -0.5]) #shape (3,)\n",
        "\n",
        "W1= np.array([[0.2, 0.1],\n",
        "      [0.3, -0.4],\n",
        "      [0.5, 0.2]])  #shape (3,2)\n",
        "\n",
        "b1 = np.array([0.1, 0.0]) #shape (2,)\n",
        "\n",
        "W2 = np.array([0.4, 0.6]) #shape (2,)\n",
        "\n",
        "b2 = 0.2\n",
        "\n",
        "y_true = 1.5\n",
        "\n",
        "lr = 0.01\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  Z, H, y_pred = forward(W1, W2, b1, b2, X)\n",
        "  loss = compute_loss(y_pred, y_true)\n",
        "  W1_grad, W2_grad, b1_grad, b2_grad = backward(y_pred, y_true, W2, H, Z, X)\n",
        "  W1, W2, b1, b2 = update_params(W1, W2, b1, b2, W1_grad, W2_grad, b1_grad, b2_grad, lr)\n",
        "\n",
        "  print(\"Epoch:\", epoch + 1,\n",
        "    \"loss:\", np.round(loss, 4),\n",
        "    \"W1_grad\", np.round(W1_grad.flatten(), 2),\n",
        "    \"W2_grad\", np.round(W2_grad.flatten(), 2),\n",
        "    \"b1_grad\", np.round(b1_grad.flatten(), 2),\n",
        "    \"b2_grad\", np.round(b2_grad.flatten(), 2)\n",
        "  )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LWUTIbhNDl0",
        "outputId": "bf9b295a-448d-4e5a-94c3-496ae9288cb8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 loss: 0.7442 W1_grad [-0.49 -0.   -0.24 -0.    0.24  0.  ] W2_grad [-0.24 -0.  ] b1_grad [-0.49 -0.  ] b2_grad [-1.22]\n",
            "Epoch: 2 loss: 0.7229 W1_grad [-0.48 -0.   -0.24 -0.    0.24  0.  ] W2_grad [-0.26 -0.  ] b1_grad [-0.48 -0.  ] b2_grad [-1.2]\n",
            "Epoch: 3 loss: 0.702 W1_grad [-0.48 -0.   -0.24 -0.    0.24  0.  ] W2_grad [-0.27 -0.  ] b1_grad [-0.48 -0.  ] b2_grad [-1.18]\n",
            "Epoch: 4 loss: 0.6816 W1_grad [-0.48 -0.   -0.24 -0.    0.24  0.  ] W2_grad [-0.28 -0.  ] b1_grad [-0.48 -0.  ] b2_grad [-1.17]\n",
            "Epoch: 5 loss: 0.6617 W1_grad [-0.47 -0.   -0.24 -0.    0.24  0.  ] W2_grad [-0.29 -0.  ] b1_grad [-0.47 -0.  ] b2_grad [-1.15]\n",
            "Epoch: 6 loss: 0.6422 W1_grad [-0.47 -0.   -0.23 -0.    0.23  0.  ] W2_grad [-0.29 -0.  ] b1_grad [-0.47 -0.  ] b2_grad [-1.13]\n",
            "Epoch: 7 loss: 0.6231 W1_grad [-0.46 -0.   -0.23 -0.    0.23  0.  ] W2_grad [-0.3 -0. ] b1_grad [-0.46 -0.  ] b2_grad [-1.12]\n",
            "Epoch: 8 loss: 0.6044 W1_grad [-0.46 -0.   -0.23 -0.    0.23  0.  ] W2_grad [-0.31 -0.  ] b1_grad [-0.46 -0.  ] b2_grad [-1.1]\n",
            "Epoch: 9 loss: 0.5861 W1_grad [-0.46 -0.   -0.23 -0.    0.23  0.  ] W2_grad [-0.32 -0.  ] b1_grad [-0.46 -0.  ] b2_grad [-1.08]\n",
            "Epoch: 10 loss: 0.5683 W1_grad [-0.45 -0.   -0.23 -0.    0.23  0.  ] W2_grad [-0.33 -0.  ] b1_grad [-0.45 -0.  ] b2_grad [-1.07]\n",
            "Epoch: 11 loss: 0.5508 W1_grad [-0.45 -0.   -0.23 -0.    0.23  0.  ] W2_grad [-0.33 -0.  ] b1_grad [-0.45 -0.  ] b2_grad [-1.05]\n",
            "Epoch: 12 loss: 0.5337 W1_grad [-0.45 -0.   -0.22 -0.    0.22  0.  ] W2_grad [-0.34 -0.  ] b1_grad [-0.45 -0.  ] b2_grad [-1.03]\n",
            "Epoch: 13 loss: 0.517 W1_grad [-0.44 -0.   -0.22 -0.    0.22  0.  ] W2_grad [-0.35 -0.  ] b1_grad [-0.44 -0.  ] b2_grad [-1.02]\n",
            "Epoch: 14 loss: 0.5006 W1_grad [-0.44 -0.   -0.22 -0.    0.22  0.  ] W2_grad [-0.35 -0.  ] b1_grad [-0.44 -0.  ] b2_grad [-1.]\n",
            "Epoch: 15 loss: 0.4847 W1_grad [-0.44 -0.   -0.22 -0.    0.22  0.  ] W2_grad [-0.36 -0.  ] b1_grad [-0.44 -0.  ] b2_grad [-0.98]\n",
            "Epoch: 16 loss: 0.469 W1_grad [-0.43 -0.   -0.22 -0.    0.22  0.  ] W2_grad [-0.36 -0.  ] b1_grad [-0.43 -0.  ] b2_grad [-0.97]\n",
            "Epoch: 17 loss: 0.4538 W1_grad [-0.43 -0.   -0.21 -0.    0.21  0.  ] W2_grad [-0.37 -0.  ] b1_grad [-0.43 -0.  ] b2_grad [-0.95]\n",
            "Epoch: 18 loss: 0.4389 W1_grad [-0.42 -0.   -0.21 -0.    0.21  0.  ] W2_grad [-0.37 -0.  ] b1_grad [-0.42 -0.  ] b2_grad [-0.94]\n",
            "Epoch: 19 loss: 0.4243 W1_grad [-0.42 -0.   -0.21 -0.    0.21  0.  ] W2_grad [-0.37 -0.  ] b1_grad [-0.42 -0.  ] b2_grad [-0.92]\n",
            "Epoch: 20 loss: 0.4101 W1_grad [-0.42 -0.   -0.21 -0.    0.21  0.  ] W2_grad [-0.38 -0.  ] b1_grad [-0.42 -0.  ] b2_grad [-0.91]\n",
            "Epoch: 21 loss: 0.3962 W1_grad [-0.41 -0.   -0.21 -0.    0.21  0.  ] W2_grad [-0.38 -0.  ] b1_grad [-0.41 -0.  ] b2_grad [-0.89]\n",
            "Epoch: 22 loss: 0.3826 W1_grad [-0.41 -0.   -0.2  -0.    0.2   0.  ] W2_grad [-0.38 -0.  ] b1_grad [-0.41 -0.  ] b2_grad [-0.87]\n",
            "Epoch: 23 loss: 0.3694 W1_grad [-0.41 -0.   -0.2  -0.    0.2   0.  ] W2_grad [-0.38 -0.  ] b1_grad [-0.41 -0.  ] b2_grad [-0.86]\n",
            "Epoch: 24 loss: 0.3565 W1_grad [-0.4 -0.  -0.2 -0.   0.2  0. ] W2_grad [-0.39 -0.  ] b1_grad [-0.4 -0. ] b2_grad [-0.84]\n",
            "Epoch: 25 loss: 0.3439 W1_grad [-0.4 -0.  -0.2 -0.   0.2  0. ] W2_grad [-0.39 -0.  ] b1_grad [-0.4 -0. ] b2_grad [-0.83]\n",
            "Epoch: 26 loss: 0.3317 W1_grad [-0.39 -0.   -0.2  -0.    0.2   0.  ] W2_grad [-0.39 -0.  ] b1_grad [-0.39 -0.  ] b2_grad [-0.81]\n",
            "Epoch: 27 loss: 0.3197 W1_grad [-0.39 -0.   -0.19 -0.    0.19  0.  ] W2_grad [-0.39 -0.  ] b1_grad [-0.39 -0.  ] b2_grad [-0.8]\n",
            "Epoch: 28 loss: 0.3081 W1_grad [-0.39 -0.   -0.19 -0.    0.19  0.  ] W2_grad [-0.39 -0.  ] b1_grad [-0.39 -0.  ] b2_grad [-0.78]\n",
            "Epoch: 29 loss: 0.2968 W1_grad [-0.38 -0.   -0.19 -0.    0.19  0.  ] W2_grad [-0.39 -0.  ] b1_grad [-0.38 -0.  ] b2_grad [-0.77]\n",
            "Epoch: 30 loss: 0.2857 W1_grad [-0.38 -0.   -0.19 -0.    0.19  0.  ] W2_grad [-0.39 -0.  ] b1_grad [-0.38 -0.  ] b2_grad [-0.76]\n",
            "Epoch: 31 loss: 0.275 W1_grad [-0.37 -0.   -0.19 -0.    0.19  0.  ] W2_grad [-0.39 -0.  ] b1_grad [-0.37 -0.  ] b2_grad [-0.74]\n",
            "Epoch: 32 loss: 0.2646 W1_grad [-0.37 -0.   -0.18 -0.    0.18  0.  ] W2_grad [-0.39 -0.  ] b1_grad [-0.37 -0.  ] b2_grad [-0.73]\n",
            "Epoch: 33 loss: 0.2545 W1_grad [-0.36 -0.   -0.18 -0.    0.18  0.  ] W2_grad [-0.39 -0.  ] b1_grad [-0.36 -0.  ] b2_grad [-0.71]\n",
            "Epoch: 34 loss: 0.2446 W1_grad [-0.36 -0.   -0.18 -0.    0.18  0.  ] W2_grad [-0.39 -0.  ] b1_grad [-0.36 -0.  ] b2_grad [-0.7]\n",
            "Epoch: 35 loss: 0.2351 W1_grad [-0.36 -0.   -0.18 -0.    0.18  0.  ] W2_grad [-0.39 -0.  ] b1_grad [-0.36 -0.  ] b2_grad [-0.69]\n",
            "Epoch: 36 loss: 0.2258 W1_grad [-0.35 -0.   -0.18 -0.    0.18  0.  ] W2_grad [-0.38 -0.  ] b1_grad [-0.35 -0.  ] b2_grad [-0.67]\n",
            "Epoch: 37 loss: 0.2168 W1_grad [-0.35 -0.   -0.17 -0.    0.17  0.  ] W2_grad [-0.38 -0.  ] b1_grad [-0.35 -0.  ] b2_grad [-0.66]\n",
            "Epoch: 38 loss: 0.2081 W1_grad [-0.34 -0.   -0.17 -0.    0.17  0.  ] W2_grad [-0.38 -0.  ] b1_grad [-0.34 -0.  ] b2_grad [-0.65]\n",
            "Epoch: 39 loss: 0.1996 W1_grad [-0.34 -0.   -0.17 -0.    0.17  0.  ] W2_grad [-0.38 -0.  ] b1_grad [-0.34 -0.  ] b2_grad [-0.63]\n",
            "Epoch: 40 loss: 0.1914 W1_grad [-0.33 -0.   -0.17 -0.    0.17  0.  ] W2_grad [-0.37 -0.  ] b1_grad [-0.33 -0.  ] b2_grad [-0.62]\n",
            "Epoch: 41 loss: 0.1835 W1_grad [-0.33 -0.   -0.16 -0.    0.16  0.  ] W2_grad [-0.37 -0.  ] b1_grad [-0.33 -0.  ] b2_grad [-0.61]\n",
            "Epoch: 42 loss: 0.1758 W1_grad [-0.32 -0.   -0.16 -0.    0.16  0.  ] W2_grad [-0.37 -0.  ] b1_grad [-0.32 -0.  ] b2_grad [-0.59]\n",
            "Epoch: 43 loss: 0.1684 W1_grad [-0.32 -0.   -0.16 -0.    0.16  0.  ] W2_grad [-0.37 -0.  ] b1_grad [-0.32 -0.  ] b2_grad [-0.58]\n",
            "Epoch: 44 loss: 0.1612 W1_grad [-0.31 -0.   -0.16 -0.    0.16  0.  ] W2_grad [-0.36 -0.  ] b1_grad [-0.31 -0.  ] b2_grad [-0.57]\n",
            "Epoch: 45 loss: 0.1543 W1_grad [-0.31 -0.   -0.15 -0.    0.15  0.  ] W2_grad [-0.36 -0.  ] b1_grad [-0.31 -0.  ] b2_grad [-0.56]\n",
            "Epoch: 46 loss: 0.1476 W1_grad [-0.3  -0.   -0.15 -0.    0.15  0.  ] W2_grad [-0.35 -0.  ] b1_grad [-0.3 -0. ] b2_grad [-0.54]\n",
            "Epoch: 47 loss: 0.1411 W1_grad [-0.3  -0.   -0.15 -0.    0.15  0.  ] W2_grad [-0.35 -0.  ] b1_grad [-0.3 -0. ] b2_grad [-0.53]\n",
            "Epoch: 48 loss: 0.1349 W1_grad [-0.29 -0.   -0.15 -0.    0.15  0.  ] W2_grad [-0.35 -0.  ] b1_grad [-0.29 -0.  ] b2_grad [-0.52]\n",
            "Epoch: 49 loss: 0.1289 W1_grad [-0.29 -0.   -0.14 -0.    0.14  0.  ] W2_grad [-0.34 -0.  ] b1_grad [-0.29 -0.  ] b2_grad [-0.51]\n",
            "Epoch: 50 loss: 0.1231 W1_grad [-0.28 -0.   -0.14 -0.    0.14  0.  ] W2_grad [-0.34 -0.  ] b1_grad [-0.28 -0.  ] b2_grad [-0.5]\n",
            "Epoch: 51 loss: 0.1175 W1_grad [-0.28 -0.   -0.14 -0.    0.14  0.  ] W2_grad [-0.33 -0.  ] b1_grad [-0.28 -0.  ] b2_grad [-0.48]\n",
            "Epoch: 52 loss: 0.1121 W1_grad [-0.27 -0.   -0.14 -0.    0.14  0.  ] W2_grad [-0.33 -0.  ] b1_grad [-0.27 -0.  ] b2_grad [-0.47]\n",
            "Epoch: 53 loss: 0.1069 W1_grad [-0.27 -0.   -0.13 -0.    0.13  0.  ] W2_grad [-0.33 -0.  ] b1_grad [-0.27 -0.  ] b2_grad [-0.46]\n",
            "Epoch: 54 loss: 0.102 W1_grad [-0.27 -0.   -0.13 -0.    0.13  0.  ] W2_grad [-0.32 -0.  ] b1_grad [-0.27 -0.  ] b2_grad [-0.45]\n",
            "Epoch: 55 loss: 0.0972 W1_grad [-0.26 -0.   -0.13 -0.    0.13  0.  ] W2_grad [-0.32 -0.  ] b1_grad [-0.26 -0.  ] b2_grad [-0.44]\n",
            "Epoch: 56 loss: 0.0926 W1_grad [-0.26 -0.   -0.13 -0.    0.13  0.  ] W2_grad [-0.31 -0.  ] b1_grad [-0.26 -0.  ] b2_grad [-0.43]\n",
            "Epoch: 57 loss: 0.0882 W1_grad [-0.25 -0.   -0.13 -0.    0.13  0.  ] W2_grad [-0.31 -0.  ] b1_grad [-0.25 -0.  ] b2_grad [-0.42]\n",
            "Epoch: 58 loss: 0.084 W1_grad [-0.25 -0.   -0.12 -0.    0.12  0.  ] W2_grad [-0.3 -0. ] b1_grad [-0.25 -0.  ] b2_grad [-0.41]\n",
            "Epoch: 59 loss: 0.0799 W1_grad [-0.24 -0.   -0.12 -0.    0.12  0.  ] W2_grad [-0.3 -0. ] b1_grad [-0.24 -0.  ] b2_grad [-0.4]\n",
            "Epoch: 60 loss: 0.076 W1_grad [-0.24 -0.   -0.12 -0.    0.12  0.  ] W2_grad [-0.29 -0.  ] b1_grad [-0.24 -0.  ] b2_grad [-0.39]\n",
            "Epoch: 61 loss: 0.0723 W1_grad [-0.23 -0.   -0.12 -0.    0.12  0.  ] W2_grad [-0.29 -0.  ] b1_grad [-0.23 -0.  ] b2_grad [-0.38]\n",
            "Epoch: 62 loss: 0.0687 W1_grad [-0.23 -0.   -0.11 -0.    0.11  0.  ] W2_grad [-0.28 -0.  ] b1_grad [-0.23 -0.  ] b2_grad [-0.37]\n",
            "Epoch: 63 loss: 0.0653 W1_grad [-0.22 -0.   -0.11 -0.    0.11  0.  ] W2_grad [-0.28 -0.  ] b1_grad [-0.22 -0.  ] b2_grad [-0.36]\n",
            "Epoch: 64 loss: 0.062 W1_grad [-0.22 -0.   -0.11 -0.    0.11  0.  ] W2_grad [-0.27 -0.  ] b1_grad [-0.22 -0.  ] b2_grad [-0.35]\n",
            "Epoch: 65 loss: 0.0589 W1_grad [-0.21 -0.   -0.11 -0.    0.11  0.  ] W2_grad [-0.27 -0.  ] b1_grad [-0.21 -0.  ] b2_grad [-0.34]\n",
            "Epoch: 66 loss: 0.0559 W1_grad [-0.21 -0.   -0.1  -0.    0.1   0.  ] W2_grad [-0.26 -0.  ] b1_grad [-0.21 -0.  ] b2_grad [-0.33]\n",
            "Epoch: 67 loss: 0.0531 W1_grad [-0.2 -0.  -0.1 -0.   0.1  0. ] W2_grad [-0.26 -0.  ] b1_grad [-0.2 -0. ] b2_grad [-0.33]\n",
            "Epoch: 68 loss: 0.0504 W1_grad [-0.2 -0.  -0.1 -0.   0.1  0. ] W2_grad [-0.25 -0.  ] b1_grad [-0.2 -0. ] b2_grad [-0.32]\n",
            "Epoch: 69 loss: 0.0478 W1_grad [-0.19 -0.   -0.1  -0.    0.1   0.  ] W2_grad [-0.25 -0.  ] b1_grad [-0.19 -0.  ] b2_grad [-0.31]\n",
            "Epoch: 70 loss: 0.0453 W1_grad [-0.19 -0.   -0.1  -0.    0.1   0.  ] W2_grad [-0.24 -0.  ] b1_grad [-0.19 -0.  ] b2_grad [-0.3]\n",
            "Epoch: 71 loss: 0.0429 W1_grad [-0.19 -0.   -0.09 -0.    0.09  0.  ] W2_grad [-0.24 -0.  ] b1_grad [-0.19 -0.  ] b2_grad [-0.29]\n",
            "Epoch: 72 loss: 0.0407 W1_grad [-0.18 -0.   -0.09 -0.    0.09  0.  ] W2_grad [-0.23 -0.  ] b1_grad [-0.18 -0.  ] b2_grad [-0.29]\n",
            "Epoch: 73 loss: 0.0385 W1_grad [-0.18 -0.   -0.09 -0.    0.09  0.  ] W2_grad [-0.23 -0.  ] b1_grad [-0.18 -0.  ] b2_grad [-0.28]\n",
            "Epoch: 74 loss: 0.0365 W1_grad [-0.17 -0.   -0.09 -0.    0.09  0.  ] W2_grad [-0.22 -0.  ] b1_grad [-0.17 -0.  ] b2_grad [-0.27]\n",
            "Epoch: 75 loss: 0.0345 W1_grad [-0.17 -0.   -0.08 -0.    0.08  0.  ] W2_grad [-0.22 -0.  ] b1_grad [-0.17 -0.  ] b2_grad [-0.26]\n",
            "Epoch: 76 loss: 0.0327 W1_grad [-0.17 -0.   -0.08 -0.    0.08  0.  ] W2_grad [-0.21 -0.  ] b1_grad [-0.17 -0.  ] b2_grad [-0.26]\n",
            "Epoch: 77 loss: 0.0309 W1_grad [-0.16 -0.   -0.08 -0.    0.08  0.  ] W2_grad [-0.21 -0.  ] b1_grad [-0.16 -0.  ] b2_grad [-0.25]\n",
            "Epoch: 78 loss: 0.0292 W1_grad [-0.16 -0.   -0.08 -0.    0.08  0.  ] W2_grad [-0.2 -0. ] b1_grad [-0.16 -0.  ] b2_grad [-0.24]\n",
            "Epoch: 79 loss: 0.0276 W1_grad [-0.15 -0.   -0.08 -0.    0.08  0.  ] W2_grad [-0.2 -0. ] b1_grad [-0.15 -0.  ] b2_grad [-0.24]\n",
            "Epoch: 80 loss: 0.0261 W1_grad [-0.15 -0.   -0.07 -0.    0.07  0.  ] W2_grad [-0.19 -0.  ] b1_grad [-0.15 -0.  ] b2_grad [-0.23]\n",
            "Epoch: 81 loss: 0.0247 W1_grad [-0.15 -0.   -0.07 -0.    0.07  0.  ] W2_grad [-0.19 -0.  ] b1_grad [-0.15 -0.  ] b2_grad [-0.22]\n",
            "Epoch: 82 loss: 0.0233 W1_grad [-0.14 -0.   -0.07 -0.    0.07  0.  ] W2_grad [-0.18 -0.  ] b1_grad [-0.14 -0.  ] b2_grad [-0.22]\n",
            "Epoch: 83 loss: 0.022 W1_grad [-0.14 -0.   -0.07 -0.    0.07  0.  ] W2_grad [-0.18 -0.  ] b1_grad [-0.14 -0.  ] b2_grad [-0.21]\n",
            "Epoch: 84 loss: 0.0208 W1_grad [-0.13 -0.   -0.07 -0.    0.07  0.  ] W2_grad [-0.18 -0.  ] b1_grad [-0.13 -0.  ] b2_grad [-0.2]\n",
            "Epoch: 85 loss: 0.0196 W1_grad [-0.13 -0.   -0.07 -0.    0.07  0.  ] W2_grad [-0.17 -0.  ] b1_grad [-0.13 -0.  ] b2_grad [-0.2]\n",
            "Epoch: 86 loss: 0.0185 W1_grad [-0.13 -0.   -0.06 -0.    0.06  0.  ] W2_grad [-0.17 -0.  ] b1_grad [-0.13 -0.  ] b2_grad [-0.19]\n",
            "Epoch: 87 loss: 0.0175 W1_grad [-0.12 -0.   -0.06 -0.    0.06  0.  ] W2_grad [-0.16 -0.  ] b1_grad [-0.12 -0.  ] b2_grad [-0.19]\n",
            "Epoch: 88 loss: 0.0165 W1_grad [-0.12 -0.   -0.06 -0.    0.06  0.  ] W2_grad [-0.16 -0.  ] b1_grad [-0.12 -0.  ] b2_grad [-0.18]\n",
            "Epoch: 89 loss: 0.0156 W1_grad [-0.12 -0.   -0.06 -0.    0.06  0.  ] W2_grad [-0.15 -0.  ] b1_grad [-0.12 -0.  ] b2_grad [-0.18]\n",
            "Epoch: 90 loss: 0.0147 W1_grad [-0.12 -0.   -0.06 -0.    0.06  0.  ] W2_grad [-0.15 -0.  ] b1_grad [-0.12 -0.  ] b2_grad [-0.17]\n",
            "Epoch: 91 loss: 0.0138 W1_grad [-0.11 -0.   -0.06 -0.    0.06  0.  ] W2_grad [-0.15 -0.  ] b1_grad [-0.11 -0.  ] b2_grad [-0.17]\n",
            "Epoch: 92 loss: 0.013 W1_grad [-0.11 -0.   -0.05 -0.    0.05  0.  ] W2_grad [-0.14 -0.  ] b1_grad [-0.11 -0.  ] b2_grad [-0.16]\n",
            "Epoch: 93 loss: 0.0123 W1_grad [-0.11 -0.   -0.05 -0.    0.05  0.  ] W2_grad [-0.14 -0.  ] b1_grad [-0.11 -0.  ] b2_grad [-0.16]\n",
            "Epoch: 94 loss: 0.0116 W1_grad [-0.1  -0.   -0.05 -0.    0.05  0.  ] W2_grad [-0.14 -0.  ] b1_grad [-0.1 -0. ] b2_grad [-0.15]\n",
            "Epoch: 95 loss: 0.0109 W1_grad [-0.1  -0.   -0.05 -0.    0.05  0.  ] W2_grad [-0.13 -0.  ] b1_grad [-0.1 -0. ] b2_grad [-0.15]\n",
            "Epoch: 96 loss: 0.0103 W1_grad [-0.1  -0.   -0.05 -0.    0.05  0.  ] W2_grad [-0.13 -0.  ] b1_grad [-0.1 -0. ] b2_grad [-0.14]\n",
            "Epoch: 97 loss: 0.0097 W1_grad [-0.09 -0.   -0.05 -0.    0.05  0.  ] W2_grad [-0.12 -0.  ] b1_grad [-0.09 -0.  ] b2_grad [-0.14]\n",
            "Epoch: 98 loss: 0.0091 W1_grad [-0.09 -0.   -0.05 -0.    0.05  0.  ] W2_grad [-0.12 -0.  ] b1_grad [-0.09 -0.  ] b2_grad [-0.13]\n",
            "Epoch: 99 loss: 0.0086 W1_grad [-0.09 -0.   -0.04 -0.    0.04  0.  ] W2_grad [-0.12 -0.  ] b1_grad [-0.09 -0.  ] b2_grad [-0.13]\n",
            "Epoch: 100 loss: 0.0081 W1_grad [-0.09 -0.   -0.04 -0.    0.04  0.  ] W2_grad [-0.11 -0.  ] b1_grad [-0.09 -0.  ] b2_grad [-0.13]\n"
          ]
        }
      ]
    }
  ]
}