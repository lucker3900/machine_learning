{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqob10W+qqSBbW3Pc0eda/"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kD5taScyO1m4"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#计算relu函数\n",
        "def relu(a):\n",
        "  return np.maximum(0, a)"
      ],
      "metadata": {
        "id": "JBsTHM3kO-Z9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#前向传播,求y_pred的值\n",
        "def forward(X, W1, B1, W2, B2, W3, B3):\n",
        "  Z1 = np.dot(W1, X) + B1\n",
        "  H1 = relu(Z1)\n",
        "\n",
        "  Z2 = np.dot(W2, H1) + B2\n",
        "  H2 = relu(Z2)\n",
        "\n",
        "  y_pred = np.dot(W3, H2) + B3\n",
        "\n",
        "  return Z1, H1, Z2, H2, y_pred"
      ],
      "metadata": {
        "id": "k4vKpHYtSDY3"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#求loss函数,\n",
        "def loss(y_pred, y_true):\n",
        "  loss = (y_pred - y_true)**2 / 2\n",
        "  return loss"
      ],
      "metadata": {
        "id": "BU5B43d2YwLO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#反向传播，求梯度\n",
        "def backward(X, Z1, Z2, H1, H2, y_pred, y_true):\n",
        "  B3_grad = y_pred - y_true\n",
        "  W3_grad = B3_grad * H2\n",
        "\n",
        "  B2_grad = B3_grad * W3 * (Z2 > 0)\n",
        "  W2_grad = np.outer(B2_grad, H1)\n",
        "\n",
        "  B1_grad = np.dot(B2_grad, W2) * (Z1 > 0)\n",
        "  W1_grad = np.outer(B1_grad, X)\n",
        "\n",
        "  return W1_grad, W2_grad, W3_grad, B1_grad, B2_grad, B3_grad\n"
      ],
      "metadata": {
        "id": "y5ugSGSnZMwG"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#更新参数值weight, bias\n",
        "def update_params(W1, W2, W3, B1, B2, B3, W1_grad, W2_grad, W3_grad, B1_grad, B2_grad, B3_grad, learning_rate):\n",
        "  W1 -= learning_rate * W1_grad\n",
        "  W2 -= learning_rate * W2_grad\n",
        "  W3 -= learning_rate * W3_grad\n",
        "\n",
        "  B1 -= learning_rate * B1_grad\n",
        "  B2 -= learning_rate * B2_grad\n",
        "  B3 -= learning_rate * B3_grad\n",
        "\n",
        "  return W1, W2, W3, B1, B2, B3"
      ],
      "metadata": {
        "id": "2k8CX7J9eUZQ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#初始化\n",
        "X = np.array([1.0, -2.0, 3.0])\n",
        "\n",
        "W1 = np.array([[0.1, -0.2, 0.3],\n",
        "        [-0.4, 0.5, -0.6],\n",
        "        [0.7, -0.8, 0.9],\n",
        "        [-1.0, 1.1, -1.2]\n",
        "        ])\n",
        "\n",
        "B1 = np.array([0.1, 0.2, 0.0, -0.1])\n",
        "\n",
        "W2 = np.array([[0.2, -0.3, 0.4, -0.5],\n",
        "        [0.5, -0.4, 0.3, -0.2],\n",
        "        [-0.1, 0.2, -0.3, 0.4]\n",
        "        ])\n",
        "\n",
        "B2 = np.array([0.0, -0.2, 0.1])\n",
        "\n",
        "W3 = np.array([0.3, -0.2, 0.1])\n",
        "\n",
        "B3 = np.array([0.5])\n",
        "\n",
        "y_true = 1.5\n",
        "\n",
        "learning_rate = 0.002\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  Z1, H1, Z2, H2, y_pred = forward(X, W1, B1, W2, B2, W3, B3)\n",
        "  loss_value = loss(y_pred, y_true)\n",
        "  W1_grad, W2_grad, W3_grad, B1_grad, B2_grad, B3_grad = backward(X, Z1, Z2, H1, H2, y_pred, y_true)\n",
        "  W1, W2, W3, B1, B2, B3 = update_params(W1, W2, W3, B1, B2, B3, W1_grad, W2_grad, W3_grad, B1_grad, B2_grad, B3_grad, learning_rate)\n",
        "\n",
        "  print(\n",
        "    \"Epoch:\", epoch + 1,\n",
        "    \"Loss:\", np.round(loss_value, 4),\n",
        "    \"y_pred:\", np.round(y_pred, 4),\n",
        "    \"W1_grad:\", np.round(W1_grad.flatten(), 2),\n",
        "    \"W2_grad:\", np.round(W2_grad.flatten(), 2),\n",
        "    \"W3_grad:\", np.round(W3_grad.flatten(), 2),\n",
        "    \"B1_grad:\", np.round(B1_grad.flatten(), 2),\n",
        "    \"B2_grad:\", np.round(B2_grad.flatten(), 2),\n",
        "    \"B3_grad:\", np.round(B3_grad.flatten(), 2),\n",
        "  )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhfPdrGUPntT",
        "outputId": "6cbd1245-a22d-426f-84c6-6dd28ebe6d91"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 Loss: [0.2592] y_pred: [0.78] W1_grad: [ 0.03 -0.06  0.09  0.   -0.    0.   -0.04  0.09 -0.13  0.   -0.    0.  ] W2_grad: [-0.32 -0.   -1.08 -0.    0.22  0.    0.72  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-1.66 -1.48 -0.  ] B1_grad: [ 0.03  0.   -0.04  0.  ] B2_grad: [-0.22  0.14 -0.  ] B3_grad: [-0.72]\n",
            "Epoch: 2 Loss: [0.2446] y_pred: [0.8005] W1_grad: [ 0.03 -0.05  0.08  0.   -0.    0.   -0.04  0.09 -0.13  0.   -0.    0.  ] W2_grad: [-0.32 -0.   -1.06 -0.    0.21  0.    0.69  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-1.62 -1.43 -0.  ] B1_grad: [ 0.03  0.   -0.04  0.  ] B2_grad: [-0.21  0.14 -0.  ] B3_grad: [-0.7]\n",
            "Epoch: 3 Loss: [0.2308] y_pred: [0.8205] W1_grad: [ 0.02 -0.05  0.07  0.   -0.    0.   -0.04  0.09 -0.13  0.   -0.    0.  ] W2_grad: [-0.31 -0.   -1.04 -0.    0.2   0.    0.66  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-1.58 -1.38 -0.  ] B1_grad: [ 0.02  0.   -0.04  0.  ] B2_grad: [-0.21  0.13 -0.  ] B3_grad: [-0.68]\n",
            "Epoch: 4 Loss: [0.2178] y_pred: [0.8401] W1_grad: [ 0.02 -0.04  0.07  0.   -0.    0.   -0.05  0.09 -0.14  0.   -0.    0.  ] W2_grad: [-0.31 -0.   -1.02 -0.    0.19  0.    0.63  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-1.54 -1.34 -0.  ] B1_grad: [ 0.02  0.   -0.05  0.  ] B2_grad: [-0.2   0.13 -0.  ] B3_grad: [-0.66]\n",
            "Epoch: 5 Loss: [0.2054] y_pred: [0.8591] W1_grad: [ 0.02 -0.04  0.06  0.   -0.    0.   -0.05  0.09 -0.14  0.   -0.    0.  ] W2_grad: [-0.3  -0.   -1.   -0.    0.18  0.    0.61  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-1.51 -1.29 -0.  ] B1_grad: [ 0.02  0.   -0.05  0.  ] B2_grad: [-0.2   0.12 -0.  ] B3_grad: [-0.64]\n",
            "Epoch: 6 Loss: [0.1937] y_pred: [0.8777] W1_grad: [ 0.02 -0.04  0.05  0.   -0.    0.   -0.05  0.09 -0.14  0.   -0.    0.  ] W2_grad: [-0.29 -0.   -0.98 -0.    0.17  0.    0.58  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-1.47 -1.25 -0.  ] B1_grad: [ 0.02  0.   -0.05  0.  ] B2_grad: [-0.2   0.12 -0.  ] B3_grad: [-0.62]\n",
            "Epoch: 7 Loss: [0.1826] y_pred: [0.8958] W1_grad: [ 0.02 -0.03  0.05  0.   -0.    0.   -0.05  0.09 -0.14  0.   -0.    0.  ] W2_grad: [-0.29 -0.   -0.96 -0.    0.17  0.    0.56  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-1.43 -1.21 -0.  ] B1_grad: [ 0.02  0.   -0.05  0.  ] B2_grad: [-0.19  0.11 -0.  ] B3_grad: [-0.6]\n",
            "Epoch: 8 Loss: [0.172] y_pred: [0.9134] W1_grad: [ 0.01 -0.03  0.04  0.   -0.    0.   -0.05  0.09 -0.14  0.   -0.    0.  ] W2_grad: [-0.28 -0.   -0.95 -0.    0.16  0.    0.53  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-1.4  -1.17 -0.  ] B1_grad: [ 0.01  0.   -0.05  0.  ] B2_grad: [-0.19  0.11 -0.  ] B3_grad: [-0.59]\n",
            "Epoch: 9 Loss: [0.1621] y_pred: [0.9306] W1_grad: [ 0.01 -0.03  0.04  0.   -0.    0.   -0.05  0.09 -0.14  0.   -0.    0.  ] W2_grad: [-0.28 -0.   -0.93 -0.    0.15  0.    0.51  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-1.36 -1.14 -0.  ] B1_grad: [ 0.01  0.   -0.05  0.  ] B2_grad: [-0.18  0.1  -0.  ] B3_grad: [-0.57]\n",
            "Epoch: 10 Loss: [0.1527] y_pred: [0.9474] W1_grad: [ 0.01 -0.02  0.03  0.   -0.    0.   -0.05  0.09 -0.14  0.   -0.    0.  ] W2_grad: [-0.27 -0.   -0.91 -0.    0.15  0.    0.49  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-1.33 -1.1  -0.  ] B1_grad: [ 0.01  0.   -0.05  0.  ] B2_grad: [-0.18  0.1  -0.  ] B3_grad: [-0.55]\n",
            "Epoch: 11 Loss: [0.1438] y_pred: [0.9637] W1_grad: [ 0.01 -0.02  0.03  0.   -0.    0.   -0.05  0.09 -0.14  0.   -0.    0.  ] W2_grad: [-0.26 -0.   -0.89 -0.    0.14  0.    0.47  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-1.3  -1.06 -0.  ] B1_grad: [ 0.01  0.   -0.05  0.  ] B2_grad: [-0.18  0.09 -0.  ] B3_grad: [-0.54]\n",
            "Epoch: 12 Loss: [0.1354] y_pred: [0.9797] W1_grad: [ 0.01 -0.02  0.03  0.   -0.    0.   -0.05  0.09 -0.14  0.   -0.    0.  ] W2_grad: [-0.26 -0.   -0.87 -0.    0.13  0.    0.45  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-1.26 -1.03 -0.  ] B1_grad: [ 0.01  0.   -0.05  0.  ] B2_grad: [-0.17  0.09 -0.  ] B3_grad: [-0.52]\n",
            "Epoch: 13 Loss: [0.1274] y_pred: [0.9952] W1_grad: [ 0.01 -0.02  0.02  0.   -0.    0.   -0.05  0.09 -0.14  0.   -0.    0.  ] W2_grad: [-0.25 -0.   -0.85 -0.    0.13  0.    0.43  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-1.23 -1.   -0.  ] B1_grad: [ 0.01  0.   -0.05  0.  ] B2_grad: [-0.17  0.09 -0.  ] B3_grad: [-0.5]\n",
            "Epoch: 14 Loss: [0.1199] y_pred: [1.0104] W1_grad: [ 0.01 -0.01  0.02  0.   -0.    0.   -0.05  0.09 -0.14  0.   -0.    0.  ] W2_grad: [-0.25 -0.   -0.83 -0.    0.12  0.    0.41  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-1.2  -0.96 -0.  ] B1_grad: [ 0.01  0.   -0.05  0.  ] B2_grad: [-0.17  0.08 -0.  ] B3_grad: [-0.49]\n",
            "Epoch: 15 Loss: [0.1128] y_pred: [1.0251] W1_grad: [ 0.01 -0.01  0.02  0.   -0.    0.   -0.05  0.09 -0.14  0.   -0.    0.  ] W2_grad: [-0.24 -0.   -0.81 -0.    0.12  0.    0.4   0.   -0.   -0.   -0.   -0.  ] W3_grad: [-1.17 -0.93 -0.  ] B1_grad: [ 0.01  0.   -0.05  0.  ] B2_grad: [-0.16  0.08 -0.  ] B3_grad: [-0.47]\n",
            "Epoch: 16 Loss: [0.106] y_pred: [1.0395] W1_grad: [ 0.   -0.01  0.01  0.   -0.    0.   -0.05  0.09 -0.14  0.   -0.    0.  ] W2_grad: [-0.24 -0.   -0.79 -0.    0.11  0.    0.38  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-1.14 -0.9  -0.  ] B1_grad: [ 0.    0.   -0.05  0.  ] B2_grad: [-0.16  0.08 -0.  ] B3_grad: [-0.46]\n",
            "Epoch: 17 Loss: [0.0997] y_pred: [1.0535] W1_grad: [ 0.   -0.01  0.01  0.   -0.    0.   -0.05  0.09 -0.14  0.   -0.    0.  ] W2_grad: [-0.23 -0.   -0.77 -0.    0.11  0.    0.36  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-1.11 -0.87 -0.  ] B1_grad: [ 0.    0.   -0.05  0.  ] B2_grad: [-0.15  0.07 -0.  ] B3_grad: [-0.45]\n",
            "Epoch: 18 Loss: [0.0937] y_pred: [1.0671] W1_grad: [ 0.   -0.01  0.01  0.   -0.    0.   -0.05  0.09 -0.14  0.   -0.    0.  ] W2_grad: [-0.22 -0.   -0.75 -0.    0.1   0.    0.35  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-1.08 -0.85 -0.  ] B1_grad: [ 0.    0.   -0.05  0.  ] B2_grad: [-0.15  0.07 -0.  ] B3_grad: [-0.43]\n",
            "Epoch: 19 Loss: [0.088] y_pred: [1.0804] W1_grad: [ 0.   -0.    0.01  0.   -0.    0.   -0.04  0.09 -0.13  0.   -0.    0.  ] W2_grad: [-0.22 -0.   -0.74 -0.    0.1   0.    0.34  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-1.05 -0.82 -0.  ] B1_grad: [ 0.    0.   -0.04  0.  ] B2_grad: [-0.15  0.07 -0.  ] B3_grad: [-0.42]\n",
            "Epoch: 20 Loss: [0.0827] y_pred: [1.0933] W1_grad: [ 0.   -0.    0.01  0.   -0.    0.   -0.04  0.09 -0.13  0.   -0.    0.  ] W2_grad: [-0.21 -0.   -0.72 -0.    0.1   0.    0.32  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-1.02 -0.79 -0.  ] B1_grad: [ 0.    0.   -0.04  0.  ] B2_grad: [-0.14  0.06 -0.  ] B3_grad: [-0.41]\n",
            "Epoch: 21 Loss: [0.0777] y_pred: [1.1059] W1_grad: [ 0.   -0.    0.    0.   -0.    0.   -0.04  0.09 -0.13  0.   -0.    0.  ] W2_grad: [-0.21 -0.   -0.7  -0.    0.09  0.    0.31  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.99 -0.77 -0.  ] B1_grad: [ 0.    0.   -0.04  0.  ] B2_grad: [-0.14  0.06 -0.  ] B3_grad: [-0.39]\n",
            "Epoch: 22 Loss: [0.0729] y_pred: [1.1181] W1_grad: [ 0.   -0.    0.    0.   -0.    0.   -0.04  0.09 -0.13  0.   -0.    0.  ] W2_grad: [-0.2  -0.   -0.68 -0.    0.09  0.    0.3   0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.96 -0.74 -0.  ] B1_grad: [ 0.    0.   -0.04  0.  ] B2_grad: [-0.14  0.06 -0.  ] B3_grad: [-0.38]\n",
            "Epoch: 23 Loss: [0.0684] y_pred: [1.13] W1_grad: [ 0.   -0.    0.    0.   -0.    0.   -0.04  0.08 -0.13  0.   -0.    0.  ] W2_grad: [-0.2  -0.   -0.66 -0.    0.08  0.    0.28  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.94 -0.72 -0.  ] B1_grad: [ 0.    0.   -0.04  0.  ] B2_grad: [-0.13  0.06 -0.  ] B3_grad: [-0.37]\n",
            "Epoch: 24 Loss: [0.0642] y_pred: [1.1416] W1_grad: [-0.    0.   -0.    0.   -0.    0.   -0.04  0.08 -0.12  0.   -0.    0.  ] W2_grad: [-0.19 -0.   -0.65 -0.    0.08  0.    0.27  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.91 -0.69 -0.  ] B1_grad: [-0.    0.   -0.04  0.  ] B2_grad: [-0.13  0.05 -0.  ] B3_grad: [-0.36]\n",
            "Epoch: 25 Loss: [0.0602] y_pred: [1.1529] W1_grad: [-0.    0.   -0.    0.   -0.    0.   -0.04  0.08 -0.12  0.   -0.    0.  ] W2_grad: [-0.19 -0.   -0.63 -0.    0.08  0.    0.26  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.88 -0.67 -0.  ] B1_grad: [-0.    0.   -0.04  0.  ] B2_grad: [-0.13  0.05 -0.  ] B3_grad: [-0.35]\n",
            "Epoch: 26 Loss: [0.0565] y_pred: [1.1639] W1_grad: [-0.    0.   -0.    0.   -0.    0.   -0.04  0.08 -0.12  0.   -0.    0.  ] W2_grad: [-0.18 -0.   -0.61 -0.    0.07  0.    0.25  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.86 -0.65 -0.  ] B1_grad: [-0.    0.   -0.04  0.  ] B2_grad: [-0.12  0.05 -0.  ] B3_grad: [-0.34]\n",
            "Epoch: 27 Loss: [0.053] y_pred: [1.1745] W1_grad: [-0.    0.   -0.    0.   -0.    0.   -0.04  0.08 -0.12  0.   -0.    0.  ] W2_grad: [-0.18 -0.   -0.6  -0.    0.07  0.    0.24  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.83 -0.63 -0.  ] B1_grad: [-0.    0.   -0.04  0.  ] B2_grad: [-0.12  0.05 -0.  ] B3_grad: [-0.33]\n",
            "Epoch: 28 Loss: [0.0497] y_pred: [1.1849] W1_grad: [-0.    0.   -0.01  0.   -0.    0.   -0.04  0.08 -0.12  0.   -0.    0.  ] W2_grad: [-0.17 -0.   -0.58 -0.    0.07  0.    0.23  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.81 -0.61 -0.  ] B1_grad: [-0.    0.   -0.04  0.  ] B2_grad: [-0.12  0.05 -0.  ] B3_grad: [-0.32]\n",
            "Epoch: 29 Loss: [0.0465] y_pred: [1.1949] W1_grad: [-0.    0.   -0.01  0.   -0.    0.   -0.04  0.08 -0.11  0.   -0.    0.  ] W2_grad: [-0.17 -0.   -0.56 -0.    0.07  0.    0.22  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.79 -0.59 -0.  ] B1_grad: [-0.    0.   -0.04  0.  ] B2_grad: [-0.11  0.04 -0.  ] B3_grad: [-0.31]\n",
            "Epoch: 30 Loss: [0.0436] y_pred: [1.2047] W1_grad: [-0.    0.   -0.01  0.   -0.    0.   -0.04  0.07 -0.11  0.   -0.    0.  ] W2_grad: [-0.16 -0.   -0.55 -0.    0.06  0.    0.21  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.76 -0.57 -0.  ] B1_grad: [-0.    0.   -0.04  0.  ] B2_grad: [-0.11  0.04 -0.  ] B3_grad: [-0.3]\n",
            "Epoch: 31 Loss: [0.0408] y_pred: [1.2142] W1_grad: [-0.    0.01 -0.01  0.   -0.    0.   -0.04  0.07 -0.11  0.   -0.    0.  ] W2_grad: [-0.16 -0.   -0.53 -0.    0.06  0.    0.21  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.74 -0.55 -0.  ] B1_grad: [-0.    0.   -0.04  0.  ] B2_grad: [-0.11  0.04 -0.  ] B3_grad: [-0.29]\n",
            "Epoch: 32 Loss: [0.0382] y_pred: [1.2234] W1_grad: [-0.    0.01 -0.01  0.   -0.    0.   -0.04  0.07 -0.11  0.   -0.    0.  ] W2_grad: [-0.15 -0.   -0.52 -0.    0.06  0.    0.2   0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.72 -0.53 -0.  ] B1_grad: [-0.    0.   -0.04  0.  ] B2_grad: [-0.1   0.04 -0.  ] B3_grad: [-0.28]\n",
            "Epoch: 33 Loss: [0.0358] y_pred: [1.2324] W1_grad: [-0.    0.01 -0.01  0.   -0.    0.   -0.03  0.07 -0.1   0.   -0.    0.  ] W2_grad: [-0.15 -0.   -0.5  -0.    0.06  0.    0.19  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.7  -0.51 -0.  ] B1_grad: [-0.    0.   -0.03  0.  ] B2_grad: [-0.1   0.04 -0.  ] B3_grad: [-0.27]\n",
            "Epoch: 34 Loss: [0.0335] y_pred: [1.2411] W1_grad: [-0.    0.01 -0.01  0.   -0.    0.   -0.03  0.07 -0.1   0.   -0.    0.  ] W2_grad: [-0.14 -0.   -0.49 -0.    0.05  0.    0.18  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.68 -0.49 -0.  ] B1_grad: [-0.    0.   -0.03  0.  ] B2_grad: [-0.1   0.04 -0.  ] B3_grad: [-0.26]\n",
            "Epoch: 35 Loss: [0.0314] y_pred: [1.2495] W1_grad: [-0.    0.01 -0.01  0.   -0.    0.   -0.03  0.07 -0.1   0.   -0.    0.  ] W2_grad: [-0.14 -0.   -0.47 -0.    0.05  0.    0.18  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.65 -0.48 -0.  ] B1_grad: [-0.    0.   -0.03  0.  ] B2_grad: [-0.09  0.03 -0.  ] B3_grad: [-0.25]\n",
            "Epoch: 36 Loss: [0.0293] y_pred: [1.2577] W1_grad: [-0.    0.01 -0.01  0.   -0.    0.   -0.03  0.06 -0.1   0.   -0.    0.  ] W2_grad: [-0.14 -0.   -0.46 -0.    0.05  0.    0.17  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.63 -0.46 -0.  ] B1_grad: [-0.    0.   -0.03  0.  ] B2_grad: [-0.09  0.03 -0.  ] B3_grad: [-0.24]\n",
            "Epoch: 37 Loss: [0.0275] y_pred: [1.2657] W1_grad: [-0.    0.01 -0.01  0.   -0.    0.   -0.03  0.06 -0.09  0.   -0.    0.  ] W2_grad: [-0.13 -0.   -0.45 -0.    0.05  0.    0.16  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.62 -0.45 -0.  ] B1_grad: [-0.    0.   -0.03  0.  ] B2_grad: [-0.09  0.03 -0.  ] B3_grad: [-0.23]\n",
            "Epoch: 38 Loss: [0.0257] y_pred: [1.2734] W1_grad: [-0.    0.01 -0.01  0.   -0.    0.   -0.03  0.06 -0.09  0.   -0.    0.  ] W2_grad: [-0.13 -0.   -0.43 -0.    0.05  0.    0.16  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.6  -0.43 -0.  ] B1_grad: [-0.    0.   -0.03  0.  ] B2_grad: [-0.09  0.03 -0.  ] B3_grad: [-0.23]\n",
            "Epoch: 39 Loss: [0.024] y_pred: [1.2809] W1_grad: [-0.    0.01 -0.01  0.   -0.    0.   -0.03  0.06 -0.09  0.   -0.    0.  ] W2_grad: [-0.12 -0.   -0.42 -0.    0.04  0.    0.15  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.58 -0.42 -0.  ] B1_grad: [-0.    0.   -0.03  0.  ] B2_grad: [-0.08  0.03 -0.  ] B3_grad: [-0.22]\n",
            "Epoch: 40 Loss: [0.0224] y_pred: [1.2881] W1_grad: [-0.    0.01 -0.01  0.   -0.    0.   -0.03  0.06 -0.09  0.   -0.    0.  ] W2_grad: [-0.12 -0.   -0.41 -0.    0.04  0.    0.14  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.56 -0.4  -0.  ] B1_grad: [-0.    0.   -0.03  0.  ] B2_grad: [-0.08  0.03 -0.  ] B3_grad: [-0.21]\n",
            "Epoch: 41 Loss: [0.021] y_pred: [1.2952] W1_grad: [-0.    0.01 -0.01  0.   -0.    0.   -0.03  0.06 -0.09  0.   -0.    0.  ] W2_grad: [-0.12 -0.   -0.4  -0.    0.04  0.    0.14  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.54 -0.39 -0.  ] B1_grad: [-0.    0.   -0.03  0.  ] B2_grad: [-0.08  0.03 -0.  ] B3_grad: [-0.2]\n",
            "Epoch: 42 Loss: [0.0196] y_pred: [1.302] W1_grad: [-0.    0.01 -0.01  0.   -0.    0.   -0.03  0.06 -0.08  0.   -0.    0.  ] W2_grad: [-0.11 -0.   -0.38 -0.    0.04  0.    0.13  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.53 -0.38 -0.  ] B1_grad: [-0.    0.   -0.03  0.  ] B2_grad: [-0.08  0.03 -0.  ] B3_grad: [-0.2]\n",
            "Epoch: 43 Loss: [0.0183] y_pred: [1.3086] W1_grad: [-0.    0.01 -0.01  0.   -0.    0.   -0.03  0.05 -0.08  0.   -0.    0.  ] W2_grad: [-0.11 -0.   -0.37 -0.    0.04  0.    0.13  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.51 -0.36 -0.  ] B1_grad: [-0.    0.   -0.03  0.  ] B2_grad: [-0.07  0.03 -0.  ] B3_grad: [-0.19]\n",
            "Epoch: 44 Loss: [0.0171] y_pred: [1.315] W1_grad: [-0.    0.01 -0.01  0.   -0.    0.   -0.03  0.05 -0.08  0.   -0.    0.  ] W2_grad: [-0.11 -0.   -0.36 -0.    0.04  0.    0.12  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.49 -0.35 -0.  ] B1_grad: [-0.    0.   -0.03  0.  ] B2_grad: [-0.07  0.02 -0.  ] B3_grad: [-0.18]\n",
            "Epoch: 45 Loss: [0.016] y_pred: [1.3212] W1_grad: [-0.    0.01 -0.01  0.   -0.    0.   -0.03  0.05 -0.08  0.   -0.    0.  ] W2_grad: [-0.1  -0.   -0.35 -0.    0.03  0.    0.12  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.48 -0.34 -0.  ] B1_grad: [-0.    0.   -0.03  0.  ] B2_grad: [-0.07  0.02 -0.  ] B3_grad: [-0.18]\n",
            "Epoch: 46 Loss: [0.0149] y_pred: [1.3273] W1_grad: [-0.    0.01 -0.01  0.   -0.    0.   -0.02  0.05 -0.07  0.   -0.    0.  ] W2_grad: [-0.1  -0.   -0.34 -0.    0.03  0.    0.11  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.46 -0.33 -0.  ] B1_grad: [-0.    0.   -0.02  0.  ] B2_grad: [-0.07  0.02 -0.  ] B3_grad: [-0.17]\n",
            "Epoch: 47 Loss: [0.0139] y_pred: [1.3331] W1_grad: [-0.    0.01 -0.01  0.   -0.    0.   -0.02  0.05 -0.07  0.   -0.    0.  ] W2_grad: [-0.1  -0.   -0.33 -0.    0.03  0.    0.11  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.45 -0.32 -0.  ] B1_grad: [-0.    0.   -0.02  0.  ] B2_grad: [-0.06  0.02 -0.  ] B3_grad: [-0.17]\n",
            "Epoch: 48 Loss: [0.013] y_pred: [1.3387] W1_grad: [-0.    0.01 -0.01  0.   -0.    0.   -0.02  0.05 -0.07  0.   -0.    0.  ] W2_grad: [-0.09 -0.   -0.32 -0.    0.03  0.    0.1   0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.43 -0.31 -0.  ] B1_grad: [-0.    0.   -0.02  0.  ] B2_grad: [-0.06  0.02 -0.  ] B3_grad: [-0.16]\n",
            "Epoch: 49 Loss: [0.0121] y_pred: [1.3442] W1_grad: [-0.    0.01 -0.01  0.   -0.    0.   -0.02  0.05 -0.07  0.   -0.    0.  ] W2_grad: [-0.09 -0.   -0.31 -0.    0.03  0.    0.1   0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.42 -0.29 -0.  ] B1_grad: [-0.    0.   -0.02  0.  ] B2_grad: [-0.06  0.02 -0.  ] B3_grad: [-0.16]\n",
            "Epoch: 50 Loss: [0.0113] y_pred: [1.3495] W1_grad: [-0.    0.01 -0.01  0.   -0.    0.   -0.02  0.04 -0.07  0.   -0.    0.  ] W2_grad: [-0.09 -0.   -0.3  -0.    0.03  0.    0.1   0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.4  -0.28 -0.  ] B1_grad: [-0.    0.   -0.02  0.  ] B2_grad: [-0.06  0.02 -0.  ] B3_grad: [-0.15]\n",
            "Epoch: 51 Loss: [0.0106] y_pred: [1.3546] W1_grad: [-0.    0.01 -0.01  0.   -0.    0.   -0.02  0.04 -0.06  0.   -0.    0.  ] W2_grad: [-0.09 -0.   -0.29 -0.    0.03  0.    0.09  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.39 -0.27 -0.  ] B1_grad: [-0.    0.   -0.02  0.  ] B2_grad: [-0.06  0.02 -0.  ] B3_grad: [-0.15]\n",
            "Epoch: 52 Loss: [0.0099] y_pred: [1.3596] W1_grad: [-0.    0.01 -0.01  0.   -0.    0.   -0.02  0.04 -0.06  0.   -0.    0.  ] W2_grad: [-0.08 -0.   -0.28 -0.    0.03  0.    0.09  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.38 -0.27 -0.  ] B1_grad: [-0.    0.   -0.02  0.  ] B2_grad: [-0.06  0.02 -0.  ] B3_grad: [-0.14]\n",
            "Epoch: 53 Loss: [0.0092] y_pred: [1.3644] W1_grad: [-0.    0.01 -0.01  0.   -0.    0.   -0.02  0.04 -0.06  0.   -0.    0.  ] W2_grad: [-0.08 -0.   -0.27 -0.    0.03  0.    0.09  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.37 -0.26 -0.  ] B1_grad: [-0.    0.   -0.02  0.  ] B2_grad: [-0.05  0.02 -0.  ] B3_grad: [-0.14]\n",
            "Epoch: 54 Loss: [0.0086] y_pred: [1.369] W1_grad: [-0.    0.01 -0.01  0.   -0.    0.   -0.02  0.04 -0.06  0.   -0.    0.  ] W2_grad: [-0.08 -0.   -0.26 -0.    0.02  0.    0.08  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.35 -0.25 -0.  ] B1_grad: [-0.    0.   -0.02  0.  ] B2_grad: [-0.05  0.02 -0.  ] B3_grad: [-0.13]\n",
            "Epoch: 55 Loss: [0.008] y_pred: [1.3735] W1_grad: [-0.    0.01 -0.01  0.   -0.    0.   -0.02  0.04 -0.06  0.   -0.    0.  ] W2_grad: [-0.07 -0.   -0.25 -0.    0.02  0.    0.08  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.34 -0.24 -0.  ] B1_grad: [-0.    0.   -0.02  0.  ] B2_grad: [-0.05  0.02 -0.  ] B3_grad: [-0.13]\n",
            "Epoch: 56 Loss: [0.0075] y_pred: [1.3779] W1_grad: [-0.    0.01 -0.01  0.   -0.    0.   -0.02  0.04 -0.06  0.   -0.    0.  ] W2_grad: [-0.07 -0.   -0.24 -0.    0.02  0.    0.08  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.33 -0.23 -0.  ] B1_grad: [-0.    0.   -0.02  0.  ] B2_grad: [-0.05  0.02 -0.  ] B3_grad: [-0.12]\n",
            "Epoch: 57 Loss: [0.007] y_pred: [1.3821] W1_grad: [-0.    0.01 -0.01  0.   -0.    0.   -0.02  0.04 -0.05  0.   -0.    0.  ] W2_grad: [-0.07 -0.   -0.24 -0.    0.02  0.    0.07  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.32 -0.22 -0.  ] B1_grad: [-0.    0.   -0.02  0.  ] B2_grad: [-0.05  0.01 -0.  ] B3_grad: [-0.12]\n",
            "Epoch: 58 Loss: [0.0065] y_pred: [1.3861] W1_grad: [-0.    0.01 -0.01  0.   -0.    0.   -0.02  0.03 -0.05  0.   -0.    0.  ] W2_grad: [-0.07 -0.   -0.23 -0.    0.02  0.    0.07  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.31 -0.21 -0.  ] B1_grad: [-0.    0.   -0.02  0.  ] B2_grad: [-0.05  0.01 -0.  ] B3_grad: [-0.11]\n",
            "Epoch: 59 Loss: [0.006] y_pred: [1.3901] W1_grad: [-0.    0.01 -0.01  0.   -0.    0.   -0.02  0.03 -0.05  0.   -0.    0.  ] W2_grad: [-0.07 -0.   -0.22 -0.    0.02  0.    0.07  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.3  -0.21 -0.  ] B1_grad: [-0.    0.   -0.02  0.  ] B2_grad: [-0.04  0.01 -0.  ] B3_grad: [-0.11]\n",
            "Epoch: 60 Loss: [0.0056] y_pred: [1.3939] W1_grad: [-0.    0.01 -0.01  0.   -0.    0.   -0.02  0.03 -0.05  0.   -0.    0.  ] W2_grad: [-0.06 -0.   -0.21 -0.    0.02  0.    0.07  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.29 -0.2  -0.  ] B1_grad: [-0.    0.   -0.02  0.  ] B2_grad: [-0.04  0.01 -0.  ] B3_grad: [-0.11]\n",
            "Epoch: 61 Loss: [0.0052] y_pred: [1.3976] W1_grad: [-0.    0.01 -0.01  0.   -0.    0.   -0.02  0.03 -0.05  0.   -0.    0.  ] W2_grad: [-0.06 -0.   -0.21 -0.    0.02  0.    0.06  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.28 -0.19 -0.  ] B1_grad: [-0.    0.   -0.02  0.  ] B2_grad: [-0.04  0.01 -0.  ] B3_grad: [-0.1]\n",
            "Epoch: 62 Loss: [0.0049] y_pred: [1.4011] W1_grad: [-0.    0.01 -0.01  0.   -0.    0.   -0.02  0.03 -0.05  0.   -0.    0.  ] W2_grad: [-0.06 -0.   -0.2  -0.    0.02  0.    0.06  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.27 -0.19 -0.  ] B1_grad: [-0.    0.   -0.02  0.  ] B2_grad: [-0.04  0.01 -0.  ] B3_grad: [-0.1]\n",
            "Epoch: 63 Loss: [0.0046] y_pred: [1.4046] W1_grad: [-0.    0.01 -0.01  0.   -0.    0.   -0.01  0.03 -0.04  0.   -0.    0.  ] W2_grad: [-0.06 -0.   -0.19 -0.    0.02  0.    0.06  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.26 -0.18 -0.  ] B1_grad: [-0.    0.   -0.01  0.  ] B2_grad: [-0.04  0.01 -0.  ] B3_grad: [-0.1]\n",
            "Epoch: 64 Loss: [0.0042] y_pred: [1.4079] W1_grad: [-0.    0.01 -0.01  0.   -0.    0.   -0.01  0.03 -0.04  0.   -0.    0.  ] W2_grad: [-0.06 -0.   -0.19 -0.    0.02  0.    0.06  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.25 -0.17 -0.  ] B1_grad: [-0.    0.   -0.01  0.  ] B2_grad: [-0.04  0.01 -0.  ] B3_grad: [-0.09]\n",
            "Epoch: 65 Loss: [0.004] y_pred: [1.4111] W1_grad: [-0.    0.01 -0.01  0.   -0.    0.   -0.01  0.03 -0.04  0.   -0.    0.  ] W2_grad: [-0.05 -0.   -0.18 -0.    0.02  0.    0.05  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.24 -0.17 -0.  ] B1_grad: [-0.    0.   -0.01  0.  ] B2_grad: [-0.04  0.01 -0.  ] B3_grad: [-0.09]\n",
            "Epoch: 66 Loss: [0.0037] y_pred: [1.4142] W1_grad: [-0.    0.01 -0.01  0.   -0.    0.   -0.01  0.03 -0.04  0.   -0.    0.  ] W2_grad: [-0.05 -0.   -0.17 -0.    0.02  0.    0.05  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.23 -0.16 -0.  ] B1_grad: [-0.    0.   -0.01  0.  ] B2_grad: [-0.03  0.01 -0.  ] B3_grad: [-0.09]\n",
            "Epoch: 67 Loss: [0.0034] y_pred: [1.4172] W1_grad: [-0.    0.01 -0.01  0.   -0.    0.   -0.01  0.03 -0.04  0.   -0.    0.  ] W2_grad: [-0.05 -0.   -0.17 -0.    0.01  0.    0.05  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.23 -0.16 -0.  ] B1_grad: [-0.    0.   -0.01  0.  ] B2_grad: [-0.03  0.01 -0.  ] B3_grad: [-0.08]\n",
            "Epoch: 68 Loss: [0.0032] y_pred: [1.4201] W1_grad: [-0.    0.   -0.01  0.   -0.    0.   -0.01  0.03 -0.04  0.   -0.    0.  ] W2_grad: [-0.05 -0.   -0.16 -0.    0.01  0.    0.05  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.22 -0.15 -0.  ] B1_grad: [-0.    0.   -0.01  0.  ] B2_grad: [-0.03  0.01 -0.  ] B3_grad: [-0.08]\n",
            "Epoch: 69 Loss: [0.003] y_pred: [1.4229] W1_grad: [-0.    0.   -0.01  0.   -0.    0.   -0.01  0.02 -0.04  0.   -0.    0.  ] W2_grad: [-0.05 -0.   -0.16 -0.    0.01  0.    0.05  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.21 -0.14 -0.  ] B1_grad: [-0.    0.   -0.01  0.  ] B2_grad: [-0.03  0.01 -0.  ] B3_grad: [-0.08]\n",
            "Epoch: 70 Loss: [0.0028] y_pred: [1.4256] W1_grad: [-0.    0.   -0.01  0.   -0.    0.   -0.01  0.02 -0.04  0.   -0.    0.  ] W2_grad: [-0.04 -0.   -0.15 -0.    0.01  0.    0.04  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.2  -0.14 -0.  ] B1_grad: [-0.    0.   -0.01  0.  ] B2_grad: [-0.03  0.01 -0.  ] B3_grad: [-0.07]\n",
            "Epoch: 71 Loss: [0.0026] y_pred: [1.4282] W1_grad: [-0.    0.   -0.01  0.   -0.    0.   -0.01  0.02 -0.03  0.   -0.    0.  ] W2_grad: [-0.04 -0.   -0.15 -0.    0.01  0.    0.04  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.2  -0.13 -0.  ] B1_grad: [-0.    0.   -0.01  0.  ] B2_grad: [-0.03  0.01 -0.  ] B3_grad: [-0.07]\n",
            "Epoch: 72 Loss: [0.0024] y_pred: [1.4307] W1_grad: [-0.    0.   -0.01  0.   -0.    0.   -0.01  0.02 -0.03  0.   -0.    0.  ] W2_grad: [-0.04 -0.   -0.14 -0.    0.01  0.    0.04  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.19 -0.13 -0.  ] B1_grad: [-0.    0.   -0.01  0.  ] B2_grad: [-0.03  0.01 -0.  ] B3_grad: [-0.07]\n",
            "Epoch: 73 Loss: [0.0022] y_pred: [1.4332] W1_grad: [-0.    0.   -0.01  0.   -0.    0.   -0.01  0.02 -0.03  0.   -0.    0.  ] W2_grad: [-0.04 -0.   -0.14 -0.    0.01  0.    0.04  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.18 -0.13 -0.  ] B1_grad: [-0.    0.   -0.01  0.  ] B2_grad: [-0.03  0.01 -0.  ] B3_grad: [-0.07]\n",
            "Epoch: 74 Loss: [0.0021] y_pred: [1.4355] W1_grad: [-0.    0.   -0.01  0.   -0.    0.   -0.01  0.02 -0.03  0.   -0.    0.  ] W2_grad: [-0.04 -0.   -0.13 -0.    0.01  0.    0.04  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.18 -0.12 -0.  ] B1_grad: [-0.    0.   -0.01  0.  ] B2_grad: [-0.03  0.01 -0.  ] B3_grad: [-0.06]\n",
            "Epoch: 75 Loss: [0.0019] y_pred: [1.4378] W1_grad: [-0.    0.   -0.01  0.   -0.    0.   -0.01  0.02 -0.03  0.   -0.    0.  ] W2_grad: [-0.04 -0.   -0.13 -0.    0.01  0.    0.04  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.17 -0.12 -0.  ] B1_grad: [-0.    0.   -0.01  0.  ] B2_grad: [-0.03  0.01 -0.  ] B3_grad: [-0.06]\n",
            "Epoch: 76 Loss: [0.0018] y_pred: [1.44] W1_grad: [-0.    0.   -0.01  0.   -0.    0.   -0.01  0.02 -0.03  0.   -0.    0.  ] W2_grad: [-0.04 -0.   -0.12 -0.    0.01  0.    0.04  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.17 -0.11 -0.  ] B1_grad: [-0.    0.   -0.01  0.  ] B2_grad: [-0.02  0.01 -0.  ] B3_grad: [-0.06]\n",
            "Epoch: 77 Loss: [0.0017] y_pred: [1.4421] W1_grad: [-0.    0.   -0.01  0.   -0.    0.   -0.01  0.02 -0.03  0.   -0.    0.  ] W2_grad: [-0.04 -0.   -0.12 -0.    0.01  0.    0.03  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.16 -0.11 -0.  ] B1_grad: [-0.    0.   -0.01  0.  ] B2_grad: [-0.02  0.01 -0.  ] B3_grad: [-0.06]\n",
            "Epoch: 78 Loss: [0.0016] y_pred: [1.4442] W1_grad: [-0.    0.   -0.01  0.   -0.    0.   -0.01  0.02 -0.03  0.   -0.    0.  ] W2_grad: [-0.03 -0.   -0.12 -0.    0.01  0.    0.03  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.15 -0.1  -0.  ] B1_grad: [-0.    0.   -0.01  0.  ] B2_grad: [-0.02  0.01 -0.  ] B3_grad: [-0.06]\n",
            "Epoch: 79 Loss: [0.0014] y_pred: [1.4462] W1_grad: [-0.    0.   -0.01  0.   -0.    0.   -0.01  0.02 -0.03  0.   -0.    0.  ] W2_grad: [-0.03 -0.   -0.11 -0.    0.01  0.    0.03  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.15 -0.1  -0.  ] B1_grad: [-0.    0.   -0.01  0.  ] B2_grad: [-0.02  0.01 -0.  ] B3_grad: [-0.05]\n",
            "Epoch: 80 Loss: [0.0013] y_pred: [1.4481] W1_grad: [-0.    0.   -0.01  0.   -0.    0.   -0.01  0.02 -0.03  0.   -0.    0.  ] W2_grad: [-0.03 -0.   -0.11 -0.    0.01  0.    0.03  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.14 -0.1  -0.  ] B1_grad: [-0.    0.   -0.01  0.  ] B2_grad: [-0.02  0.01 -0.  ] B3_grad: [-0.05]\n",
            "Epoch: 81 Loss: [0.0013] y_pred: [1.4499] W1_grad: [-0.    0.   -0.01  0.   -0.    0.   -0.01  0.02 -0.02  0.   -0.    0.  ] W2_grad: [-0.03 -0.   -0.1  -0.    0.01  0.    0.03  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.14 -0.09 -0.  ] B1_grad: [-0.    0.   -0.01  0.  ] B2_grad: [-0.02  0.01 -0.  ] B3_grad: [-0.05]\n",
            "Epoch: 82 Loss: [0.0012] y_pred: [1.4517] W1_grad: [-0.    0.   -0.    0.   -0.    0.   -0.01  0.02 -0.02  0.   -0.    0.  ] W2_grad: [-0.03 -0.   -0.1  -0.    0.01  0.    0.03  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.13 -0.09 -0.  ] B1_grad: [-0.    0.   -0.01  0.  ] B2_grad: [-0.02  0.01 -0.  ] B3_grad: [-0.05]\n",
            "Epoch: 83 Loss: [0.0011] y_pred: [1.4534] W1_grad: [-0.    0.   -0.    0.   -0.    0.   -0.01  0.02 -0.02  0.   -0.    0.  ] W2_grad: [-0.03 -0.   -0.1  -0.    0.01  0.    0.03  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.13 -0.09 -0.  ] B1_grad: [-0.    0.   -0.01  0.  ] B2_grad: [-0.02  0.01 -0.  ] B3_grad: [-0.05]\n",
            "Epoch: 84 Loss: [0.001] y_pred: [1.4551] W1_grad: [-0.    0.   -0.    0.   -0.    0.   -0.01  0.01 -0.02  0.   -0.    0.  ] W2_grad: [-0.03 -0.   -0.09 -0.    0.01  0.    0.03  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.12 -0.08 -0.  ] B1_grad: [-0.    0.   -0.01  0.  ] B2_grad: [-0.02  0.01 -0.  ] B3_grad: [-0.04]\n",
            "Epoch: 85 Loss: [0.0009] y_pred: [1.4567] W1_grad: [-0.    0.   -0.    0.   -0.    0.   -0.01  0.01 -0.02  0.   -0.    0.  ] W2_grad: [-0.03 -0.   -0.09 -0.    0.01  0.    0.03  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.12 -0.08 -0.  ] B1_grad: [-0.    0.   -0.01  0.  ] B2_grad: [-0.02  0.01 -0.  ] B3_grad: [-0.04]\n",
            "Epoch: 86 Loss: [0.0009] y_pred: [1.4582] W1_grad: [-0.    0.   -0.    0.   -0.    0.   -0.01  0.01 -0.02  0.   -0.    0.  ] W2_grad: [-0.03 -0.   -0.09 -0.    0.01  0.    0.02  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.12 -0.08 -0.  ] B1_grad: [-0.    0.   -0.01  0.  ] B2_grad: [-0.02  0.   -0.  ] B3_grad: [-0.04]\n",
            "Epoch: 87 Loss: [0.0008] y_pred: [1.4597] W1_grad: [-0.    0.   -0.    0.   -0.    0.   -0.01  0.01 -0.02  0.   -0.    0.  ] W2_grad: [-0.02 -0.   -0.08 -0.    0.01  0.    0.02  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.11 -0.08 -0.  ] B1_grad: [-0.    0.   -0.01  0.  ] B2_grad: [-0.02  0.   -0.  ] B3_grad: [-0.04]\n",
            "Epoch: 88 Loss: [0.0008] y_pred: [1.4611] W1_grad: [-0.    0.   -0.    0.   -0.    0.   -0.01  0.01 -0.02  0.   -0.    0.  ] W2_grad: [-0.02 -0.   -0.08 -0.    0.01  0.    0.02  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.11 -0.07 -0.  ] B1_grad: [-0.    0.   -0.01  0.  ] B2_grad: [-0.02  0.   -0.  ] B3_grad: [-0.04]\n",
            "Epoch: 89 Loss: [0.0007] y_pred: [1.4625] W1_grad: [-0.    0.   -0.    0.   -0.    0.   -0.01  0.01 -0.02  0.   -0.    0.  ] W2_grad: [-0.02 -0.   -0.08 -0.    0.01  0.    0.02  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.1  -0.07 -0.  ] B1_grad: [-0.    0.   -0.01  0.  ] B2_grad: [-0.02  0.   -0.  ] B3_grad: [-0.04]\n",
            "Epoch: 90 Loss: [0.0007] y_pred: [1.4639] W1_grad: [-0.    0.   -0.    0.   -0.    0.   -0.01  0.01 -0.02  0.   -0.    0.  ] W2_grad: [-0.02 -0.   -0.08 -0.    0.01  0.    0.02  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.1  -0.07 -0.  ] B1_grad: [-0.    0.   -0.01  0.  ] B2_grad: [-0.01  0.   -0.  ] B3_grad: [-0.04]\n",
            "Epoch: 91 Loss: [0.0006] y_pred: [1.4651] W1_grad: [-0.    0.   -0.    0.   -0.    0.   -0.01  0.01 -0.02  0.   -0.    0.  ] W2_grad: [-0.02 -0.   -0.07 -0.    0.01  0.    0.02  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.1  -0.07 -0.  ] B1_grad: [-0.    0.   -0.01  0.  ] B2_grad: [-0.01  0.   -0.  ] B3_grad: [-0.03]\n",
            "Epoch: 92 Loss: [0.0006] y_pred: [1.4664] W1_grad: [-0.    0.   -0.    0.   -0.    0.   -0.01  0.01 -0.02  0.   -0.    0.  ] W2_grad: [-0.02 -0.   -0.07 -0.    0.01  0.    0.02  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.09 -0.06 -0.  ] B1_grad: [-0.    0.   -0.01  0.  ] B2_grad: [-0.01  0.   -0.  ] B3_grad: [-0.03]\n",
            "Epoch: 93 Loss: [0.0005] y_pred: [1.4676] W1_grad: [-0.    0.   -0.    0.   -0.    0.   -0.01  0.01 -0.02  0.   -0.    0.  ] W2_grad: [-0.02 -0.   -0.07 -0.    0.01  0.    0.02  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.09 -0.06 -0.  ] B1_grad: [-0.    0.   -0.01  0.  ] B2_grad: [-0.01  0.   -0.  ] B3_grad: [-0.03]\n",
            "Epoch: 94 Loss: [0.0005] y_pred: [1.4688] W1_grad: [-0.    0.   -0.    0.   -0.    0.   -0.01  0.01 -0.02  0.   -0.    0.  ] W2_grad: [-0.02 -0.   -0.07 -0.    0.01  0.    0.02  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.09 -0.06 -0.  ] B1_grad: [-0.    0.   -0.01  0.  ] B2_grad: [-0.01  0.   -0.  ] B3_grad: [-0.03]\n",
            "Epoch: 95 Loss: [0.0005] y_pred: [1.4699] W1_grad: [-0.    0.   -0.    0.   -0.    0.   -0.01  0.01 -0.02  0.   -0.    0.  ] W2_grad: [-0.02 -0.   -0.06 -0.    0.01  0.    0.02  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.08 -0.06 -0.  ] B1_grad: [-0.    0.   -0.01  0.  ] B2_grad: [-0.01  0.   -0.  ] B3_grad: [-0.03]\n",
            "Epoch: 96 Loss: [0.0004] y_pred: [1.4709] W1_grad: [-0.    0.   -0.    0.   -0.    0.   -0.    0.01 -0.01  0.   -0.    0.  ] W2_grad: [-0.02 -0.   -0.06 -0.    0.    0.    0.02  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.08 -0.05 -0.  ] B1_grad: [-0.  0. -0.  0.] B2_grad: [-0.01  0.   -0.  ] B3_grad: [-0.03]\n",
            "Epoch: 97 Loss: [0.0004] y_pred: [1.472] W1_grad: [-0.    0.   -0.    0.   -0.    0.   -0.    0.01 -0.01  0.   -0.    0.  ] W2_grad: [-0.02 -0.   -0.06 -0.    0.    0.    0.02  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.08 -0.05 -0.  ] B1_grad: [-0.  0. -0.  0.] B2_grad: [-0.01  0.   -0.  ] B3_grad: [-0.03]\n",
            "Epoch: 98 Loss: [0.0004] y_pred: [1.473] W1_grad: [-0.    0.   -0.    0.   -0.    0.   -0.    0.01 -0.01  0.   -0.    0.  ] W2_grad: [-0.02 -0.   -0.06 -0.    0.    0.    0.02  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.07 -0.05 -0.  ] B1_grad: [-0.  0. -0.  0.] B2_grad: [-0.01  0.   -0.  ] B3_grad: [-0.03]\n",
            "Epoch: 99 Loss: [0.0003] y_pred: [1.474] W1_grad: [-0.    0.   -0.    0.   -0.    0.   -0.    0.01 -0.01  0.   -0.    0.  ] W2_grad: [-0.02 -0.   -0.05 -0.    0.    0.    0.02  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.07 -0.05 -0.  ] B1_grad: [-0.  0. -0.  0.] B2_grad: [-0.01  0.   -0.  ] B3_grad: [-0.03]\n",
            "Epoch: 100 Loss: [0.0003] y_pred: [1.4749] W1_grad: [-0.    0.   -0.    0.   -0.    0.   -0.    0.01 -0.01  0.   -0.    0.  ] W2_grad: [-0.02 -0.   -0.05 -0.    0.    0.    0.01  0.   -0.   -0.   -0.   -0.  ] W3_grad: [-0.07 -0.05 -0.  ] B1_grad: [-0.  0. -0.  0.] B2_grad: [-0.01  0.   -0.  ] B3_grad: [-0.03]\n"
          ]
        }
      ]
    }
  ]
}