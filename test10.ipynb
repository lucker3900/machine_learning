{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6rfveVg7iRRpIOzhBcZcS"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "id7UscS2Yj3Z"
      },
      "outputs": [],
      "source": [
        "#考虑一个用于多分类任务的神经网络，其结构如下：\n",
        "\n",
        "#输入层： 3 个神经元\n",
        "#隐藏层： 4 个神经元，使用 tanh 激活函数\n",
        "#输出层： 4 个神经元，使用 Softmax 激活函数"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "DGxAom4Tarhl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sigmoid激活函数计算\n",
        "def softmax(a):\n",
        "  max_value = np.max(a, axis=1, keepdims=True) # 求出每行的最大值\n",
        "  shifted = a - max_value # 每行减去最大值\n",
        "  z = np.exp(shifted) # 指数转换\n",
        "\n",
        "  sums = np.sum(z, axis=1, keepdims=True) #这一行的总和\n",
        "  return z / sums"
      ],
      "metadata": {
        "id": "rAAIyN_RavZo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tanh激活函数计算\n",
        "def tanh(a):\n",
        "  return np.tanh(a)"
      ],
      "metadata": {
        "id": "maHKPTlxcKM6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#正向传播,求输出函数y_pred\n",
        "def forward(W1, W2, b1, b2, X):\n",
        "  Z = np.dot(X, W1.T) + b1.T # shape(2,4)\n",
        "  H = tanh(Z) # ndarray (2,4)\n",
        "\n",
        "  Zo = np.dot(H, W2.T) + b2.T # ndarray (2,4)\n",
        "  y_pred = softmax(Zo) # ndarray (2,4)\n",
        "\n",
        "  return H, y_pred"
      ],
      "metadata": {
        "id": "4CN998YAgWbs"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#求损失函数loss\n",
        "def compute_loss(y_pred, y_true):\n",
        "  loss = -np.sum(y_true * np.log(y_pred + 1e-10)) / y_pred.shape[0]\n",
        "  return loss"
      ],
      "metadata": {
        "id": "bXgvUQnFicok"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#求反向传播，计算梯度\n",
        "def backward(y_pred, y_true, H, W2, X):\n",
        "  y_diff = y_pred - y_true\n",
        "\n",
        "  dzo = y_diff # shape (2,4)\n",
        "  b2_grad = np.mean(y_diff, axis=0, keepdims=True).T #shape (4,1)\n",
        "  W2_grad = np.dot(H.T, dzo) #shape\n",
        "\n",
        "  dh = np.dot(dzo, W2) # ndarray (2,4)\n",
        "  dz = 1 - np.tanh(dh)**2 # ndarray (2,4)\n",
        "  b1_grad = np.mean(dz, axis=0, keepdims=True).T\n",
        "  W1_grad = np.dot(dz.T, X)\n",
        "\n",
        "  return W1_grad, W2_grad, b1_grad, b2_grad"
      ],
      "metadata": {
        "id": "VeghzSXPkcRl"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#更新参数weight,bias\n",
        "def update_params(W1, W2, b1, b2, W1_grad, W2_grad, b1_grad, b2_grad, learning_rate):\n",
        "  W1 -= learning_rate * W1_grad\n",
        "  W2 -= learning_rate * W2_grad\n",
        "  b1 -= learning_rate * b1_grad\n",
        "  b2 -= learning_rate * b2_grad\n",
        "  return W1, W2, b1, b2"
      ],
      "metadata": {
        "id": "U_AIXDrVrdSW"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([[0.1, 0.2, 0.3],\n",
        "        [0.4, 0.5, 0.6]])\n",
        "\n",
        "y_true = np.array([[1, 0, 0, 0],\n",
        "          [0, 0, 1, 0]])\n",
        "\n",
        "W1 = np.array([[0.1, 0.2, 0.3],\n",
        "        [0.4, 0.5, 0.6],\n",
        "        [0.7, 0.8, 0.9],\n",
        "        [0.2, 0.3, 0.1]]) #(输入层到隐藏层)\n",
        "\n",
        "b1 = np.array([[0.1],\n",
        "        [0.2],\n",
        "        [0.3],\n",
        "        [0.1]]) #(隐藏层偏置)\n",
        "\n",
        "W2 = np.array([[0.5, 0.4, 0.3, 0.2],\n",
        "        [0.1, 0.6, 0.7, 0.8],\n",
        "        [0.9, 0.8, 0.7, 0.6],\n",
        "        [0.5, 0.4, 0.3, 0.2]]) #(隐藏层到输出层)\n",
        "\n",
        "b2 = np.array([[0.2],\n",
        "        [0.3],\n",
        "        [0.1],\n",
        "        [0.2]]) #(输出层偏置)\n",
        "\n",
        "learning_rate = 0.03\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  H, y_pred = forward(W1, W2, b1, b2, X)\n",
        "  loss = compute_loss(y_pred, y_true)\n",
        "  W1_grad, W2_grad, b1_grad, b2_grad = backward(y_pred, y_true, H, W2, X)\n",
        "  W1, W2, b1, b2 = update_params(W1, W2, b1, b2, W1_grad, W2_grad, b1_grad, b2_grad, learning_rate)\n",
        "\n",
        "  print(\"Epochs:\", epoch + 1,\n",
        "    \"loss:\", np.round(np.mean(loss), 4),\n",
        "    \"W1_grad:\", np.round(W1_grad.flatten(), 2),\n",
        "    \"W1_grad:\", np.round(W2_grad.flatten(), 2),\n",
        "    \"W1_grad:\", np.round(b1_grad.flatten(), 2),\n",
        "    \"W1_grad:\", np.round(b2_grad.flatten(), 2)\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PkirtzhbTZa",
        "outputId": "d5f191d2-6c83-4456-a688-aeb004930f6f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 1 loss: 1.3271 W1_grad: [0.45 0.64 0.82 0.48 0.68 0.87 0.49 0.68 0.87 0.49 0.68 0.87] W1_grad: [-0.13  0.2  -0.17  0.11 -0.27  0.38 -0.31  0.21 -0.4   0.48 -0.36  0.27\n",
            " -0.11  0.18 -0.17  0.1 ] W1_grad: [0.94 0.97 0.96 0.95] W1_grad: [-0.33  0.31 -0.15  0.17]\n",
            "Epochs: 2 loss: 1.3217 W1_grad: [0.45 0.63 0.82 0.48 0.68 0.87 0.49 0.68 0.87 0.49 0.68 0.87] W1_grad: [-0.1   0.17 -0.16  0.09 -0.24  0.36 -0.32  0.2  -0.37  0.47 -0.37  0.27\n",
            " -0.08  0.15 -0.15  0.08] W1_grad: [0.93 0.97 0.96 0.95] W1_grad: [-0.32  0.31 -0.16  0.18]\n",
            "Epochs: 3 loss: 1.3173 W1_grad: [0.45 0.63 0.82 0.48 0.68 0.87 0.49 0.68 0.87 0.49 0.68 0.87] W1_grad: [-0.08  0.14 -0.14  0.08 -0.21  0.34 -0.32  0.19 -0.34  0.46 -0.38  0.27\n",
            " -0.06  0.12 -0.13  0.07] W1_grad: [0.93 0.97 0.96 0.96] W1_grad: [-0.31  0.3  -0.17  0.18]\n",
            "Epochs: 4 loss: 1.3138 W1_grad: [0.44 0.63 0.81 0.48 0.68 0.87 0.49 0.68 0.87 0.49 0.68 0.87] W1_grad: [-0.05  0.11 -0.12  0.06 -0.18  0.31 -0.32  0.19 -0.31  0.44 -0.39  0.26\n",
            " -0.03  0.09 -0.11  0.05] W1_grad: [0.93 0.97 0.96 0.96] W1_grad: [-0.31  0.3  -0.18  0.18]\n",
            "Epochs: 5 loss: 1.3112 W1_grad: [0.44 0.63 0.81 0.48 0.68 0.87 0.48 0.68 0.87 0.49 0.68 0.87] W1_grad: [-0.03  0.08 -0.1   0.05 -0.15  0.29 -0.31  0.18 -0.28  0.43 -0.41  0.26\n",
            " -0.01  0.06 -0.09  0.03] W1_grad: [0.93 0.97 0.96 0.96] W1_grad: [-0.3   0.3  -0.19  0.19]\n",
            "Epochs: 6 loss: 1.3096 W1_grad: [0.44 0.62 0.81 0.48 0.68 0.87 0.48 0.68 0.87 0.49 0.68 0.88] W1_grad: [-0.    0.05 -0.08  0.03 -0.12  0.26 -0.31  0.16 -0.25  0.41 -0.42  0.26\n",
            "  0.02  0.03 -0.06  0.02] W1_grad: [0.92 0.97 0.96 0.96] W1_grad: [-0.29  0.3  -0.2   0.19]\n",
            "Epochs: 7 loss: 1.3088 W1_grad: [0.44 0.62 0.81 0.48 0.68 0.87 0.48 0.67 0.86 0.49 0.68 0.88] W1_grad: [ 0.02  0.02 -0.05  0.01 -0.09  0.24 -0.3   0.15 -0.22  0.39 -0.42  0.25\n",
            "  0.04 -0.   -0.03 -0.  ] W1_grad: [0.92 0.97 0.96 0.96] W1_grad: [-0.29  0.29 -0.21  0.2 ]\n",
            "Epochs: 8 loss: 1.3089 W1_grad: [0.43 0.62 0.8  0.49 0.68 0.88 0.48 0.67 0.86 0.49 0.68 0.88] W1_grad: [ 0.04 -0.01 -0.02 -0.01 -0.06  0.21 -0.29  0.14 -0.19  0.37 -0.43  0.25\n",
            "  0.06 -0.03  0.   -0.02] W1_grad: [0.92 0.98 0.95 0.97] W1_grad: [-0.28  0.29 -0.21  0.2 ]\n",
            "Epochs: 9 loss: 1.3096 W1_grad: [0.43 0.62 0.8  0.49 0.68 0.88 0.48 0.67 0.86 0.49 0.69 0.88] W1_grad: [ 0.05 -0.03  0.01 -0.02 -0.04  0.18 -0.27  0.12 -0.16  0.35 -0.43  0.24\n",
            "  0.08 -0.06  0.03 -0.05] W1_grad: [0.92 0.98 0.95 0.97] W1_grad: [-0.27  0.29 -0.22  0.2 ]\n",
            "Epochs: 10 loss: 1.3109 W1_grad: [0.43 0.61 0.8  0.49 0.68 0.88 0.48 0.67 0.86 0.49 0.69 0.88] W1_grad: [ 0.07 -0.06  0.04 -0.04 -0.01  0.16 -0.25  0.11 -0.13  0.33 -0.44  0.23\n",
            "  0.09 -0.09  0.07 -0.07] W1_grad: [0.91 0.98 0.95 0.97] W1_grad: [-0.26  0.29 -0.23  0.21]\n",
            "Epochs: 11 loss: 1.3127 W1_grad: [0.43 0.61 0.79 0.49 0.68 0.88 0.47 0.66 0.85 0.49 0.69 0.88] W1_grad: [ 0.08 -0.09  0.07 -0.07  0.01  0.13 -0.23  0.09 -0.1   0.31 -0.44  0.23\n",
            "  0.11 -0.12  0.1  -0.09] W1_grad: [0.91 0.98 0.95 0.97] W1_grad: [-0.25  0.28 -0.24  0.21]\n",
            "Epochs: 12 loss: 1.3149 W1_grad: [0.43 0.61 0.79 0.49 0.68 0.88 0.47 0.66 0.85 0.49 0.69 0.88] W1_grad: [ 0.1  -0.11  0.1  -0.09  0.03  0.1  -0.2   0.07 -0.07  0.29 -0.43  0.22\n",
            "  0.12 -0.15  0.14 -0.11] W1_grad: [0.91 0.98 0.94 0.97] W1_grad: [-0.25  0.28 -0.25  0.22]\n",
            "Epochs: 13 loss: 1.3173 W1_grad: [0.43 0.61 0.79 0.49 0.69 0.88 0.47 0.66 0.85 0.49 0.69 0.88] W1_grad: [ 0.11 -0.14  0.14 -0.11  0.05  0.07 -0.17  0.05 -0.05  0.27 -0.42  0.2\n",
            "  0.13 -0.17  0.18 -0.13] W1_grad: [0.91 0.98 0.94 0.97] W1_grad: [-0.24  0.28 -0.26  0.22]\n",
            "Epochs: 14 loss: 1.3199 W1_grad: [0.42 0.6  0.79 0.49 0.69 0.88 0.47 0.65 0.84 0.49 0.69 0.88] W1_grad: [ 0.12 -0.16  0.17 -0.13  0.07  0.04 -0.14  0.03 -0.02  0.24 -0.41  0.19\n",
            "  0.14 -0.19  0.21 -0.16] W1_grad: [0.9  0.98 0.94 0.98] W1_grad: [-0.23  0.27 -0.26  0.22]\n",
            "Epochs: 15 loss: 1.3226 W1_grad: [0.42 0.6  0.78 0.49 0.69 0.88 0.46 0.65 0.84 0.49 0.69 0.88] W1_grad: [ 0.12 -0.18  0.2  -0.15  0.08  0.01 -0.1   0.01  0.01  0.22 -0.4   0.18\n",
            "  0.15 -0.21  0.25 -0.18] W1_grad: [0.9  0.98 0.93 0.98] W1_grad: [-0.22  0.27 -0.27  0.23]\n",
            "Epochs: 16 loss: 1.3254 W1_grad: [0.42 0.6  0.78 0.49 0.69 0.89 0.46 0.64 0.83 0.49 0.69 0.89] W1_grad: [ 0.13 -0.2   0.24 -0.17  0.1  -0.02 -0.07 -0.02  0.03  0.2  -0.39  0.16\n",
            "  0.15 -0.23  0.28 -0.2 ] W1_grad: [0.9  0.99 0.93 0.98] W1_grad: [-0.22  0.26 -0.28  0.23]\n",
            "Epochs: 17 loss: 1.3281 W1_grad: [0.42 0.6  0.78 0.49 0.69 0.89 0.46 0.64 0.82 0.49 0.69 0.89] W1_grad: [ 0.14 -0.21  0.27 -0.19  0.11 -0.04 -0.03 -0.04  0.05  0.17 -0.37  0.14\n",
            "  0.16 -0.25  0.32 -0.22] W1_grad: [0.9  0.99 0.92 0.98] W1_grad: [-0.21  0.26 -0.28  0.23]\n",
            "Epochs: 18 loss: 1.3308 W1_grad: [0.42 0.6  0.78 0.49 0.69 0.89 0.45 0.63 0.82 0.5  0.69 0.89] W1_grad: [ 0.14 -0.23  0.3  -0.21  0.12 -0.07  0.01 -0.07  0.07  0.15 -0.34  0.13\n",
            "  0.16 -0.27  0.35 -0.24] W1_grad: [0.9  0.99 0.92 0.98] W1_grad: [-0.2   0.25 -0.29  0.23]\n",
            "Epochs: 19 loss: 1.3336 W1_grad: [0.42 0.6  0.78 0.49 0.69 0.89 0.45 0.63 0.81 0.5  0.69 0.89] W1_grad: [ 0.14 -0.24  0.33 -0.23  0.13 -0.09  0.05 -0.09  0.09  0.12 -0.32  0.11\n",
            "  0.16 -0.28  0.38 -0.26] W1_grad: [0.9  0.99 0.91 0.98] W1_grad: [-0.19  0.25 -0.29  0.24]\n",
            "Epochs: 20 loss: 1.3363 W1_grad: [0.42 0.6  0.77 0.49 0.69 0.89 0.44 0.62 0.8  0.5  0.69 0.89] W1_grad: [ 0.15 -0.26  0.36 -0.25  0.13 -0.12  0.1  -0.11  0.1   0.1  -0.29  0.09\n",
            "  0.17 -0.29  0.41 -0.28] W1_grad: [0.9  0.99 0.9  0.98] W1_grad: [-0.19  0.25 -0.3   0.24]\n",
            "Epochs: 21 loss: 1.3391 W1_grad: [0.42 0.6  0.77 0.49 0.69 0.89 0.44 0.62 0.79 0.5  0.69 0.89] W1_grad: [ 0.15 -0.27  0.39 -0.27  0.14 -0.14  0.14 -0.14  0.12  0.08 -0.26  0.07\n",
            "  0.17 -0.3   0.44 -0.3 ] W1_grad: [0.9  0.99 0.9  0.99] W1_grad: [-0.18  0.24 -0.3   0.24]\n",
            "Epochs: 22 loss: 1.3419 W1_grad: [0.42 0.59 0.77 0.49 0.69 0.89 0.43 0.61 0.79 0.5  0.69 0.89] W1_grad: [ 0.15 -0.28  0.41 -0.28  0.14 -0.16  0.18 -0.16  0.13  0.05 -0.23  0.05\n",
            "  0.17 -0.31  0.46 -0.32] W1_grad: [0.89 0.99 0.89 0.99] W1_grad: [-0.17  0.24 -0.31  0.24]\n",
            "Epochs: 23 loss: 1.3448 W1_grad: [0.42 0.59 0.77 0.5  0.69 0.89 0.42 0.6  0.78 0.5  0.69 0.89] W1_grad: [ 0.15 -0.29  0.44 -0.3   0.14 -0.18  0.22 -0.19  0.14  0.03 -0.2   0.03\n",
            "  0.16 -0.32  0.49 -0.33] W1_grad: [0.89 0.99 0.88 0.99] W1_grad: [-0.16  0.23 -0.31  0.25]\n",
            "Epochs: 24 loss: 1.3479 W1_grad: [0.41 0.59 0.77 0.5  0.69 0.89 0.42 0.59 0.77 0.5  0.69 0.89] W1_grad: [ 0.14 -0.29  0.46 -0.32  0.14 -0.19  0.26 -0.21  0.15  0.01 -0.16  0.\n",
            "  0.16 -0.32  0.51 -0.35] W1_grad: [0.89 0.99 0.88 0.99] W1_grad: [-0.16  0.23 -0.32  0.25]\n",
            "Epochs: 25 loss: 1.3511 W1_grad: [0.41 0.59 0.77 0.5  0.7  0.89 0.41 0.59 0.76 0.5  0.7  0.89] W1_grad: [ 0.14 -0.3   0.49 -0.33  0.14 -0.21  0.3  -0.23  0.15 -0.01 -0.13 -0.02\n",
            "  0.16 -0.33  0.53 -0.36] W1_grad: [0.89 0.99 0.87 0.99] W1_grad: [-0.15  0.22 -0.32  0.25]\n",
            "Epochs: 26 loss: 1.3545 W1_grad: [0.41 0.59 0.77 0.5  0.7  0.89 0.41 0.58 0.75 0.5  0.7  0.89] W1_grad: [ 0.14 -0.3   0.51 -0.35  0.14 -0.22  0.33 -0.25  0.16 -0.03 -0.09 -0.04\n",
            "  0.16 -0.33  0.55 -0.38] W1_grad: [0.89 1.   0.86 0.99] W1_grad: [-0.14  0.22 -0.32  0.25]\n",
            "Epochs: 27 loss: 1.3581 W1_grad: [0.41 0.59 0.77 0.5  0.7  0.9  0.4  0.57 0.74 0.5  0.7  0.89] W1_grad: [ 0.14 -0.31  0.53 -0.36  0.14 -0.23  0.37 -0.28  0.16 -0.05 -0.05 -0.06\n",
            "  0.15 -0.33  0.57 -0.39] W1_grad: [0.89 1.   0.86 0.99] W1_grad: [-0.14  0.21 -0.33  0.25]\n",
            "Epochs: 28 loss: 1.3618 W1_grad: [0.41 0.59 0.77 0.5  0.7  0.9  0.4  0.57 0.74 0.5  0.7  0.89] W1_grad: [ 0.13 -0.31  0.55 -0.37  0.14 -0.24  0.4  -0.29  0.16 -0.07 -0.01 -0.08\n",
            "  0.15 -0.33  0.59 -0.4 ] W1_grad: [0.89 1.   0.85 0.99] W1_grad: [-0.13  0.21 -0.33  0.25]\n",
            "Epochs: 29 loss: 1.3657 W1_grad: [0.41 0.59 0.77 0.5  0.7  0.9  0.39 0.56 0.73 0.5  0.7  0.89] W1_grad: [ 0.13 -0.31  0.56 -0.38  0.13 -0.25  0.43 -0.31  0.16 -0.08  0.03 -0.11\n",
            "  0.15 -0.33  0.6  -0.41] W1_grad: [0.89 1.   0.84 0.99] W1_grad: [-0.12  0.2  -0.34  0.25]\n",
            "Epochs: 30 loss: 1.3697 W1_grad: [0.41 0.59 0.77 0.5  0.7  0.9  0.38 0.55 0.72 0.5  0.7  0.9 ] W1_grad: [ 0.13 -0.31  0.58 -0.4   0.13 -0.26  0.46 -0.33  0.16 -0.1   0.06 -0.13\n",
            "  0.14 -0.33  0.62 -0.42] W1_grad: [0.89 1.   0.83 0.99] W1_grad: [-0.12  0.2  -0.34  0.26]\n",
            "Epochs: 31 loss: 1.3737 W1_grad: [0.42 0.59 0.77 0.5  0.7  0.9  0.38 0.54 0.71 0.5  0.7  0.9 ] W1_grad: [ 0.12 -0.31  0.6  -0.41  0.12 -0.27  0.49 -0.35  0.16 -0.11  0.1  -0.15\n",
            "  0.14 -0.33  0.63 -0.43] W1_grad: [0.89 1.   0.83 0.99] W1_grad: [-0.11  0.2  -0.34  0.26]\n",
            "Epochs: 32 loss: 1.3778 W1_grad: [0.42 0.59 0.77 0.5  0.7  0.9  0.37 0.54 0.7  0.5  0.7  0.9 ] W1_grad: [ 0.12 -0.31  0.61 -0.42  0.12 -0.27  0.52 -0.36  0.15 -0.12  0.14 -0.17\n",
            "  0.13 -0.33  0.64 -0.44] W1_grad: [0.89 1.   0.82 0.99] W1_grad: [-0.11  0.19 -0.34  0.26]\n",
            "Epochs: 33 loss: 1.3818 W1_grad: [0.42 0.59 0.77 0.5  0.7  0.9  0.37 0.53 0.69 0.5  0.7  0.9 ] W1_grad: [ 0.11 -0.31  0.62 -0.42  0.11 -0.28  0.54 -0.38  0.15 -0.14  0.18 -0.19\n",
            "  0.13 -0.33  0.65 -0.45] W1_grad: [0.89 1.   0.82 0.99] W1_grad: [-0.1   0.19 -0.35  0.26]\n",
            "Epochs: 34 loss: 1.3857 W1_grad: [0.42 0.59 0.77 0.5  0.7  0.9  0.36 0.53 0.69 0.5  0.7  0.9 ] W1_grad: [ 0.11 -0.31  0.64 -0.43  0.11 -0.28  0.56 -0.39  0.14 -0.15  0.21 -0.21\n",
            "  0.12 -0.33  0.66 -0.46] W1_grad: [0.89 1.   0.81 0.99] W1_grad: [-0.1   0.19 -0.35  0.26]\n",
            "Epochs: 35 loss: 1.3895 W1_grad: [0.42 0.59 0.77 0.5  0.7  0.9  0.36 0.52 0.68 0.5  0.7  0.9 ] W1_grad: [ 0.1  -0.31  0.65 -0.44  0.1  -0.28  0.58 -0.4   0.14 -0.16  0.25 -0.23\n",
            "  0.12 -0.33  0.67 -0.46] W1_grad: [0.89 1.   0.8  0.99] W1_grad: [-0.09  0.18 -0.35  0.26]\n",
            "Epochs: 36 loss: 1.3931 W1_grad: [0.42 0.6  0.77 0.5  0.7  0.9  0.35 0.51 0.67 0.5  0.7  0.9 ] W1_grad: [ 0.1  -0.31  0.66 -0.45  0.1  -0.28  0.6  -0.41  0.13 -0.17  0.28 -0.24\n",
            "  0.11 -0.32  0.68 -0.47] W1_grad: [0.9  1.   0.8  0.99] W1_grad: [-0.09  0.18 -0.35  0.26]\n",
            "Epochs: 37 loss: 1.3964 W1_grad: [0.42 0.6  0.77 0.5  0.7  0.9  0.35 0.51 0.67 0.5  0.7  0.9 ] W1_grad: [ 0.09 -0.31  0.67 -0.45  0.09 -0.29  0.61 -0.42  0.13 -0.18  0.31 -0.26\n",
            "  0.11 -0.32  0.69 -0.47] W1_grad: [0.9  1.   0.79 0.99] W1_grad: [-0.08  0.18 -0.36  0.26]\n",
            "Epochs: 38 loss: 1.3995 W1_grad: [0.42 0.6  0.77 0.5  0.7  0.9  0.35 0.5  0.66 0.5  0.7  0.9 ] W1_grad: [ 0.09 -0.31  0.67 -0.46  0.09 -0.29  0.63 -0.43  0.12 -0.18  0.34 -0.28\n",
            "  0.1  -0.32  0.69 -0.48] W1_grad: [0.9  1.   0.79 0.99] W1_grad: [-0.08  0.17 -0.36  0.26]\n",
            "Epochs: 39 loss: 1.4023 W1_grad: [0.42 0.6  0.78 0.5  0.7  0.9  0.34 0.5  0.66 0.5  0.7  0.9 ] W1_grad: [ 0.09 -0.3   0.68 -0.46  0.09 -0.29  0.64 -0.44  0.11 -0.19  0.37 -0.29\n",
            "  0.1  -0.32  0.7  -0.48] W1_grad: [0.9  1.   0.78 0.99] W1_grad: [-0.07  0.17 -0.36  0.26]\n",
            "Epochs: 40 loss: 1.4048 W1_grad: [0.42 0.6  0.78 0.5  0.7  0.9  0.34 0.49 0.65 0.5  0.7  0.9 ] W1_grad: [ 0.08 -0.3   0.69 -0.47  0.08 -0.29  0.65 -0.45  0.11 -0.2   0.4  -0.31\n",
            "  0.09 -0.31  0.71 -0.49] W1_grad: [0.9  1.   0.78 1.  ] W1_grad: [-0.07  0.17 -0.36  0.26]\n",
            "Epochs: 41 loss: 1.4069 W1_grad: [0.42 0.6  0.78 0.5  0.7  0.9  0.34 0.49 0.65 0.5  0.7  0.9 ] W1_grad: [ 0.08 -0.3   0.7  -0.47  0.08 -0.29  0.67 -0.45  0.1  -0.2   0.42 -0.32\n",
            "  0.09 -0.31  0.71 -0.49] W1_grad: [0.9  1.   0.78 1.  ] W1_grad: [-0.06  0.17 -0.36  0.26]\n",
            "Epochs: 42 loss: 1.4086 W1_grad: [0.42 0.6  0.78 0.5  0.7  0.9  0.33 0.49 0.64 0.5  0.7  0.9 ] W1_grad: [ 0.07 -0.3   0.7  -0.48  0.07 -0.29  0.67 -0.46  0.09 -0.21  0.45 -0.33\n",
            "  0.09 -0.31  0.71 -0.49] W1_grad: [0.9  1.   0.77 1.  ] W1_grad: [-0.06  0.16 -0.36  0.26]\n",
            "Epochs: 43 loss: 1.4099 W1_grad: [0.42 0.6  0.78 0.5  0.7  0.9  0.33 0.48 0.64 0.5  0.7  0.9 ] W1_grad: [ 0.07 -0.3   0.71 -0.48  0.07 -0.29  0.68 -0.46  0.09 -0.21  0.47 -0.34\n",
            "  0.08 -0.31  0.72 -0.49] W1_grad: [0.9  1.   0.77 1.  ] W1_grad: [-0.06  0.16 -0.36  0.26]\n",
            "Epochs: 44 loss: 1.4108 W1_grad: [0.42 0.6  0.78 0.5  0.7  0.9  0.33 0.48 0.63 0.5  0.7  0.9 ] W1_grad: [ 0.07 -0.29  0.71 -0.48  0.07 -0.29  0.69 -0.47  0.08 -0.22  0.49 -0.35\n",
            "  0.08 -0.3   0.72 -0.5 ] W1_grad: [0.9  1.   0.77 1.  ] W1_grad: [-0.05  0.16 -0.36  0.26]\n",
            "Epochs: 45 loss: 1.4114 W1_grad: [0.42 0.6  0.78 0.5  0.7  0.9  0.32 0.48 0.63 0.5  0.7  0.9 ] W1_grad: [ 0.06 -0.29  0.71 -0.48  0.06 -0.29  0.7  -0.47  0.07 -0.22  0.51 -0.36\n",
            "  0.07 -0.3   0.72 -0.5 ] W1_grad: [0.9  1.   0.76 1.  ] W1_grad: [-0.05  0.16 -0.37  0.26]\n",
            "Epochs: 46 loss: 1.4115 W1_grad: [0.42 0.6  0.78 0.5  0.7  0.9  0.32 0.47 0.63 0.5  0.7  0.9 ] W1_grad: [ 0.06 -0.29  0.72 -0.49  0.06 -0.29  0.7  -0.48  0.07 -0.22  0.53 -0.37\n",
            "  0.07 -0.3   0.73 -0.5 ] W1_grad: [0.9  1.   0.76 1.  ] W1_grad: [-0.05  0.16 -0.37  0.26]\n",
            "Epochs: 47 loss: 1.4112 W1_grad: [0.42 0.6  0.78 0.5  0.7  0.9  0.32 0.47 0.62 0.5  0.7  0.9 ] W1_grad: [ 0.06 -0.29  0.72 -0.49  0.06 -0.28  0.71 -0.48  0.06 -0.23  0.54 -0.38\n",
            "  0.07 -0.3   0.73 -0.5 ] W1_grad: [0.9  1.   0.76 1.  ] W1_grad: [-0.04  0.15 -0.37  0.26]\n",
            "Epochs: 48 loss: 1.4106 W1_grad: [0.42 0.6  0.78 0.5  0.7  0.9  0.32 0.47 0.62 0.5  0.7  0.9 ] W1_grad: [ 0.05 -0.29  0.72 -0.49  0.05 -0.28  0.71 -0.48  0.06 -0.23  0.56 -0.39\n",
            "  0.06 -0.29  0.73 -0.5 ] W1_grad: [0.9  1.   0.75 1.  ] W1_grad: [-0.04  0.15 -0.37  0.26]\n",
            "Epochs: 49 loss: 1.4096 W1_grad: [0.42 0.6  0.78 0.5  0.7  0.9  0.32 0.47 0.62 0.5  0.7  0.9 ] W1_grad: [ 0.05 -0.29  0.73 -0.49  0.05 -0.28  0.72 -0.48  0.05 -0.23  0.57 -0.4\n",
            "  0.06 -0.29  0.73 -0.5 ] W1_grad: [0.9  1.   0.75 1.  ] W1_grad: [-0.04  0.15 -0.37  0.26]\n",
            "Epochs: 50 loss: 1.4083 W1_grad: [0.42 0.6  0.78 0.5  0.7  0.9  0.32 0.47 0.62 0.5  0.7  0.9 ] W1_grad: [ 0.05 -0.28  0.73 -0.49  0.05 -0.28  0.72 -0.48  0.05 -0.23  0.59 -0.4\n",
            "  0.05 -0.29  0.73 -0.5 ] W1_grad: [0.9  1.   0.75 1.  ] W1_grad: [-0.04  0.15 -0.37  0.26]\n",
            "Epochs: 51 loss: 1.4066 W1_grad: [0.42 0.6  0.78 0.5  0.7  0.9  0.31 0.46 0.61 0.5  0.7  0.9 ] W1_grad: [ 0.04 -0.28  0.73 -0.49  0.04 -0.28  0.72 -0.48  0.04 -0.24  0.6  -0.41\n",
            "  0.05 -0.29  0.73 -0.5 ] W1_grad: [0.9  1.   0.75 1.  ] W1_grad: [-0.03  0.15 -0.37  0.25]\n",
            "Epochs: 52 loss: 1.4046 W1_grad: [0.42 0.6  0.78 0.5  0.7  0.9  0.31 0.46 0.61 0.5  0.7  0.9 ] W1_grad: [ 0.04 -0.28  0.73 -0.49  0.04 -0.28  0.72 -0.49  0.04 -0.24  0.61 -0.41\n",
            "  0.05 -0.29  0.73 -0.5 ] W1_grad: [0.9  1.   0.75 1.  ] W1_grad: [-0.03  0.15 -0.37  0.25]\n",
            "Epochs: 53 loss: 1.4022 W1_grad: [0.42 0.6  0.78 0.5  0.7  0.9  0.31 0.46 0.61 0.5  0.7  0.9 ] W1_grad: [ 0.04 -0.28  0.73 -0.49  0.04 -0.28  0.72 -0.49  0.04 -0.24  0.62 -0.42\n",
            "  0.05 -0.28  0.73 -0.5 ] W1_grad: [0.9  1.   0.75 1.  ] W1_grad: [-0.03  0.14 -0.37  0.25]\n",
            "Epochs: 54 loss: 1.3996 W1_grad: [0.42 0.6  0.78 0.5  0.7  0.9  0.31 0.46 0.61 0.5  0.7  0.9 ] W1_grad: [ 0.04 -0.28  0.73 -0.49  0.04 -0.28  0.73 -0.49  0.03 -0.24  0.63 -0.42\n",
            "  0.04 -0.28  0.73 -0.49] W1_grad: [0.9  1.   0.74 1.  ] W1_grad: [-0.03  0.14 -0.37  0.25]\n",
            "Epochs: 55 loss: 1.3968 W1_grad: [0.42 0.6  0.78 0.5  0.7  0.9  0.31 0.46 0.61 0.5  0.7  0.9 ] W1_grad: [ 0.03 -0.28  0.73 -0.49  0.03 -0.28  0.73 -0.49  0.03 -0.24  0.64 -0.43\n",
            "  0.04 -0.28  0.73 -0.49] W1_grad: [0.9  1.   0.74 1.  ] W1_grad: [-0.03  0.14 -0.37  0.25]\n",
            "Epochs: 56 loss: 1.3937 W1_grad: [0.42 0.6  0.78 0.5  0.7  0.9  0.31 0.46 0.61 0.5  0.7  0.9 ] W1_grad: [ 0.03 -0.28  0.73 -0.49  0.03 -0.27  0.73 -0.48  0.03 -0.24  0.65 -0.43\n",
            "  0.04 -0.28  0.73 -0.49] W1_grad: [0.9  1.   0.74 1.  ] W1_grad: [-0.02  0.14 -0.37  0.25]\n",
            "Epochs: 57 loss: 1.3903 W1_grad: [0.42 0.6  0.78 0.5  0.7  0.9  0.31 0.46 0.6  0.5  0.7  0.9 ] W1_grad: [ 0.03 -0.27  0.73 -0.49  0.03 -0.27  0.73 -0.48  0.02 -0.24  0.65 -0.43\n",
            "  0.03 -0.28  0.73 -0.49] W1_grad: [0.9  1.   0.74 1.  ] W1_grad: [-0.02  0.14 -0.37  0.25]\n",
            "Epochs: 58 loss: 1.3867 W1_grad: [0.42 0.6  0.78 0.5  0.7  0.9  0.31 0.46 0.6  0.5  0.7  0.9 ] W1_grad: [ 0.03 -0.27  0.73 -0.48  0.03 -0.27  0.73 -0.48  0.02 -0.24  0.66 -0.43\n",
            "  0.03 -0.28  0.73 -0.49] W1_grad: [0.9  1.   0.74 1.  ] W1_grad: [-0.02  0.14 -0.37  0.25]\n",
            "Epochs: 59 loss: 1.383 W1_grad: [0.42 0.6  0.78 0.5  0.7  0.9  0.31 0.45 0.6  0.5  0.7  0.9 ] W1_grad: [ 0.02 -0.27  0.73 -0.48  0.02 -0.27  0.73 -0.48  0.02 -0.24  0.66 -0.44\n",
            "  0.03 -0.27  0.73 -0.49] W1_grad: [0.9  1.   0.74 1.  ] W1_grad: [-0.02  0.14 -0.37  0.25]\n",
            "Epochs: 60 loss: 1.379 W1_grad: [0.42 0.6  0.78 0.5  0.7  0.9  0.31 0.45 0.6  0.5  0.7  0.9 ] W1_grad: [ 0.02 -0.27  0.73 -0.48  0.02 -0.27  0.73 -0.48  0.01 -0.25  0.67 -0.44\n",
            "  0.03 -0.27  0.73 -0.49] W1_grad: [0.9  1.   0.74 1.  ] W1_grad: [-0.02  0.14 -0.37  0.25]\n",
            "Epochs: 61 loss: 1.3749 W1_grad: [0.42 0.6  0.78 0.5  0.7  0.9  0.31 0.45 0.6  0.5  0.7  0.9 ] W1_grad: [ 0.02 -0.27  0.73 -0.48  0.02 -0.27  0.73 -0.48  0.01 -0.25  0.67 -0.44\n",
            "  0.03 -0.27  0.73 -0.48] W1_grad: [0.9  1.   0.74 1.  ] W1_grad: [-0.02  0.14 -0.36  0.24]\n",
            "Epochs: 62 loss: 1.3706 W1_grad: [0.42 0.6  0.78 0.5  0.7  0.9  0.31 0.45 0.6  0.5  0.7  0.9 ] W1_grad: [ 0.02 -0.27  0.73 -0.48  0.02 -0.27  0.73 -0.48  0.01 -0.25  0.68 -0.44\n",
            "  0.02 -0.27  0.73 -0.48] W1_grad: [0.9  1.   0.74 1.  ] W1_grad: [-0.02  0.14 -0.36  0.24]\n",
            "Epochs: 63 loss: 1.3662 W1_grad: [0.42 0.6  0.78 0.5  0.7  0.9  0.31 0.45 0.6  0.5  0.7  0.9 ] W1_grad: [ 0.02 -0.27  0.72 -0.48  0.02 -0.27  0.72 -0.48  0.01 -0.25  0.68 -0.44\n",
            "  0.02 -0.27  0.73 -0.48] W1_grad: [0.9  1.   0.74 1.  ] W1_grad: [-0.01  0.13 -0.36  0.24]\n",
            "Epochs: 64 loss: 1.3617 W1_grad: [0.42 0.6  0.78 0.5  0.7  0.9  0.31 0.45 0.6  0.5  0.7  0.9 ] W1_grad: [ 0.02 -0.26  0.72 -0.47  0.02 -0.26  0.72 -0.47  0.01 -0.25  0.68 -0.44\n",
            "  0.02 -0.27  0.72 -0.48] W1_grad: [0.9  1.   0.74 1.  ] W1_grad: [-0.01  0.13 -0.36  0.24]\n",
            "Epochs: 65 loss: 1.3571 W1_grad: [0.42 0.6  0.78 0.5  0.7  0.9  0.3  0.45 0.6  0.5  0.7  0.9 ] W1_grad: [ 0.01 -0.26  0.72 -0.47  0.01 -0.26  0.72 -0.47  0.   -0.25  0.68 -0.44\n",
            "  0.02 -0.27  0.72 -0.48] W1_grad: [0.9  1.   0.74 1.  ] W1_grad: [-0.01  0.13 -0.36  0.24]\n",
            "Epochs: 66 loss: 1.3523 W1_grad: [0.42 0.6  0.78 0.5  0.7  0.9  0.3  0.45 0.6  0.5  0.7  0.9 ] W1_grad: [ 0.01 -0.26  0.72 -0.47  0.01 -0.26  0.72 -0.47  0.   -0.25  0.69 -0.44\n",
            "  0.02 -0.26  0.72 -0.47] W1_grad: [0.9  1.   0.74 1.  ] W1_grad: [-0.01  0.13 -0.36  0.24]\n",
            "Epochs: 67 loss: 1.3475 W1_grad: [0.42 0.6  0.78 0.5  0.7  0.9  0.3  0.45 0.6  0.5  0.7  0.9 ] W1_grad: [ 0.01 -0.26  0.72 -0.47  0.01 -0.26  0.72 -0.47  0.   -0.25  0.69 -0.44\n",
            "  0.02 -0.26  0.72 -0.47] W1_grad: [0.9  1.   0.74 1.  ] W1_grad: [-0.01  0.13 -0.36  0.24]\n",
            "Epochs: 68 loss: 1.3426 W1_grad: [0.42 0.6  0.78 0.5  0.7  0.9  0.3  0.45 0.6  0.5  0.7  0.9 ] W1_grad: [ 0.01 -0.26  0.72 -0.47  0.01 -0.26  0.72 -0.47  0.   -0.25  0.69 -0.44\n",
            "  0.01 -0.26  0.72 -0.47] W1_grad: [0.9  1.   0.74 1.  ] W1_grad: [-0.01  0.13 -0.36  0.24]\n",
            "Epochs: 69 loss: 1.3377 W1_grad: [0.42 0.6  0.78 0.5  0.7  0.9  0.3  0.45 0.6  0.5  0.7  0.9 ] W1_grad: [ 0.01 -0.26  0.71 -0.46  0.01 -0.26  0.71 -0.46 -0.   -0.25  0.69 -0.44\n",
            "  0.01 -0.26  0.71 -0.47] W1_grad: [0.9  1.   0.74 1.  ] W1_grad: [-0.01  0.13 -0.36  0.23]\n",
            "Epochs: 70 loss: 1.3327 W1_grad: [0.42 0.6  0.78 0.5  0.7  0.9  0.3  0.45 0.6  0.5  0.7  0.9 ] W1_grad: [ 0.01 -0.26  0.71 -0.46  0.01 -0.26  0.71 -0.46 -0.   -0.25  0.69 -0.44\n",
            "  0.01 -0.26  0.71 -0.46] W1_grad: [0.9  1.   0.74 1.  ] W1_grad: [-0.01  0.13 -0.36  0.23]\n",
            "Epochs: 71 loss: 1.3276 W1_grad: [0.42 0.6  0.78 0.5  0.7  0.9  0.3  0.45 0.6  0.5  0.7  0.9 ] W1_grad: [ 0.01 -0.26  0.71 -0.46  0.01 -0.26  0.71 -0.46 -0.   -0.25  0.69 -0.44\n",
            "  0.01 -0.26  0.71 -0.46] W1_grad: [0.9  1.   0.74 1.  ] W1_grad: [-0.01  0.13 -0.35  0.23]\n",
            "Epochs: 72 loss: 1.3226 W1_grad: [0.42 0.6  0.78 0.5  0.7  0.9  0.3  0.45 0.6  0.5  0.7  0.9 ] W1_grad: [ 0.01 -0.26  0.71 -0.46  0.01 -0.26  0.71 -0.46 -0.   -0.25  0.69 -0.44\n",
            "  0.01 -0.26  0.71 -0.46] W1_grad: [0.9  1.   0.74 1.  ] W1_grad: [-0.01  0.13 -0.35  0.23]\n",
            "Epochs: 73 loss: 1.3174 W1_grad: [0.42 0.6  0.78 0.5  0.7  0.9  0.3  0.45 0.6  0.5  0.7  0.9 ] W1_grad: [ 0.   -0.25  0.71 -0.46  0.01 -0.25  0.71 -0.46 -0.   -0.25  0.69 -0.44\n",
            "  0.01 -0.26  0.71 -0.46] W1_grad: [0.9  1.   0.74 1.  ] W1_grad: [-0.    0.13 -0.35  0.23]\n",
            "Epochs: 74 loss: 1.3123 W1_grad: [0.42 0.6  0.78 0.5  0.7  0.9  0.31 0.45 0.6  0.5  0.7  0.9 ] W1_grad: [ 0.   -0.25  0.7  -0.45  0.   -0.25  0.7  -0.45 -0.01 -0.24  0.69 -0.44\n",
            "  0.01 -0.25  0.7  -0.45] W1_grad: [0.9  1.   0.74 1.  ] W1_grad: [-0.    0.13 -0.35  0.23]\n",
            "Epochs: 75 loss: 1.3071 W1_grad: [0.42 0.6  0.78 0.5  0.7  0.9  0.31 0.45 0.6  0.5  0.7  0.9 ] W1_grad: [ 0.   -0.25  0.7  -0.45  0.   -0.25  0.7  -0.45 -0.01 -0.24  0.69 -0.44\n",
            "  0.01 -0.25  0.7  -0.45] W1_grad: [0.9  1.   0.74 1.  ] W1_grad: [-0.    0.13 -0.35  0.23]\n",
            "Epochs: 76 loss: 1.302 W1_grad: [0.42 0.6  0.78 0.5  0.7  0.9  0.31 0.45 0.6  0.5  0.7  0.9 ] W1_grad: [ 0.   -0.25  0.7  -0.45  0.   -0.25  0.7  -0.45 -0.01 -0.24  0.69 -0.43\n",
            "  0.   -0.25  0.7  -0.45] W1_grad: [0.9  1.   0.74 1.  ] W1_grad: [-0.    0.13 -0.35  0.23]\n",
            "Epochs: 77 loss: 1.2968 W1_grad: [0.42 0.6  0.78 0.5  0.7  0.9  0.31 0.45 0.6  0.5  0.7  0.9 ] W1_grad: [ 0.   -0.25  0.7  -0.45  0.   -0.25  0.7  -0.45 -0.01 -0.24  0.68 -0.43\n",
            "  0.   -0.25  0.7  -0.45] W1_grad: [0.9  1.   0.74 1.  ] W1_grad: [-0.    0.13 -0.35  0.22]\n",
            "Epochs: 78 loss: 1.2916 W1_grad: [0.42 0.6  0.78 0.5  0.7  0.9  0.31 0.45 0.6  0.5  0.7  0.9 ] W1_grad: [ 0.   -0.25  0.69 -0.44  0.   -0.25  0.69 -0.44 -0.01 -0.24  0.68 -0.43\n",
            "  0.   -0.25  0.69 -0.44] W1_grad: [0.9  1.   0.74 1.  ] W1_grad: [-0.    0.13 -0.35  0.22]\n",
            "Epochs: 79 loss: 1.2865 W1_grad: [0.42 0.6  0.78 0.5  0.7  0.9  0.31 0.45 0.6  0.5  0.7  0.9 ] W1_grad: [ 0.   -0.25  0.69 -0.44  0.   -0.25  0.69 -0.44 -0.01 -0.24  0.68 -0.43\n",
            "  0.   -0.25  0.69 -0.44] W1_grad: [0.9  1.   0.74 1.  ] W1_grad: [-0.    0.12 -0.34  0.22]\n",
            "Epochs: 80 loss: 1.2813 W1_grad: [0.42 0.6  0.78 0.5  0.7  0.9  0.31 0.45 0.6  0.5  0.7  0.9 ] W1_grad: [-0.   -0.25  0.69 -0.44  0.   -0.25  0.69 -0.44 -0.01 -0.24  0.68 -0.43\n",
            "  0.   -0.25  0.69 -0.44] W1_grad: [0.9  1.   0.74 1.  ] W1_grad: [-0.    0.12 -0.34  0.22]\n",
            "Epochs: 81 loss: 1.2762 W1_grad: [0.42 0.6  0.78 0.5  0.7  0.9  0.31 0.45 0.6  0.5  0.7  0.9 ] W1_grad: [-0.   -0.25  0.68 -0.44 -0.   -0.25  0.68 -0.44 -0.01 -0.24  0.68 -0.43\n",
            "  0.   -0.25  0.68 -0.44] W1_grad: [0.9  1.   0.74 1.  ] W1_grad: [-0.    0.12 -0.34  0.22]\n",
            "Epochs: 82 loss: 1.271 W1_grad: [0.42 0.6  0.79 0.5  0.7  0.9  0.31 0.45 0.6  0.5  0.7  0.9 ] W1_grad: [-0.   -0.25  0.68 -0.43 -0.   -0.25  0.68 -0.43 -0.01 -0.24  0.67 -0.43\n",
            " -0.   -0.25  0.68 -0.43] W1_grad: [0.9  1.   0.74 1.  ] W1_grad: [-0.    0.12 -0.34  0.22]\n",
            "Epochs: 83 loss: 1.2659 W1_grad: [0.42 0.6  0.79 0.5  0.7  0.9  0.31 0.45 0.6  0.5  0.7  0.9 ] W1_grad: [-0.   -0.24  0.68 -0.43 -0.   -0.25  0.68 -0.43 -0.01 -0.24  0.67 -0.42\n",
            " -0.   -0.25  0.68 -0.43] W1_grad: [0.9  1.   0.74 1.  ] W1_grad: [-0.    0.12 -0.34  0.22]\n",
            "Epochs: 84 loss: 1.2608 W1_grad: [0.42 0.6  0.79 0.5  0.7  0.9  0.31 0.46 0.6  0.5  0.7  0.9 ] W1_grad: [-0.   -0.24  0.68 -0.43 -0.   -0.24  0.68 -0.43 -0.01 -0.24  0.67 -0.42\n",
            " -0.   -0.24  0.68 -0.43] W1_grad: [0.9  1.   0.74 1.  ] W1_grad: [ 0.    0.12 -0.34  0.22]\n",
            "Epochs: 85 loss: 1.2558 W1_grad: [0.42 0.61 0.79 0.5  0.7  0.9  0.31 0.46 0.6  0.5  0.7  0.9 ] W1_grad: [-0.   -0.24  0.67 -0.43 -0.   -0.24  0.67 -0.43 -0.01 -0.24  0.67 -0.42\n",
            " -0.   -0.24  0.67 -0.43] W1_grad: [0.9  1.   0.74 1.  ] W1_grad: [ 0.    0.12 -0.34  0.21]\n",
            "Epochs: 86 loss: 1.2507 W1_grad: [0.42 0.61 0.79 0.5  0.7  0.9  0.31 0.46 0.6  0.5  0.7  0.9 ] W1_grad: [-0.   -0.24  0.67 -0.42 -0.   -0.24  0.67 -0.42 -0.01 -0.24  0.67 -0.42\n",
            " -0.   -0.24  0.67 -0.43] W1_grad: [0.9  1.   0.74 1.  ] W1_grad: [ 0.    0.12 -0.33  0.21]\n",
            "Epochs: 87 loss: 1.2457 W1_grad: [0.42 0.61 0.79 0.5  0.7  0.9  0.31 0.46 0.6  0.5  0.7  0.9 ] W1_grad: [-0.   -0.24  0.67 -0.42 -0.   -0.24  0.67 -0.42 -0.01 -0.24  0.66 -0.42\n",
            " -0.   -0.24  0.67 -0.42] W1_grad: [0.91 1.   0.74 1.  ] W1_grad: [ 0.    0.12 -0.33  0.21]\n",
            "Epochs: 88 loss: 1.2408 W1_grad: [0.42 0.61 0.79 0.5  0.7  0.9  0.31 0.46 0.6  0.5  0.7  0.9 ] W1_grad: [-0.   -0.24  0.66 -0.42 -0.   -0.24  0.66 -0.42 -0.01 -0.24  0.66 -0.41\n",
            " -0.   -0.24  0.66 -0.42] W1_grad: [0.91 1.   0.74 1.  ] W1_grad: [ 0.    0.12 -0.33  0.21]\n",
            "Epochs: 89 loss: 1.2358 W1_grad: [0.42 0.61 0.79 0.5  0.7  0.9  0.31 0.46 0.6  0.5  0.7  0.9 ] W1_grad: [-0.   -0.24  0.66 -0.42 -0.   -0.24  0.66 -0.42 -0.01 -0.24  0.66 -0.41\n",
            " -0.   -0.24  0.66 -0.42] W1_grad: [0.91 1.   0.74 1.  ] W1_grad: [ 0.    0.12 -0.33  0.21]\n",
            "Epochs: 90 loss: 1.2309 W1_grad: [0.43 0.61 0.79 0.5  0.7  0.9  0.31 0.46 0.61 0.5  0.7  0.9 ] W1_grad: [-0.   -0.24  0.66 -0.41 -0.   -0.24  0.66 -0.41 -0.01 -0.24  0.65 -0.41\n",
            " -0.   -0.24  0.66 -0.42] W1_grad: [0.91 1.   0.74 1.  ] W1_grad: [ 0.    0.12 -0.33  0.21]\n",
            "Epochs: 91 loss: 1.2261 W1_grad: [0.43 0.61 0.79 0.5  0.7  0.9  0.31 0.46 0.61 0.5  0.7  0.9 ] W1_grad: [-0.   -0.24  0.65 -0.41 -0.   -0.24  0.65 -0.41 -0.01 -0.23  0.65 -0.41\n",
            " -0.   -0.24  0.65 -0.41] W1_grad: [0.91 1.   0.74 1.  ] W1_grad: [ 0.    0.12 -0.33  0.21]\n",
            "Epochs: 92 loss: 1.2212 W1_grad: [0.43 0.61 0.79 0.5  0.7  0.9  0.31 0.46 0.61 0.5  0.7  0.9 ] W1_grad: [-0.   -0.24  0.65 -0.41 -0.   -0.24  0.65 -0.41 -0.01 -0.23  0.65 -0.41\n",
            " -0.   -0.24  0.65 -0.41] W1_grad: [0.91 1.   0.74 1.  ] W1_grad: [ 0.    0.12 -0.33  0.21]\n",
            "Epochs: 93 loss: 1.2164 W1_grad: [0.43 0.61 0.79 0.5  0.7  0.9  0.31 0.46 0.61 0.5  0.7  0.9 ] W1_grad: [-0.   -0.24  0.65 -0.41 -0.   -0.24  0.65 -0.41 -0.01 -0.23  0.65 -0.4\n",
            " -0.   -0.24  0.65 -0.41] W1_grad: [0.91 1.   0.74 1.  ] W1_grad: [ 0.    0.12 -0.32  0.2 ]\n",
            "Epochs: 94 loss: 1.2117 W1_grad: [0.43 0.61 0.79 0.5  0.7  0.9  0.31 0.46 0.61 0.5  0.7  0.9 ] W1_grad: [-0.   -0.24  0.64 -0.41 -0.   -0.24  0.64 -0.41 -0.01 -0.23  0.64 -0.4\n",
            " -0.   -0.24  0.64 -0.41] W1_grad: [0.91 1.   0.74 1.  ] W1_grad: [ 0.    0.12 -0.32  0.2 ]\n",
            "Epochs: 95 loss: 1.207 W1_grad: [0.43 0.61 0.79 0.5  0.7  0.9  0.31 0.46 0.61 0.5  0.7  0.9 ] W1_grad: [-0.   -0.23  0.64 -0.4  -0.   -0.23  0.64 -0.4  -0.01 -0.23  0.64 -0.4\n",
            " -0.   -0.23  0.64 -0.4 ] W1_grad: [0.91 1.   0.74 1.  ] W1_grad: [ 0.    0.12 -0.32  0.2 ]\n",
            "Epochs: 96 loss: 1.2023 W1_grad: [0.43 0.61 0.79 0.5  0.7  0.9  0.31 0.46 0.61 0.5  0.7  0.9 ] W1_grad: [-0.   -0.23  0.64 -0.4  -0.   -0.23  0.64 -0.4  -0.01 -0.23  0.64 -0.4\n",
            " -0.   -0.23  0.64 -0.4 ] W1_grad: [0.91 1.   0.74 1.  ] W1_grad: [ 0.    0.12 -0.32  0.2 ]\n",
            "Epochs: 97 loss: 1.1977 W1_grad: [0.43 0.61 0.79 0.5  0.7  0.9  0.31 0.46 0.61 0.5  0.7  0.9 ] W1_grad: [-0.   -0.23  0.63 -0.4  -0.   -0.23  0.63 -0.4  -0.01 -0.23  0.63 -0.4\n",
            " -0.   -0.23  0.63 -0.4 ] W1_grad: [0.91 1.   0.74 1.  ] W1_grad: [ 0.    0.12 -0.32  0.2 ]\n",
            "Epochs: 98 loss: 1.1931 W1_grad: [0.43 0.61 0.79 0.5  0.7  0.9  0.31 0.46 0.61 0.5  0.7  0.9 ] W1_grad: [-0.   -0.23  0.63 -0.4  -0.   -0.23  0.63 -0.4  -0.01 -0.23  0.63 -0.39\n",
            " -0.   -0.23  0.63 -0.4 ] W1_grad: [0.91 1.   0.74 1.  ] W1_grad: [ 0.    0.12 -0.32  0.2 ]\n",
            "Epochs: 99 loss: 1.1885 W1_grad: [0.43 0.61 0.79 0.5  0.7  0.9  0.31 0.46 0.61 0.5  0.7  0.9 ] W1_grad: [-0.   -0.23  0.63 -0.39 -0.   -0.23  0.63 -0.39 -0.01 -0.23  0.63 -0.39\n",
            " -0.   -0.23  0.63 -0.39] W1_grad: [0.91 1.   0.74 1.  ] W1_grad: [ 0.    0.12 -0.31  0.2 ]\n",
            "Epochs: 100 loss: 1.184 W1_grad: [0.43 0.61 0.79 0.5  0.7  0.9  0.31 0.46 0.61 0.5  0.7  0.9 ] W1_grad: [-0.   -0.23  0.62 -0.39 -0.   -0.23  0.62 -0.39 -0.01 -0.23  0.62 -0.39\n",
            " -0.   -0.23  0.62 -0.39] W1_grad: [0.91 1.   0.74 1.  ] W1_grad: [ 0.    0.11 -0.31  0.2 ]\n"
          ]
        }
      ]
    }
  ]
}