{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+2Gz4ehzrysuoW/W7t8NU"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tZppRZL87JA3"
      },
      "outputs": [],
      "source": [
        "#题目描述：\n",
        "\n",
        "#考虑一个用于二元分类任务的简单神经网络，其结构如下：\n",
        "\n",
        "#输入层： 2 个神经元\n",
        "#隐藏层： 2 个神经元，使用 Sigmoid 激活函数\n",
        "#输出层： 1 个神经元，使用 Sigmoid 激活函数"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "WXHKaVjq7OaQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sigmoid激活函数\n",
        "def sigmoid(a):\n",
        "  return 1/(1 + np.exp(-a))"
      ],
      "metadata": {
        "id": "Huh8VgQMCAuy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#前向传播,求得输出y_pred\n",
        "def forward(W1, W2, X, b1, b2):\n",
        "  Z = np.dot(W1, X) + b1 #shape (2,2)\n",
        "  H = sigmoid(Z) #shape (2,2)\n",
        "\n",
        "  Zo = np.dot(H, W2.T) + b2 #shape (2,1)\n",
        "  y_pred = sigmoid(Zo) #shape (2,1)\n",
        "\n",
        "  return Z, H, Zo, y_pred"
      ],
      "metadata": {
        "id": "JBeEH2G7CNVC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#求损失函数loss\n",
        "\n",
        "def compute_loss(y_pred, y_true):\n",
        "  loss = (y_pred - y_true)**2 / 2\n",
        "  return loss"
      ],
      "metadata": {
        "id": "DRr3bA1bFf1o"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#计算反向传播,求梯度\n",
        "def backward(y_pred, y_true, H, W2, X):\n",
        "  y_diff = y_pred - y_true #shape (2,1)\n",
        "\n",
        "  dzo = y_diff * y_pred * (1-y_pred) #shape (2,1)\n",
        "  b2_grad = np.mean(dzo, axis=0, keepdims=True) #shape (1,1)\n",
        "  W2_grad = np.dot(dzo.T, H)\n",
        "\n",
        "  dh = np.dot(dzo, W2) #shape (2,2)\n",
        "  dz = dh*(1-dh) #shape (2,2)\n",
        "  b1_grad = np.mean(dz, axis=0, keepdims=True).T #shape(2,1)\n",
        "  W1_grad = np.dot(dz.T, X) #shape (2,2)\n",
        "\n",
        "  return W1_grad, W2_grad, b1_grad, b2_grad"
      ],
      "metadata": {
        "id": "MxXc5V6cGM7L"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#更新参数weight, bias\n",
        "\n",
        "def update_params(W1, W2, b1, b2, W1_grad, W2_grad, b1_grad, b2_grad, learning_rate):\n",
        "  W1 -= learning_rate * W1_grad\n",
        "  W2 -= learning_rate * W2_grad\n",
        "  b1 -= learning_rate * b1_grad\n",
        "  b2 -= learning_rate * b2_grad\n",
        "\n",
        "  return W1, W2, b1, b2"
      ],
      "metadata": {
        "id": "AmlwNwQULT5y"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([[0.1, 0.2], [0.3, 0.4]])\n",
        "y_true = np.array([[0], [1]])\n",
        "\n",
        "W1 = np.array([[0.1, 0.2], [0.3, 0.4]]) #(输入层到隐藏层)\n",
        "b1 = np.array([[0.1], [0.2]]) #(隐藏层偏置)\n",
        "W2 = np.array([[0.5, 0.6]]) #(隐藏层到输出层)\n",
        "b2 = np.array([[0.7]]) #(输出层偏置)\n",
        "\n",
        "learning_rate = 0.1\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  Z, H, Zo, y_pred = forward(W1, W2, X, b1, b2)\n",
        "  loss = compute_loss(y_pred, y_true)\n",
        "  W1_grad, W2_grad, b1_grad, b2_grad = backward(y_pred, y_true, H, W2, X)\n",
        "  W1, W2, b1, b2 = update_params(W1, W2, b1, b2, W1_grad, W2_grad, b1_grad, b2_grad, learning_rate)\n",
        "\n",
        "  print(\"Epoch:\", epoch + 1,\n",
        "    \"loss:\", np.round(np.mean(loss), 4),\n",
        "    \"W1_grad\", np.round(W1_grad.flatten(), 2),\n",
        "    \"W2_grad\", np.round(W2_grad.flatten(), 2),\n",
        "    \"b1_grad\", np.round(b1_grad.flatten(), 2),\n",
        "    \"b2_grad\", np.round(b2_grad.flatten(), 2),\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2H9WC72k7-8v",
        "outputId": "03000380-9fcc-4084-f944-aba383f87638"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 loss: 0.165 W1_grad [0.   0.01 0.   0.01] W2_grad [0.05 0.05] b1_grad [0.02 0.03] b2_grad [0.05]\n",
            "Epoch: 2 loss: 0.1644 W1_grad [0.   0.01 0.   0.01] W2_grad [0.05 0.05] b1_grad [0.02 0.03] b2_grad [0.05]\n",
            "Epoch: 3 loss: 0.1639 W1_grad [0.   0.01 0.   0.01] W2_grad [0.05 0.05] b1_grad [0.02 0.03] b2_grad [0.05]\n",
            "Epoch: 4 loss: 0.1633 W1_grad [0.   0.01 0.   0.01] W2_grad [0.05 0.05] b1_grad [0.02 0.03] b2_grad [0.05]\n",
            "Epoch: 5 loss: 0.1628 W1_grad [0.   0.01 0.   0.01] W2_grad [0.05 0.05] b1_grad [0.02 0.03] b2_grad [0.05]\n",
            "Epoch: 6 loss: 0.1622 W1_grad [0.   0.   0.   0.01] W2_grad [0.05 0.05] b1_grad [0.02 0.03] b2_grad [0.05]\n",
            "Epoch: 7 loss: 0.1617 W1_grad [0.   0.   0.   0.01] W2_grad [0.05 0.05] b1_grad [0.02 0.02] b2_grad [0.05]\n",
            "Epoch: 8 loss: 0.1612 W1_grad [0.   0.   0.   0.01] W2_grad [0.05 0.05] b1_grad [0.02 0.02] b2_grad [0.05]\n",
            "Epoch: 9 loss: 0.1606 W1_grad [0.   0.   0.   0.01] W2_grad [0.05 0.05] b1_grad [0.02 0.02] b2_grad [0.05]\n",
            "Epoch: 10 loss: 0.1601 W1_grad [0.   0.   0.   0.01] W2_grad [0.05 0.05] b1_grad [0.02 0.02] b2_grad [0.05]\n",
            "Epoch: 11 loss: 0.1596 W1_grad [0.   0.   0.   0.01] W2_grad [0.05 0.05] b1_grad [0.02 0.02] b2_grad [0.05]\n",
            "Epoch: 12 loss: 0.159 W1_grad [0. 0. 0. 0.] W2_grad [0.05 0.05] b1_grad [0.02 0.02] b2_grad [0.05]\n",
            "Epoch: 13 loss: 0.1585 W1_grad [0. 0. 0. 0.] W2_grad [0.05 0.05] b1_grad [0.02 0.02] b2_grad [0.05]\n",
            "Epoch: 14 loss: 0.158 W1_grad [0. 0. 0. 0.] W2_grad [0.05 0.05] b1_grad [0.02 0.02] b2_grad [0.05]\n",
            "Epoch: 15 loss: 0.1575 W1_grad [0. 0. 0. 0.] W2_grad [0.05 0.05] b1_grad [0.02 0.02] b2_grad [0.05]\n",
            "Epoch: 16 loss: 0.157 W1_grad [ 0.  0. -0.  0.] W2_grad [0.05 0.05] b1_grad [0.02 0.02] b2_grad [0.05]\n",
            "Epoch: 17 loss: 0.1565 W1_grad [ 0.  0. -0.  0.] W2_grad [0.05 0.05] b1_grad [0.02 0.02] b2_grad [0.05]\n",
            "Epoch: 18 loss: 0.156 W1_grad [-0.  0. -0.  0.] W2_grad [0.05 0.05] b1_grad [0.02 0.02] b2_grad [0.05]\n",
            "Epoch: 19 loss: 0.1555 W1_grad [-0.  0. -0.  0.] W2_grad [0.05 0.05] b1_grad [0.02 0.02] b2_grad [0.05]\n",
            "Epoch: 20 loss: 0.155 W1_grad [-0.  0. -0.  0.] W2_grad [0.05 0.05] b1_grad [0.02 0.02] b2_grad [0.05]\n",
            "Epoch: 21 loss: 0.1545 W1_grad [-0.  0. -0.  0.] W2_grad [0.05 0.05] b1_grad [0.02 0.02] b2_grad [0.05]\n",
            "Epoch: 22 loss: 0.154 W1_grad [-0.  0. -0.  0.] W2_grad [0.05 0.05] b1_grad [0.02 0.02] b2_grad [0.05]\n",
            "Epoch: 23 loss: 0.1535 W1_grad [-0.  0. -0.  0.] W2_grad [0.05 0.05] b1_grad [0.02 0.02] b2_grad [0.05]\n",
            "Epoch: 24 loss: 0.153 W1_grad [-0.  0. -0.  0.] W2_grad [0.05 0.05] b1_grad [0.02 0.02] b2_grad [0.05]\n",
            "Epoch: 25 loss: 0.1526 W1_grad [-0.  0. -0.  0.] W2_grad [0.05 0.05] b1_grad [0.02 0.02] b2_grad [0.05]\n",
            "Epoch: 26 loss: 0.1521 W1_grad [-0.  0. -0.  0.] W2_grad [0.05 0.05] b1_grad [0.02 0.02] b2_grad [0.05]\n",
            "Epoch: 27 loss: 0.1516 W1_grad [-0.  0. -0.  0.] W2_grad [0.05 0.05] b1_grad [0.02 0.02] b2_grad [0.05]\n",
            "Epoch: 28 loss: 0.1512 W1_grad [-0.  0. -0.  0.] W2_grad [0.05 0.05] b1_grad [0.02 0.02] b2_grad [0.05]\n",
            "Epoch: 29 loss: 0.1507 W1_grad [-0.  0. -0.  0.] W2_grad [0.05 0.05] b1_grad [0.02 0.02] b2_grad [0.05]\n",
            "Epoch: 30 loss: 0.1502 W1_grad [-0.  0. -0.  0.] W2_grad [0.05 0.05] b1_grad [0.02 0.02] b2_grad [0.05]\n",
            "Epoch: 31 loss: 0.1498 W1_grad [-0.  0. -0.  0.] W2_grad [0.05 0.05] b1_grad [0.01 0.02] b2_grad [0.05]\n",
            "Epoch: 32 loss: 0.1494 W1_grad [-0.  0. -0.  0.] W2_grad [0.05 0.05] b1_grad [0.01 0.02] b2_grad [0.05]\n",
            "Epoch: 33 loss: 0.1489 W1_grad [-0.  0. -0.  0.] W2_grad [0.05 0.05] b1_grad [0.01 0.02] b2_grad [0.05]\n",
            "Epoch: 34 loss: 0.1485 W1_grad [-0.  0. -0.  0.] W2_grad [0.05 0.05] b1_grad [0.01 0.02] b2_grad [0.05]\n",
            "Epoch: 35 loss: 0.1481 W1_grad [-0.  0. -0.  0.] W2_grad [0.05 0.05] b1_grad [0.01 0.02] b2_grad [0.05]\n",
            "Epoch: 36 loss: 0.1476 W1_grad [-0.  0. -0.  0.] W2_grad [0.05 0.05] b1_grad [0.01 0.02] b2_grad [0.05]\n",
            "Epoch: 37 loss: 0.1472 W1_grad [-0.  0. -0.  0.] W2_grad [0.04 0.05] b1_grad [0.01 0.02] b2_grad [0.04]\n",
            "Epoch: 38 loss: 0.1468 W1_grad [-0.  0. -0.  0.] W2_grad [0.04 0.04] b1_grad [0.01 0.02] b2_grad [0.04]\n",
            "Epoch: 39 loss: 0.1464 W1_grad [-0.  0. -0.  0.] W2_grad [0.04 0.04] b1_grad [0.01 0.02] b2_grad [0.04]\n",
            "Epoch: 40 loss: 0.146 W1_grad [-0.  0. -0.  0.] W2_grad [0.04 0.04] b1_grad [0.01 0.02] b2_grad [0.04]\n",
            "Epoch: 41 loss: 0.1456 W1_grad [-0.  0. -0.  0.] W2_grad [0.04 0.04] b1_grad [0.01 0.02] b2_grad [0.04]\n",
            "Epoch: 42 loss: 0.1452 W1_grad [-0.  0. -0.  0.] W2_grad [0.04 0.04] b1_grad [0.01 0.02] b2_grad [0.04]\n",
            "Epoch: 43 loss: 0.1448 W1_grad [-0.  0. -0.  0.] W2_grad [0.04 0.04] b1_grad [0.01 0.02] b2_grad [0.04]\n",
            "Epoch: 44 loss: 0.1444 W1_grad [-0.  0. -0.  0.] W2_grad [0.04 0.04] b1_grad [0.01 0.02] b2_grad [0.04]\n",
            "Epoch: 45 loss: 0.144 W1_grad [-0.  0. -0.  0.] W2_grad [0.04 0.04] b1_grad [0.01 0.01] b2_grad [0.04]\n",
            "Epoch: 46 loss: 0.1437 W1_grad [-0.  0. -0.  0.] W2_grad [0.04 0.04] b1_grad [0.01 0.01] b2_grad [0.04]\n",
            "Epoch: 47 loss: 0.1433 W1_grad [-0.  0. -0.  0.] W2_grad [0.04 0.04] b1_grad [0.01 0.01] b2_grad [0.04]\n",
            "Epoch: 48 loss: 0.1429 W1_grad [-0.  0. -0.  0.] W2_grad [0.04 0.04] b1_grad [0.01 0.01] b2_grad [0.04]\n",
            "Epoch: 49 loss: 0.1426 W1_grad [-0.  0. -0.  0.] W2_grad [0.04 0.04] b1_grad [0.01 0.01] b2_grad [0.04]\n",
            "Epoch: 50 loss: 0.1422 W1_grad [-0.  0. -0.  0.] W2_grad [0.04 0.04] b1_grad [0.01 0.01] b2_grad [0.04]\n",
            "Epoch: 51 loss: 0.1419 W1_grad [-0.  0. -0.  0.] W2_grad [0.04 0.04] b1_grad [0.01 0.01] b2_grad [0.04]\n",
            "Epoch: 52 loss: 0.1415 W1_grad [-0.  0. -0.  0.] W2_grad [0.04 0.04] b1_grad [0.01 0.01] b2_grad [0.04]\n",
            "Epoch: 53 loss: 0.1412 W1_grad [-0.  0. -0.  0.] W2_grad [0.04 0.04] b1_grad [0.01 0.01] b2_grad [0.04]\n",
            "Epoch: 54 loss: 0.1409 W1_grad [-0.  0. -0.  0.] W2_grad [0.04 0.04] b1_grad [0.01 0.01] b2_grad [0.04]\n",
            "Epoch: 55 loss: 0.1406 W1_grad [-0.  0. -0.  0.] W2_grad [0.04 0.04] b1_grad [0.01 0.01] b2_grad [0.04]\n",
            "Epoch: 56 loss: 0.1402 W1_grad [-0.  0. -0.  0.] W2_grad [0.04 0.04] b1_grad [0.01 0.01] b2_grad [0.04]\n",
            "Epoch: 57 loss: 0.1399 W1_grad [-0.  0. -0.  0.] W2_grad [0.04 0.04] b1_grad [0.01 0.01] b2_grad [0.04]\n",
            "Epoch: 58 loss: 0.1396 W1_grad [-0.  0. -0. -0.] W2_grad [0.04 0.04] b1_grad [0.01 0.01] b2_grad [0.04]\n",
            "Epoch: 59 loss: 0.1393 W1_grad [-0.  0. -0. -0.] W2_grad [0.04 0.04] b1_grad [0.01 0.01] b2_grad [0.04]\n",
            "Epoch: 60 loss: 0.139 W1_grad [-0.  0. -0. -0.] W2_grad [0.04 0.04] b1_grad [0.01 0.01] b2_grad [0.04]\n",
            "Epoch: 61 loss: 0.1387 W1_grad [-0. -0. -0. -0.] W2_grad [0.04 0.04] b1_grad [0.01 0.01] b2_grad [0.04]\n",
            "Epoch: 62 loss: 0.1384 W1_grad [-0. -0. -0. -0.] W2_grad [0.04 0.04] b1_grad [0.01 0.01] b2_grad [0.04]\n",
            "Epoch: 63 loss: 0.1381 W1_grad [-0. -0. -0. -0.] W2_grad [0.04 0.04] b1_grad [0.01 0.01] b2_grad [0.04]\n",
            "Epoch: 64 loss: 0.1379 W1_grad [-0. -0. -0. -0.] W2_grad [0.04 0.04] b1_grad [0.01 0.01] b2_grad [0.04]\n",
            "Epoch: 65 loss: 0.1376 W1_grad [-0. -0. -0. -0.] W2_grad [0.04 0.04] b1_grad [0.01 0.01] b2_grad [0.04]\n",
            "Epoch: 66 loss: 0.1373 W1_grad [-0. -0. -0. -0.] W2_grad [0.04 0.04] b1_grad [0.01 0.01] b2_grad [0.04]\n",
            "Epoch: 67 loss: 0.1371 W1_grad [-0. -0. -0. -0.] W2_grad [0.04 0.04] b1_grad [0.01 0.01] b2_grad [0.04]\n",
            "Epoch: 68 loss: 0.1368 W1_grad [-0. -0. -0. -0.] W2_grad [0.03 0.03] b1_grad [0.01 0.01] b2_grad [0.04]\n",
            "Epoch: 69 loss: 0.1365 W1_grad [-0. -0. -0. -0.] W2_grad [0.03 0.03] b1_grad [0.01 0.01] b2_grad [0.04]\n",
            "Epoch: 70 loss: 0.1363 W1_grad [-0. -0. -0. -0.] W2_grad [0.03 0.03] b1_grad [0.01 0.01] b2_grad [0.04]\n",
            "Epoch: 71 loss: 0.136 W1_grad [-0. -0. -0. -0.] W2_grad [0.03 0.03] b1_grad [0.01 0.01] b2_grad [0.04]\n",
            "Epoch: 72 loss: 0.1358 W1_grad [-0. -0. -0. -0.] W2_grad [0.03 0.03] b1_grad [0.01 0.01] b2_grad [0.04]\n",
            "Epoch: 73 loss: 0.1356 W1_grad [-0. -0. -0. -0.] W2_grad [0.03 0.03] b1_grad [0.01 0.01] b2_grad [0.04]\n",
            "Epoch: 74 loss: 0.1353 W1_grad [-0. -0. -0. -0.] W2_grad [0.03 0.03] b1_grad [0.01 0.01] b2_grad [0.03]\n",
            "Epoch: 75 loss: 0.1351 W1_grad [-0. -0. -0. -0.] W2_grad [0.03 0.03] b1_grad [0.01 0.01] b2_grad [0.03]\n",
            "Epoch: 76 loss: 0.1349 W1_grad [-0. -0. -0. -0.] W2_grad [0.03 0.03] b1_grad [0.01 0.01] b2_grad [0.03]\n",
            "Epoch: 77 loss: 0.1347 W1_grad [-0. -0. -0. -0.] W2_grad [0.03 0.03] b1_grad [0.01 0.01] b2_grad [0.03]\n",
            "Epoch: 78 loss: 0.1344 W1_grad [-0. -0. -0. -0.] W2_grad [0.03 0.03] b1_grad [0.01 0.01] b2_grad [0.03]\n",
            "Epoch: 79 loss: 0.1342 W1_grad [-0. -0. -0. -0.] W2_grad [0.03 0.03] b1_grad [0.   0.01] b2_grad [0.03]\n",
            "Epoch: 80 loss: 0.134 W1_grad [-0. -0. -0. -0.] W2_grad [0.03 0.03] b1_grad [0.   0.01] b2_grad [0.03]\n",
            "Epoch: 81 loss: 0.1338 W1_grad [-0. -0. -0. -0.] W2_grad [0.03 0.03] b1_grad [0.   0.01] b2_grad [0.03]\n",
            "Epoch: 82 loss: 0.1336 W1_grad [-0. -0. -0. -0.] W2_grad [0.03 0.03] b1_grad [0.   0.01] b2_grad [0.03]\n",
            "Epoch: 83 loss: 0.1334 W1_grad [-0. -0. -0. -0.] W2_grad [0.03 0.03] b1_grad [0.   0.01] b2_grad [0.03]\n",
            "Epoch: 84 loss: 0.1332 W1_grad [-0. -0. -0. -0.] W2_grad [0.03 0.03] b1_grad [0.   0.01] b2_grad [0.03]\n",
            "Epoch: 85 loss: 0.1331 W1_grad [-0. -0. -0. -0.] W2_grad [0.03 0.03] b1_grad [0.   0.01] b2_grad [0.03]\n",
            "Epoch: 86 loss: 0.1329 W1_grad [-0. -0. -0. -0.] W2_grad [0.03 0.03] b1_grad [0.   0.01] b2_grad [0.03]\n",
            "Epoch: 87 loss: 0.1327 W1_grad [-0. -0. -0. -0.] W2_grad [0.03 0.03] b1_grad [0.   0.01] b2_grad [0.03]\n",
            "Epoch: 88 loss: 0.1325 W1_grad [-0. -0. -0. -0.] W2_grad [0.03 0.03] b1_grad [0.   0.01] b2_grad [0.03]\n",
            "Epoch: 89 loss: 0.1323 W1_grad [-0. -0. -0. -0.] W2_grad [0.03 0.03] b1_grad [0.   0.01] b2_grad [0.03]\n",
            "Epoch: 90 loss: 0.1322 W1_grad [-0. -0. -0. -0.] W2_grad [0.03 0.03] b1_grad [0.   0.01] b2_grad [0.03]\n",
            "Epoch: 91 loss: 0.132 W1_grad [-0. -0. -0. -0.] W2_grad [0.03 0.03] b1_grad [0.   0.01] b2_grad [0.03]\n",
            "Epoch: 92 loss: 0.1319 W1_grad [-0. -0. -0. -0.] W2_grad [0.03 0.03] b1_grad [0.   0.01] b2_grad [0.03]\n",
            "Epoch: 93 loss: 0.1317 W1_grad [-0. -0. -0. -0.] W2_grad [0.03 0.03] b1_grad [0.   0.01] b2_grad [0.03]\n",
            "Epoch: 94 loss: 0.1315 W1_grad [-0. -0. -0. -0.] W2_grad [0.03 0.03] b1_grad [0.   0.01] b2_grad [0.03]\n",
            "Epoch: 95 loss: 0.1314 W1_grad [-0. -0. -0. -0.] W2_grad [0.03 0.03] b1_grad [0.   0.01] b2_grad [0.03]\n",
            "Epoch: 96 loss: 0.1312 W1_grad [-0. -0. -0. -0.] W2_grad [0.03 0.03] b1_grad [0.   0.01] b2_grad [0.03]\n",
            "Epoch: 97 loss: 0.1311 W1_grad [-0. -0. -0. -0.] W2_grad [0.03 0.02] b1_grad [0.   0.01] b2_grad [0.03]\n",
            "Epoch: 98 loss: 0.131 W1_grad [-0. -0. -0. -0.] W2_grad [0.03 0.02] b1_grad [0.   0.01] b2_grad [0.03]\n",
            "Epoch: 99 loss: 0.1308 W1_grad [-0. -0. -0. -0.] W2_grad [0.02 0.02] b1_grad [0.   0.01] b2_grad [0.03]\n",
            "Epoch: 100 loss: 0.1307 W1_grad [-0. -0. -0. -0.] W2_grad [0.02 0.02] b1_grad [0. 0.] b2_grad [0.03]\n"
          ]
        }
      ]
    }
  ]
}