{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDua7f/Q1nUIL3B/3eNxmM"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#题目描述：\n",
        "\n",
        "#假设我们有一个简单的三层全连接神经网络，用于分类任务。网络结构如下：\n",
        "\n",
        "#输入层：3个神经元\n",
        "#隐藏层1：4个神经元，使用 ReLU 激活函数\n",
        "#隐藏层2：2个神经元，使用 ReLU 激活函数\n",
        "#输出层：3个神经元，使用 Softmax 激活函数"
      ],
      "metadata": {
        "id": "qTU5fJTrv3PA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "P-d3b9tGnaCa"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#relu激活函数\n",
        "def relu(a):\n",
        "  return np.maximum(0, a)"
      ],
      "metadata": {
        "id": "3xkOb3MNvYeo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#softmax激活函数\n",
        "def softmax(b):\n",
        "  z = np.exp(b - np.max(b))\n",
        "  return z / np.sum(z)"
      ],
      "metadata": {
        "id": "IGwRBf86v0wJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#前向传播求输出y_pred\n",
        "def forward(W1, W2, W3, b1, b2, b3, X):\n",
        "  Z1 = np.dot(X, W1.T) + b1.T # ndarray(2,4)\n",
        "  H1 = relu(Z1) #ndarray(2,4)\n",
        "\n",
        "  Z2 = np.dot(H1, W2.T) + b2 # ndarray(2,2)\n",
        "  H2 = relu(Z2) #ndarray(2,2)\n",
        "\n",
        "  Z3 = np.dot(H2, W3.T) + b3.T\n",
        "  y_pred = softmax(Z3) #ndarray(2,3)\n",
        "\n",
        "  return Z1, H1, Z2, H2, Z3, y_pred"
      ],
      "metadata": {
        "id": "iMUuPSDlXruR"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#求损失loss函数\n",
        "def compute_loss(y_pred, y_true):\n",
        "  loss = (y_pred - y_true)**2 / 2 #adarray(2,3)\n",
        "  return loss"
      ],
      "metadata": {
        "id": "ypYraWLaejX6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#反向传播，求参数导数\n",
        "def backward(y_pred, y_true, H1, H2, Z1, Z2, Z3, W2, W3, X):\n",
        "  y_diff = y_pred - y_true #ndarray (2,3)\n",
        "\n",
        "  dz3 = y_diff #ndarray(2,3)\n",
        "  b3_grad = dz3.mean(axis=0, keepdims=True).T #abarray(3,1)\n",
        "  W3_grad = np.dot(dz3.T, H2) #ndarray(3,2)\n",
        "\n",
        "  dh2 = np.dot(dz3, W3) #shape(2,2)\n",
        "  dz2 = dh2 * (Z2>0) #shape(2,2)\n",
        "  b2_grad = np.mean(dz2, axis=0, keepdims=True).T #shape(2,1)\n",
        "  W2_grad = np.dot(dz2.T, H1)\n",
        "\n",
        "  dh1 = np.dot(dz2.T, W2) #shape(2,4)\n",
        "  dz1 = dh1 * (Z1>0) #shape(2,4)\n",
        "  b1_grad = np.mean(dz1, axis=0, keepdims=True).T #shape(4,1)\n",
        "  W1_grad = np.dot(dz1.T, X) #shape(4,3)\n",
        "\n",
        "  return W1_grad, W2_grad, W3_grad, b1_grad, b2_grad, b3_grad"
      ],
      "metadata": {
        "id": "0dtgKVUmf7j0"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#更新参数weight, bias\n",
        "def update_params(W1, W2, W3, b1, b2, b3, learning_rate):\n",
        "  W1 -= learning_rate * W1_grad\n",
        "  W2 -= learning_rate * W2_grad\n",
        "  W3 -= learning_rate * W3_grad\n",
        "\n",
        "  b1 -= learning_rate * b1_grad\n",
        "  b2 -= learning_rate * b2_grad\n",
        "  b3 -= learning_rate * b3_grad\n",
        "  return W1, W2, W3, b1, b2, b3"
      ],
      "metadata": {
        "id": "RQiS_wmx_SUL"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([[0.2, 0.4, 0.1],\n",
        "        [0.7, 0.3, 0.8]])   #ndarray (2, 3)\n",
        "\n",
        "y_true = np.array([[1, 0, 0],\n",
        "          [0, 0, 1]])   # ndarray(2, 3)\n",
        "\n",
        "W1 = np.array([[0.1, 0.2, 0.3],\n",
        "        [0.4, 0.5, 0.6],\n",
        "        [0.7, 0.8, 0.9],    # ndarray(4, 3)\n",
        "        [0.2, 0.3, 0.1]])   #（输入层到隐藏层1）\n",
        "\n",
        "b1 = np.array([[0.1],\n",
        "        [0.2],\n",
        "        [0.3],  # ndarray(4, 1)\n",
        "        [0.1]]) #（隐藏层1 偏置）\n",
        "\n",
        "W2 = np.array([[0.5, 0.4, 0.3, 0.2], # ndarray(2, 4)\n",
        "        [0.1, 0.6, 0.7, 0.8]]) #（隐藏层1 到隐藏层2）\n",
        "\n",
        "b2 = np.array([[0.2],  # ndarray(2, 1)\n",
        "        [0.3]])  #（隐藏层2 偏置）\n",
        "\n",
        "W3 = np.array([[0.9, 0.8],\n",
        "        [0.7, 0.6],   # ndarray(3, 2)\n",
        "        [0.5, 0.4]])   #（隐藏层2 到输出层）\n",
        "\n",
        "b3 = np.array([[0.1],\n",
        "        [0.2],  # ndarray(3, 1)\n",
        "        [0.3]]) #（输出层偏置）\n",
        "\n",
        "learning_rate = 0.02\n",
        "\n",
        "epochs = 40\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  Z1, H1, Z2, H2, Z3, y_pred = forward(W1, W2, W3, b1, b2, b3, X)\n",
        "  loss = compute_loss(y_pred, y_true)\n",
        "\n",
        "  W1_grad, W2_grad, W3_grad, b1_grad, b2_grad, b3_grad = backward(y_pred, y_true, H1, H2, Z1, Z2, Z3, W2, W3, X)\n",
        "  W1, W2, W3, b1, b2, b3 = update_params(W1, W2, W3, b1, b2, b3, learning_rate)\n",
        "\n",
        "  print(\"epoch:\", epoch + 1,\n",
        "    \"loss\", np.round(loss.mean(), 4),\n",
        "    \"W1_grad\", np.round(W1_grad.flatten(), 2),\n",
        "    \"W2_grad\", np.round(W2_grad.flatten(), 2),\n",
        "    \"W3_grad\", np.round(W3_grad.flatten(), 2),\n",
        "    \"b1_grad\", np.round(b1_grad.flatten(), 2),\n",
        "    \"b2_grad\", np.round(b2_grad.flatten(), 2),\n",
        "    \"b3_grad\", np.round(b3_grad.flatten(), 2)\n",
        "  )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6H3k01mtvGx",
        "outputId": "7e86f349-6320-42d1-e657-4d7eaa20a878"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1 loss 0.158 W1_grad [-0.3  -0.24 -0.29 -0.17 -0.14 -0.16 -0.09 -0.08 -0.09 -0.02 -0.02 -0.02] W2_grad [-0.11 -0.26 -0.41 -0.15 -0.08 -0.19 -0.31 -0.12] W3_grad [-0.02 -0.04  0.41  0.66 -1.38 -2.23] b1_grad [-0.34 -0.2  -0.11 -0.03] b2_grad [-0.31 -0.26] b3_grad [-0.22  0.14 -0.42]\n",
            "epoch: 2 loss 0.1556 W1_grad [-0.3  -0.25 -0.3  -0.19 -0.16 -0.19 -0.12 -0.1  -0.12 -0.05 -0.04 -0.05] W2_grad [-0.13 -0.29 -0.46 -0.16 -0.11 -0.25 -0.38 -0.14] W3_grad [-0.03 -0.04  0.39  0.63 -1.38 -2.21] b1_grad [-0.35 -0.22 -0.14 -0.06] b2_grad [-0.32 -0.28] b3_grad [-0.22  0.14 -0.41]\n",
            "epoch: 3 loss 0.153 W1_grad [-0.31 -0.25 -0.3  -0.22 -0.18 -0.21 -0.15 -0.12 -0.15 -0.08 -0.06 -0.08] W2_grad [-0.15 -0.33 -0.51 -0.17 -0.13 -0.3  -0.46 -0.16] W3_grad [-0.03 -0.04  0.38  0.59 -1.39 -2.2 ] b1_grad [-0.35 -0.25 -0.17 -0.09] b2_grad [-0.34 -0.3 ] b3_grad [-0.22  0.13 -0.4 ]\n",
            "epoch: 4 loss 0.1501 W1_grad [-0.31 -0.25 -0.31 -0.24 -0.19 -0.24 -0.18 -0.14 -0.18 -0.1  -0.08 -0.1 ] W2_grad [-0.17 -0.36 -0.56 -0.19 -0.16 -0.34 -0.53 -0.17] W3_grad [-0.04 -0.05  0.36  0.56 -1.4  -2.18] b1_grad [-0.36 -0.27 -0.2  -0.11] b2_grad [-0.35 -0.32] b3_grad [-0.22  0.12 -0.39]\n",
            "epoch: 5 loss 0.147 W1_grad [-0.32 -0.26 -0.32 -0.26 -0.21 -0.26 -0.21 -0.16 -0.21 -0.13 -0.1  -0.13] W2_grad [-0.19 -0.39 -0.6  -0.2  -0.19 -0.39 -0.6  -0.19] W3_grad [-0.05 -0.07  0.34  0.53 -1.4  -2.17] b1_grad [-0.37 -0.3  -0.23 -0.14] b2_grad [-0.37 -0.34] b3_grad [-0.23  0.11 -0.38]\n",
            "epoch: 6 loss 0.1435 W1_grad [-0.33 -0.27 -0.33 -0.29 -0.22 -0.29 -0.24 -0.18 -0.24 -0.15 -0.11 -0.16] W2_grad [-0.21 -0.43 -0.65 -0.21 -0.21 -0.44 -0.66 -0.21] W3_grad [-0.06 -0.09  0.32  0.49 -1.4  -2.14] b1_grad [-0.38 -0.32 -0.26 -0.17] b2_grad [-0.38 -0.36] b3_grad [-0.23  0.1  -0.37]\n",
            "epoch: 7 loss 0.1396 W1_grad [-0.34 -0.27 -0.33 -0.31 -0.24 -0.31 -0.27 -0.2  -0.27 -0.18 -0.13 -0.18] W2_grad [-0.23 -0.46 -0.69 -0.23 -0.24 -0.48 -0.72 -0.23] W3_grad [-0.08 -0.12  0.3   0.45 -1.4  -2.12] b1_grad [-0.39 -0.34 -0.29 -0.19] b2_grad [-0.39 -0.38] b3_grad [-0.23  0.09 -0.35]\n",
            "epoch: 8 loss 0.1355 W1_grad [-0.35 -0.28 -0.34 -0.33 -0.26 -0.33 -0.29 -0.22 -0.29 -0.2  -0.14 -0.2 ] W2_grad [-0.25 -0.49 -0.73 -0.24 -0.27 -0.52 -0.78 -0.24] W3_grad [-0.11 -0.16  0.28  0.42 -1.39 -2.08] b1_grad [-0.4  -0.37 -0.32 -0.21] b2_grad [-0.4  -0.39] b3_grad [-0.24  0.08 -0.34]\n",
            "epoch: 9 loss 0.1311 W1_grad [-0.36 -0.29 -0.35 -0.35 -0.27 -0.35 -0.32 -0.24 -0.32 -0.22 -0.16 -0.22] W2_grad [-0.27 -0.52 -0.77 -0.25 -0.29 -0.56 -0.83 -0.26] W3_grad [-0.15 -0.21  0.25  0.38 -1.37 -2.03] b1_grad [-0.41 -0.39 -0.35 -0.23] b2_grad [-0.41 -0.41] b3_grad [-0.25  0.07 -0.32]\n",
            "epoch: 10 loss 0.1265 W1_grad [-0.37 -0.29 -0.36 -0.37 -0.29 -0.37 -0.34 -0.26 -0.35 -0.23 -0.17 -0.24] W2_grad [-0.29 -0.55 -0.81 -0.27 -0.31 -0.59 -0.87 -0.28] W3_grad [-0.19 -0.27  0.23  0.34 -1.35 -1.98] b1_grad [-0.42 -0.41 -0.37 -0.25] b2_grad [-0.42 -0.42] b3_grad [-0.26  0.06 -0.3 ]\n",
            "epoch: 11 loss 0.1218 W1_grad [-0.38 -0.3  -0.37 -0.39 -0.3  -0.39 -0.37 -0.28 -0.37 -0.25 -0.18 -0.25] W2_grad [-0.31 -0.58 -0.85 -0.28 -0.34 -0.62 -0.91 -0.29] W3_grad [-0.25 -0.34  0.2   0.29 -1.31 -1.91] b1_grad [-0.43 -0.43 -0.4  -0.26] b2_grad [-0.43 -0.43] b3_grad [-0.27  0.05 -0.28]\n",
            "epoch: 12 loss 0.1173 W1_grad [-0.39 -0.31 -0.39 -0.41 -0.32 -0.41 -0.39 -0.3  -0.39 -0.26 -0.19 -0.26] W2_grad [-0.33 -0.61 -0.88 -0.29 -0.36 -0.65 -0.95 -0.31] W3_grad [-0.31 -0.42  0.18  0.25 -1.27 -1.84] b1_grad [-0.44 -0.45 -0.42 -0.27] b2_grad [-0.44 -0.44] b3_grad [-0.28  0.04 -0.26]\n",
            "epoch: 13 loss 0.1131 W1_grad [-0.4  -0.32 -0.4  -0.43 -0.33 -0.43 -0.41 -0.31 -0.41 -0.27 -0.19 -0.27] W2_grad [-0.35 -0.63 -0.91 -0.31 -0.38 -0.68 -0.98 -0.32] W3_grad [-0.37 -0.52  0.15  0.22 -1.23 -1.77] b1_grad [-0.46 -0.47 -0.45 -0.28] b2_grad [-0.45 -0.45] b3_grad [-0.29  0.03 -0.24]\n",
            "epoch: 14 loss 0.1093 W1_grad [-0.42 -0.33 -0.41 -0.44 -0.34 -0.45 -0.43 -0.33 -0.43 -0.27 -0.2  -0.28] W2_grad [-0.37 -0.65 -0.93 -0.32 -0.39 -0.7  -1.   -0.33] W3_grad [-0.45 -0.62  0.13  0.18 -1.19 -1.69] b1_grad [-0.47 -0.49 -0.47 -0.29] b2_grad [-0.46 -0.45] b3_grad [-0.3   0.03 -0.22]\n",
            "epoch: 15 loss 0.106 W1_grad [-0.43 -0.35 -0.43 -0.46 -0.36 -0.46 -0.45 -0.34 -0.45 -0.28 -0.21 -0.28] W2_grad [-0.39 -0.68 -0.96 -0.33 -0.41 -0.72 -1.03 -0.35] W3_grad [-0.53 -0.72  0.1   0.15 -1.15 -1.62] b1_grad [-0.49 -0.51 -0.49 -0.3 ] b2_grad [-0.46 -0.46] b3_grad [-0.32  0.02 -0.21]\n",
            "epoch: 16 loss 0.1032 W1_grad [-0.45 -0.36 -0.45 -0.48 -0.37 -0.48 -0.47 -0.36 -0.47 -0.28 -0.21 -0.29] W2_grad [-0.41 -0.7  -0.98 -0.35 -0.43 -0.74 -1.05 -0.36] W3_grad [-0.61 -0.83  0.08  0.12 -1.11 -1.55] b1_grad [-0.51 -0.53 -0.52 -0.3 ] b2_grad [-0.47 -0.47] b3_grad [-0.33  0.02 -0.19]\n",
            "epoch: 17 loss 0.1009 W1_grad [-0.47 -0.37 -0.46 -0.5  -0.38 -0.5  -0.49 -0.38 -0.49 -0.29 -0.22 -0.29] W2_grad [-0.43 -0.72 -1.01 -0.36 -0.45 -0.76 -1.07 -0.37] W3_grad [-0.69 -0.93  0.07  0.09 -1.07 -1.49] b1_grad [-0.53 -0.55 -0.54 -0.31] b2_grad [-0.48 -0.48] b3_grad [-0.33  0.01 -0.18]\n",
            "epoch: 18 loss 0.0991 W1_grad [-0.49 -0.39 -0.48 -0.52 -0.4  -0.52 -0.52 -0.39 -0.52 -0.3  -0.22 -0.3 ] W2_grad [-0.45 -0.75 -1.04 -0.38 -0.47 -0.79 -1.1  -0.39] W3_grad [-0.77 -1.03  0.05  0.07 -1.05 -1.45] b1_grad [-0.55 -0.57 -0.57 -0.32] b2_grad [-0.49 -0.48] b3_grad [-0.34  0.01 -0.17]\n",
            "epoch: 19 loss 0.0977 W1_grad [-0.51 -0.4  -0.5  -0.54 -0.42 -0.54 -0.54 -0.41 -0.54 -0.3  -0.23 -0.31] W2_grad [-0.47 -0.77 -1.07 -0.39 -0.5  -0.81 -1.13 -0.4 ] W3_grad [-0.85 -1.13  0.04  0.05 -1.03 -1.41] b1_grad [-0.57 -0.59 -0.59 -0.33] b2_grad [-0.5  -0.49] b3_grad [-0.35  0.01 -0.16]\n",
            "epoch: 20 loss 0.0967 W1_grad [-0.53 -0.42 -0.53 -0.56 -0.43 -0.56 -0.57 -0.43 -0.57 -0.31 -0.23 -0.31] W2_grad [-0.5  -0.8  -1.1  -0.41 -0.52 -0.84 -1.16 -0.42] W3_grad [-0.92 -1.23  0.03  0.04 -1.02 -1.39] b1_grad [-0.59 -0.62 -0.62 -0.34] b2_grad [-0.5 -0.5] b3_grad [-0.36  0.   -0.15]\n",
            "epoch: 21 loss 0.0959 W1_grad [-0.55 -0.43 -0.55 -0.59 -0.45 -0.59 -0.6  -0.46 -0.6  -0.32 -0.24 -0.32] W2_grad [-0.52 -0.83 -1.13 -0.42 -0.55 -0.88 -1.2  -0.44] W3_grad [-1.   -1.32  0.02  0.03 -1.03 -1.39] b1_grad [-0.62 -0.65 -0.65 -0.35] b2_grad [-0.51 -0.51] b3_grad [-0.36  0.   -0.14]\n",
            "epoch: 22 loss 0.0954 W1_grad [-0.58 -0.45 -0.58 -0.61 -0.47 -0.61 -0.63 -0.48 -0.63 -0.33 -0.25 -0.34] W2_grad [-0.55 -0.86 -1.17 -0.44 -0.58 -0.91 -1.24 -0.46] W3_grad [-1.07 -1.4   0.01  0.02 -1.04 -1.39] b1_grad [-0.65 -0.68 -0.69 -0.36] b2_grad [-0.52 -0.53] b3_grad [-0.36  0.   -0.14]\n",
            "epoch: 23 loss 0.0951 W1_grad [-0.6  -0.47 -0.6  -0.64 -0.49 -0.65 -0.66 -0.51 -0.67 -0.34 -0.26 -0.35] W2_grad [-0.58 -0.9  -1.21 -0.46 -0.61 -0.95 -1.28 -0.48] W3_grad [-1.14 -1.49  0.01  0.01 -1.06 -1.42] b1_grad [-0.67 -0.71 -0.73 -0.37] b2_grad [-0.53 -0.54] b3_grad [-0.36  0.   -0.14]\n",
            "epoch: 24 loss 0.0949 W1_grad [-0.63 -0.49 -0.63 -0.68 -0.52 -0.68 -0.7  -0.54 -0.7  -0.36 -0.27 -0.36] W2_grad [-0.61 -0.94 -1.25 -0.48 -0.65 -1.   -1.34 -0.51] W3_grad [-1.21 -1.57  0.01  0.01 -1.1  -1.45] b1_grad [-0.71 -0.74 -0.77 -0.39] b2_grad [-0.54 -0.55] b3_grad [-0.37  0.   -0.13]\n",
            "epoch: 25 loss 0.0949 W1_grad [-0.66 -0.52 -0.66 -0.71 -0.55 -0.71 -0.74 -0.57 -0.75 -0.38 -0.28 -0.38] W2_grad [-0.65 -0.98 -1.3  -0.51 -0.69 -1.05 -1.39 -0.53] W3_grad [-1.28 -1.65  0.    0.01 -1.14 -1.5 ] b1_grad [-0.74 -0.78 -0.81 -0.41] b2_grad [-0.56 -0.57] b3_grad [-0.37  0.   -0.13]\n",
            "epoch: 26 loss 0.0949 W1_grad [-0.7  -0.54 -0.7  -0.75 -0.57 -0.75 -0.79 -0.6  -0.79 -0.39 -0.3  -0.4 ] W2_grad [-0.69 -1.03 -1.36 -0.53 -0.73 -1.1  -1.46 -0.56] W3_grad [-1.35 -1.73  0.    0.   -1.19 -1.56] b1_grad [-0.77 -0.82 -0.86 -0.43] b2_grad [-0.57 -0.58] b3_grad [-0.37  0.   -0.13]\n",
            "epoch: 27 loss 0.095 W1_grad [-0.73 -0.57 -0.73 -0.79 -0.61 -0.8  -0.84 -0.64 -0.84 -0.41 -0.31 -0.42] W2_grad [-0.73 -1.08 -1.42 -0.56 -0.78 -1.16 -1.53 -0.59] W3_grad [-1.42 -1.81  0.    0.   -1.25 -1.63] b1_grad [-0.81 -0.87 -0.92 -0.45] b2_grad [-0.58 -0.6 ] b3_grad [-0.37  0.   -0.13]\n",
            "epoch: 28 loss 0.0951 W1_grad [-0.77 -0.6  -0.77 -0.84 -0.64 -0.84 -0.89 -0.68 -0.9  -0.44 -0.33 -0.44] W2_grad [-0.78 -1.14 -1.48 -0.59 -0.84 -1.23 -1.6  -0.63] W3_grad [-1.49 -1.89  0.    0.   -1.32 -1.71] b1_grad [-0.85 -0.92 -0.97 -0.47] b2_grad [-0.59 -0.62] b3_grad [-0.37  0.   -0.13]\n",
            "epoch: 29 loss 0.0953 W1_grad [-0.81 -0.63 -0.81 -0.89 -0.68 -0.89 -0.95 -0.72 -0.96 -0.46 -0.34 -0.47] W2_grad [-0.83 -1.2  -1.55 -0.62 -0.89 -1.3  -1.69 -0.66] W3_grad [-1.57 -1.98  0.    0.   -1.4  -1.8 ] b1_grad [-0.89 -0.97 -1.04 -0.5 ] b2_grad [-0.61 -0.64] b3_grad [-0.37  0.   -0.13]\n",
            "epoch: 30 loss 0.0954 W1_grad [-0.85 -0.66 -0.86 -0.94 -0.72 -0.95 -1.01 -0.77 -1.02 -0.49 -0.36 -0.49] W2_grad [-0.88 -1.26 -1.63 -0.65 -0.96 -1.38 -1.78 -0.7 ] W3_grad [-1.66 -2.08  0.    0.   -1.49 -1.9 ] b1_grad [-0.94 -1.03 -1.1  -0.52] b2_grad [-0.62 -0.66] b3_grad [-0.37  0.   -0.13]\n",
            "epoch: 31 loss 0.0956 W1_grad [-0.9  -0.69 -0.91 -1.   -0.76 -1.01 -1.08 -0.82 -1.09 -0.52 -0.39 -0.52] W2_grad [-0.94 -1.33 -1.71 -0.69 -1.03 -1.46 -1.88 -0.75] W3_grad [-1.75 -2.18  0.    0.   -1.58 -2.01] b1_grad [-0.99 -1.09 -1.18 -0.56] b2_grad [-0.64 -0.68] b3_grad [-0.36  0.   -0.14]\n",
            "epoch: 32 loss 0.0958 W1_grad [-0.95 -0.73 -0.96 -1.06 -0.81 -1.07 -1.16 -0.88 -1.17 -0.55 -0.41 -0.55] W2_grad [-1.   -1.41 -1.81 -0.73 -1.1  -1.55 -1.99 -0.79] W3_grad [-1.84 -2.29  0.    0.   -1.69 -2.13] b1_grad [-1.05 -1.16 -1.26 -0.59] b2_grad [-0.66 -0.7 ] b3_grad [-0.36  0.   -0.14]\n",
            "epoch: 33 loss 0.0959 W1_grad [-1.01 -0.77 -1.02 -1.13 -0.86 -1.14 -1.24 -0.94 -1.25 -0.58 -0.44 -0.59] W2_grad [-1.08 -1.5  -1.91 -0.77 -1.18 -1.65 -2.11 -0.85] W3_grad [-1.95 -2.42  0.    0.   -1.8  -2.26] b1_grad [-1.11 -1.23 -1.34 -0.63] b2_grad [-0.68 -0.72] b3_grad [-0.36  0.   -0.14]\n",
            "epoch: 34 loss 0.096 W1_grad [-1.07 -0.82 -1.08 -1.21 -0.91 -1.22 -1.33 -1.   -1.34 -0.62 -0.46 -0.63] W2_grad [-1.15 -1.59 -2.02 -0.82 -1.27 -1.77 -2.24 -0.9 ] W3_grad [-2.07 -2.55  0.    0.   -1.92 -2.4 ] b1_grad [-1.17 -1.31 -1.44 -0.67] b2_grad [-0.69 -0.75] b3_grad [-0.36  0.   -0.14]\n",
            "epoch: 35 loss 0.0961 W1_grad [-1.14 -0.87 -1.15 -1.29 -0.97 -1.3  -1.43 -1.07 -1.44 -0.66 -0.49 -0.67] W2_grad [-1.24 -1.7  -2.14 -0.88 -1.37 -1.89 -2.38 -0.97] W3_grad [-2.2  -2.7   0.    0.   -2.06 -2.56] b1_grad [-1.24 -1.4  -1.55 -0.71] b2_grad [-0.71 -0.77] b3_grad [-0.36  0.   -0.14]\n",
            "epoch: 36 loss 0.0962 W1_grad [-1.21 -0.92 -1.22 -1.38 -1.04 -1.39 -1.53 -1.15 -1.55 -0.71 -0.53 -0.72] W2_grad [-1.33 -1.81 -2.27 -0.93 -1.48 -2.02 -2.54 -1.03] W3_grad [-2.34 -2.87  0.    0.   -2.2  -2.73] b1_grad [-1.32 -1.5  -1.66 -0.76] b2_grad [-0.74 -0.8 ] b3_grad [-0.36  0.   -0.14]\n",
            "epoch: 37 loss 0.0963 W1_grad [-1.29 -0.98 -1.3  -1.48 -1.12 -1.5  -1.65 -1.24 -1.67 -0.76 -0.57 -0.77] W2_grad [-1.43 -1.94 -2.42 -1.   -1.6  -2.17 -2.71 -1.11] W3_grad [-2.5  -3.05  0.    0.   -2.37 -2.92] b1_grad [-1.41 -1.6  -1.79 -0.82] b2_grad [-0.76 -0.83] b3_grad [-0.36  0.   -0.14]\n",
            "epoch: 38 loss 0.0964 W1_grad [-1.38 -1.05 -1.4  -1.59 -1.2  -1.61 -1.78 -1.34 -1.8  -0.82 -0.61 -0.83] W2_grad [-1.55 -2.08 -2.59 -1.07 -1.73 -2.33 -2.91 -1.19] W3_grad [-2.68 -3.25  0.    0.   -2.55 -3.13] b1_grad [-1.51 -1.72 -1.93 -0.88] b2_grad [-0.78 -0.86] b3_grad [-0.36  0.   -0.14]\n",
            "epoch: 39 loss 0.0965 W1_grad [-1.48 -1.12 -1.5  -1.72 -1.29 -1.73 -1.93 -1.45 -1.95 -0.88 -0.65 -0.89] W2_grad [-1.68 -2.24 -2.77 -1.15 -1.88 -2.52 -3.12 -1.29] W3_grad [-2.87 -3.48  0.    0.   -2.74 -3.36] b1_grad [-1.61 -1.86 -2.09 -0.94] b2_grad [-0.81 -0.89] b3_grad [-0.36  0.   -0.14]\n",
            "epoch: 40 loss 0.0966 W1_grad [-1.6  -1.21 -1.61 -1.85 -1.39 -1.87 -2.09 -1.57 -2.12 -0.95 -0.71 -0.96] W2_grad [-1.82 -2.41 -2.97 -1.24 -2.05 -2.72 -3.36 -1.39] W3_grad [-3.09 -3.74  0.    0.   -2.97 -3.62] b1_grad [-1.73 -2.   -2.26 -1.02] b2_grad [-0.84 -0.92] b3_grad [-0.36  0.   -0.14]\n"
          ]
        }
      ]
    }
  ]
}