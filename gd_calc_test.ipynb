{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNrw/HErpoo6qbYYTXBRWWl"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "QKJe8iwGU3Wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available else \"cpu\""
      ],
      "metadata": {
        "id": "otaTTLNwU5Hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Relu函数\n",
        "def relu(a):\n",
        "  a= np.maximum(0,a)\n",
        "  return a"
      ],
      "metadata": {
        "id": "UQsi4DWYU7Xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#前向传播,求输出\n",
        "def forward(W, X, V, B, b3):\n",
        "  H = np.dot(W,X) + B\n",
        "\n",
        "  print(f\"隐藏层h1,h2节点:{H}\\n\")\n",
        "  y_pred = np.dot(V,H) + b3\n",
        "  print(f\"y输出值:{y_pred}\\n\")\n",
        "  H = np.reshape(H, (2,1))\n",
        "\n",
        "  return H, y_pred"
      ],
      "metadata": {
        "id": "p3hjIxW1U9In"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#计算loss值\n",
        "def computer_loss(y_pred, y_true):\n",
        "  loss = pow((y_pred - y_true),2) / 2\n",
        "  print(f\"loss的值{loss}\\n\")\n",
        "  return loss"
      ],
      "metadata": {
        "id": "nrZa1x32U-gT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#计算v1,v2,b3,w1,w2,b1,b2的梯度\n",
        "def computer_gradian(y_pred, y_true, H, W, X):\n",
        "  W_grad = []\n",
        "  y_diff = y_pred - y_true\n",
        "  B_grad = np.dot(W,X)\n",
        "\n",
        "  W_grad.append((y_pred - y_true) * H * X)\n",
        "\n",
        "  Wg = np.reshape(W_grad, (2,3))\n",
        "\n",
        "  return Wg, y_diff * relu(H), B_grad, y_diff"
      ],
      "metadata": {
        "id": "co7KCgqrVBWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#反向传导,更新weight, bias\n",
        "def backward(lr, W, B, b3, W_grad, B_grad, b3_grad):\n",
        "  W -= lr * W_grad\n",
        "  B -= lr * B_grad\n",
        "  b3 -= lr * b3_grad\n",
        "  return W, B, b3"
      ],
      "metadata": {
        "id": "tHweahgRVDO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYSq1lSUUsAf",
        "outputId": "e36db9ef-59cd-4dda-dc58-d3248b3a15af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "隐藏层h1,h2节点:[ 3.95 -1.8 ]\n",
            "\n",
            "y输出值:1.735\n",
            "\n",
            "loss的值0.027612500000000022\n",
            "\n",
            "w偏导数([[ 1.8565   -1.392375  2.78475 ]\n",
            " [-0.846     0.6345   -1.269   ]]), v偏导数([[0.92825]\n",
            " [0.     ]]), 隐藏层b偏导数[ 3.85 -1.6 ], 输出层b3偏导数0.2350000000000001\n",
            "\n",
            "更新后w的值([[ 0.481435   -0.28607625  0.7721525 ]\n",
            " [-0.19154     0.393655   -0.18731   ]]), 更新后隐藏节点b的值[ 0.0615 -0.184 ], 更新后b3的值0.047650000000000005\n",
            "隐藏层h1,h2节点:[ 3.76994188 -1.7194925 ]\n",
            "\n",
            "y输出值:1.6549138124999998\n",
            "\n",
            "loss的值0.011999144651642547\n",
            "\n",
            "w偏导数([[ 1.16803214 -0.8760241   1.75204821]\n",
            " [-0.53274628  0.39955971 -0.79911942]]), v偏导数([[0.58401607]\n",
            " [0.        ]]), 隐藏层b偏导数[ 3.70844188 -1.5354925 ], 输出层b3偏导数0.1549138124999998\n",
            "\n",
            "更新后w的值([[ 0.46975468 -0.27731601  0.75463202]\n",
            " [-0.18621254  0.3896594  -0.17931881]]), 更新后隐藏节点b的值[ 0.02441558 -0.16864508], 更新后b3的值0.04610086187500001\n",
            "隐藏层h1,h2节点:[ 3.64379501 -1.66351567]\n",
            "\n",
            "y输出值:1.598647963104632\n",
            "\n",
            "loss的值0.004865710312346416\n",
            "\n",
            "w偏导数([[ 0.71890591 -0.53917943  1.07835887]\n",
            " [-0.32820487  0.24615365 -0.4923073 ]]), v偏导数([[0.35945296]\n",
            " [0.        ]]), 隐藏层b偏导数[ 3.61937942 -1.4948706 ], 输出层b3偏导数0.09864796310463197\n",
            "\n",
            "更新后w的值([[ 0.46256562 -0.27192421  0.74384843]\n",
            " [-0.18293049  0.38719787 -0.17439573]]), 更新后隐藏节点b的值[-0.01177821 -0.15369637], 更新后b3的值0.04511438224395369\n",
            "隐藏层h1,h2节点:[ 3.55278464 -1.62354134]\n",
            "\n",
            "y输出值:1.5579388206830929\n",
            "\n",
            "loss的值0.0016784534710737947\n",
            "\n",
            "w偏导数([[ 0.4116883  -0.30876623  0.61753246]\n",
            " [-0.18813214  0.14109911 -0.28219821]]), v偏导数([[0.20584415]\n",
            " [0.        ]]), 隐藏层b偏导数[ 3.56456285 -1.46984498], 输出层b3偏导数0.05793882068309286\n",
            "\n",
            "更新后w的值([[ 0.45844874 -0.26883655  0.7376731 ]\n",
            " [-0.18104917  0.38578688 -0.17157375]]), 更新后隐藏节点b的值[-0.04742384 -0.13899792], 更新后b3的值0.044534994037122765\n",
            "隐藏层h1,h2节点:[ 3.48574777 -1.59449782]\n",
            "\n",
            "y输出值:1.5278597446474664\n",
            "\n",
            "loss的值0.0003880826859110173\n",
            "\n",
            "w偏导数([[ 0.19422409 -0.14566806  0.29133613]\n",
            " [-0.0888446   0.06663345 -0.13326691]]), v偏导数([[0.09711204]\n",
            " [0.        ]]), 隐藏层b偏导数[ 3.53317162 -1.4554999 ], 输出层b3偏导数0.027859744647466433\n",
            "\n",
            "更新后w的值([[ 0.4565065  -0.26737987  0.73475974]\n",
            " [-0.18016072  0.38512054 -0.17024108]]), 更新后隐藏节点b的值[-0.08275556 -0.12444292], 更新后b3的值0.0442563965906481\n",
            "隐藏层h1,h2节点:[ 3.43560647 -1.57316842]\n",
            "\n",
            "y输出值:1.5052798753496637\n",
            "\n",
            "loss的值1.3938541853993064e-05\n",
            "\n",
            "w偏导数([[ 0.03627915 -0.02720936  0.05441872]\n",
            " [-0.01661227  0.0124592  -0.0249184 ]]), v偏导数([[0.01813957]\n",
            " [0.        ]]), 隐藏层b偏导数[ 3.51836203 -1.4487255 ], 输出层b3偏导数0.005279875349663676\n",
            "\n",
            "更新后w的值([[ 0.4561437  -0.26710778  0.73421556]\n",
            " [-0.1799946   0.38499595 -0.1699919 ]]), 更新后隐藏节点b的值[-0.11793918 -0.10995567], 更新后b3的值0.04420359783715146\n",
            "隐藏层h1,h2节点:[ 3.39765657 -1.55741448]\n",
            "\n",
            "y输出值:1.4881145070511297\n",
            "\n",
            "loss的值7.063247131882297e-05\n",
            "\n",
            "w偏导数([[-0.08076565  0.06057423 -0.12114847]\n",
            " [ 0.03702128 -0.02776596  0.05553192]]), v偏导数([[-0.04038282]\n",
            " [-0.        ]]), 隐藏层b偏导数[ 3.51559574 -1.44745881], 输出层b3偏导数-0.011885492948870313\n",
            "\n",
            "更新后w的值([[ 0.45695136 -0.26771352  0.73542704]\n",
            " [-0.18036481  0.38527361 -0.17054722]]), 更新后隐藏节点b的值[-0.15309514 -0.09548108], 更新后b3的值0.044322452766640166\n",
            "隐藏层h1,h2节点:[ 3.36865899 -1.54576276]\n",
            "\n",
            "y输出值:1.474926087573012\n",
            "\n",
            "loss的值0.0003143505421981338\n",
            "\n",
            "w偏导数([[-0.16893092  0.12669819 -0.25339638]\n",
            " [ 0.07751664 -0.05813748  0.11627496]]), v偏导数([[-0.08446546]\n",
            " [-0.        ]]), 隐藏层b偏导数[ 3.52175412 -1.45028169], 输出层b3偏导数-0.025073912426988088\n",
            "\n",
            "更新后w的值([[ 0.45864067 -0.2689805   0.737961  ]\n",
            " [-0.18113998  0.38585498 -0.17170997]]), 更新后隐藏节点b的值[-0.18831268 -0.08097826], 更新后b3的值0.044573191890910045\n",
            "隐藏层h1,h2节点:[ 3.34632243 -1.53717059]\n",
            "\n",
            "y输出值:1.4646965395527647\n",
            "\n",
            "loss的值0.0006231671597747541\n",
            "\n",
            "w偏导数([[-0.23627352  0.17720514 -0.35441028]\n",
            " [ 0.10853488 -0.08140116  0.16280232]]), v偏导数([[-0.11813676]\n",
            " [-0.        ]]), 隐藏层b偏导数[ 3.53463511 -1.45619233], 输出层b3偏导数-0.03530346044723531\n",
            "\n",
            "更新后w的值([[ 0.46100341 -0.27075255  0.74150511]\n",
            " [-0.18222533  0.38666899 -0.17333799]]), 更新后隐藏节点b的值[-0.22365903 -0.06641634], 更新后b3的值0.0449262264953824\n",
            "隐藏层h1,h2节点:[ 3.32899194 -1.53088445]\n",
            "\n",
            "y输出值:1.456689910818648\n",
            "\n",
            "loss的值0.0009378819124483336\n",
            "\n",
            "w偏导数([[-0.28835788  0.21626841 -0.43253681]\n",
            " [ 0.13260548 -0.09945411  0.19890823]]), v偏导数([[-0.14417894]\n",
            " [-0.        ]]), 隐藏层b偏导数[ 3.55265096 -1.46446811], 输出层b3偏导数-0.04331008918135204\n",
            "\n",
            "更新后w的值([[ 0.46388698 -0.27291524  0.74583048]\n",
            " [-0.18355138  0.38766354 -0.17532707]]), 更新后隐藏节点b的值[-0.25918554 -0.05177166], 更新后b3的值0.04535932738719592\n"
          ]
        }
      ],
      "source": [
        "#初始化\n",
        "X = np.array([2.0, -1.5, 3.0])\n",
        "W1 = np.array([0.5, -0.3, 0.8])\n",
        "W2 = np.array([-0.2, 0.4, -0.2])\n",
        "W = np.array([\n",
        "   [0.5, -0.3, 0.8],\n",
        "   [-0.2, 0.4, -0.2]\n",
        "   ])\n",
        "\n",
        "V = np.array([0.7, 0.6])\n",
        "b1 = 0.1\n",
        "b2 = -0.2\n",
        "B = np.array([0.1, -0.2])\n",
        "b3 = 0.05\n",
        "y_true = 1.5\n",
        "lr = 0.01\n",
        "\n",
        "eporchs = 10\n",
        "\n",
        "for eporch in range(eporchs):\n",
        "  H, y_pred = forward(W, X, V, B, b3)\n",
        "  loss = computer_loss(y_pred, y_true)\n",
        "\n",
        "  W_grad, V_grad, B_grad, b3_grad = computer_gradian(y_pred, y_true, H, W, X)\n",
        "  print(f\"w偏导数({W_grad}), v偏导数({V_grad}), 隐藏层b偏导数{B_grad}, 输出层b3偏导数{b3_grad}\\n\")\n",
        "\n",
        "  W, B, b3 = backward(lr, W, B, b3, W_grad, B_grad, b3_grad)\n",
        "  print(f\"更新后w的值({W}), 更新后隐藏节点b的值{B}, 更新后b3的值{b3}\")"
      ]
    }
  ]
}