{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQNxN5OWvf/aMrz8pDX3zj"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Ul2zWH6BsJTo"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#题目描述：\n",
        "\n",
        "#假设我们有一个简单的两层全连接神经网络，用于回归任务。网络结构如下：\n",
        "\n",
        "#输入层：2个神经元\n",
        "#隐藏层：3个神经元，使用 ReLU 激活函数\n",
        "#输出层：1个神经元，线性输出"
      ],
      "metadata": {
        "id": "YJz7DCutuIXn"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#relu函数\n",
        "def relu(a):\n",
        "  return np.maximum(0, a)"
      ],
      "metadata": {
        "id": "LRwXgHb7ty83"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#前向传播，求y_pred\n",
        "\n",
        "def forward(W1, W2, X, b1, b2):\n",
        "  Z = np.dot(X, W1.T) + b1.T\n",
        "  H = relu(Z)\n",
        "\n",
        "  y_pred = np.dot(H, W2.T) + b2\n",
        "  return Z, H, y_pred"
      ],
      "metadata": {
        "id": "P614PX_0uCWn"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#求损失函数loss的值\n",
        "\n",
        "def compute_loss(y_pred, y_true):\n",
        "  loss = np.mean((y_pred - y_true)**2) / 2\n",
        "  return loss"
      ],
      "metadata": {
        "id": "AYZukD83vEcI"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#反向传播,梯度计算\n",
        "def backward(y_pred, y_true, W2, H, Z, X):\n",
        "  y_diff = y_pred - y_true\n",
        "  b2_grad = np.mean((y_diff), axis=0, keepdims=True)\n",
        "\n",
        "  W2_grad = np.dot(y_diff.T, H)\n",
        "\n",
        "  dw2 = np.dot((y_pred-y_true), W2)\n",
        "  dh = dw2 * (Z>0)\n",
        "  b1_grad = np.mean(dh, axis=1, keepdims=True)\n",
        "  W1_grad = np.dot(dh.T, X)\n",
        "\n",
        "  return W1_grad, W2_grad, b1_grad, b2_grad"
      ],
      "metadata": {
        "id": "vMwEhQvivpn5"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_params(W1, W2, b1, b2, W1_grad, W2_grad, b1_grad, b2_grad, learning_rate):\n",
        "  W1 -= learning_rate * W1_grad\n",
        "  W2 -= learning_rate * W2_grad\n",
        "\n",
        "  b1 -= learning_rate * b1_grad\n",
        "  b2 -= learning_rate * b2_grad\n",
        "  return W1, W2, b1, b2"
      ],
      "metadata": {
        "id": "B0nekePvThK5"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 数据\n",
        "X = np.array([[0.5, 0.3],\n",
        "        [0.8, 0.6],\n",
        "        [0.2, 0.9]])\n",
        "\n",
        "y_true = np.array([[0.8], [1.2], [0.5]])\n",
        "\n",
        "# 初始参数\n",
        "W1 = np.array([[0.1, 0.2],\n",
        "        [0.3, 0.4],\n",
        "        [0.5, 0.6]])\n",
        "\n",
        "b1 = np.array([[0.1], [0.2], [0.3]])\n",
        "W2 = np.array([[0.7, 0.8, 0.9]])\n",
        "b2 = np.array([[0.4]])\n",
        "\n",
        "# 学习率\n",
        "learning_rate = 0.01\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  Z, H, y_pred = forward(W1, W2, X, b1, b2)\n",
        "  loss = compute_loss(y_pred, y_true)\n",
        "  W1_grad, W2_grad, b1_grad, b2_grad = backward(y_pred, y_true, W2, H, Z, X)\n",
        "  W1, W2, b1, b2 = update_params(W1, W2, b1, b2, W1_grad, W2_grad, b1_grad, b2_grad, learning_rate)\n",
        "\n",
        "  print(\n",
        "    \"Epoch:\", epoch + 1,\n",
        "    \"loss\", np.round(loss, 4),\n",
        "    \"W1_grad\", np.round(W1_grad.flatten(), 2),\n",
        "    \"W2_grad\", np.round(W2_grad.flatten(), 2),\n",
        "    \"b1_grad\", np.round(b1_grad.flatten(), 2),\n",
        "    \"b2_grad\", np.round(b2_grad.flatten(), 2)\n",
        "  )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wn8WtT9zsPWA",
        "outputId": "f503b810-7d24-43f1-a818-1390f0ae5f0a"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 loss 0.5902 W1_grad [0.98 1.46 1.13 1.67 1.27 1.88] W2_grad [0.87 1.88 2.9 ] b1_grad [0.62 0.73 1.16] b2_grad [1.05]\n",
            "Epoch: 2 loss 0.4818 W1_grad [0.86 1.3  0.97 1.47 1.08 1.64] W2_grad [0.73 1.62 2.51] b1_grad [0.54 0.61 1.05] b2_grad [0.94]\n",
            "Epoch: 3 loss 0.3987 W1_grad [0.75 1.17 0.84 1.31 0.93 1.45] W2_grad [0.61 1.41 2.2 ] b1_grad [0.47 0.52 0.95] b2_grad [0.85]\n",
            "Epoch: 4 loss 0.3339 W1_grad [0.67 1.06 0.74 1.18 0.81 1.29] W2_grad [0.52 1.24 1.94] b1_grad [0.42 0.45 0.87] b2_grad [0.77]\n",
            "Epoch: 5 loss 0.2827 W1_grad [0.59 0.97 0.65 1.06 0.71 1.16] W2_grad [0.44 1.1  1.72] b1_grad [0.37 0.38 0.8 ] b2_grad [0.7]\n",
            "Epoch: 6 loss 0.2417 W1_grad [0.53 0.89 0.57 0.96 0.62 1.04] W2_grad [0.38 0.98 1.54] b1_grad [0.33 0.33 0.74] b2_grad [0.64]\n",
            "Epoch: 7 loss 0.2084 W1_grad [0.47 0.81 0.51 0.88 0.55 0.95] W2_grad [0.33 0.87 1.38] b1_grad [0.29 0.28 0.69] b2_grad [0.59]\n",
            "Epoch: 8 loss 0.1813 W1_grad [0.42 0.75 0.45 0.81 0.48 0.86] W2_grad [0.29 0.78 1.24] b1_grad [0.26 0.24 0.65] b2_grad [0.54]\n",
            "Epoch: 9 loss 0.159 W1_grad [0.38 0.7  0.4  0.74 0.43 0.79] W2_grad [0.25 0.71 1.12] b1_grad [0.23 0.2  0.61] b2_grad [0.5]\n",
            "Epoch: 10 loss 0.1404 W1_grad [0.34 0.65 0.36 0.68 0.38 0.72] W2_grad [0.22 0.64 1.01] b1_grad [0.21 0.17 0.58] b2_grad [0.46]\n",
            "Epoch: 11 loss 0.1249 W1_grad [0.3  0.6  0.32 0.63 0.33 0.67] W2_grad [0.19 0.58 0.92] b1_grad [0.19 0.14 0.55] b2_grad [0.43]\n",
            "Epoch: 12 loss 0.1119 W1_grad [0.27 0.56 0.28 0.59 0.3  0.62] W2_grad [0.17 0.53 0.84] b1_grad [0.17 0.12 0.52] b2_grad [0.39]\n",
            "Epoch: 13 loss 0.1009 W1_grad [0.24 0.53 0.25 0.55 0.26 0.57] W2_grad [0.15 0.49 0.77] b1_grad [0.15 0.1  0.5 ] b2_grad [0.37]\n",
            "Epoch: 14 loss 0.0916 W1_grad [0.22 0.49 0.22 0.51 0.23 0.53] W2_grad [0.13 0.45 0.7 ] b1_grad [0.14 0.08 0.47] b2_grad [0.34]\n",
            "Epoch: 15 loss 0.0836 W1_grad [0.19 0.46 0.2  0.48 0.21 0.5 ] W2_grad [0.12 0.41 0.64] b1_grad [0.12 0.06 0.45] b2_grad [0.32]\n",
            "Epoch: 16 loss 0.0768 W1_grad [0.17 0.44 0.18 0.45 0.18 0.46] W2_grad [0.11 0.38 0.59] b1_grad [0.11 0.04 0.44] b2_grad [0.3]\n",
            "Epoch: 17 loss 0.071 W1_grad [0.15 0.41 0.16 0.42 0.16 0.44] W2_grad [0.1  0.35 0.54] b1_grad [0.1  0.03 0.42] b2_grad [0.28]\n",
            "Epoch: 18 loss 0.066 W1_grad [0.13 0.39 0.14 0.4  0.14 0.41] W2_grad [0.09 0.32 0.5 ] b1_grad [0.09 0.01 0.4 ] b2_grad [0.26]\n",
            "Epoch: 19 loss 0.0616 W1_grad [0.12 0.37 0.12 0.38 0.12 0.38] W2_grad [0.08 0.3  0.46] b1_grad [0.08 0.   0.39] b2_grad [0.24]\n",
            "Epoch: 20 loss 0.0578 W1_grad [0.1  0.35 0.1  0.35 0.11 0.36] W2_grad [0.07 0.27 0.42] b1_grad [ 0.07 -0.01  0.38] b2_grad [0.23]\n",
            "Epoch: 21 loss 0.0545 W1_grad [0.09 0.33 0.09 0.34 0.09 0.34] W2_grad [0.06 0.26 0.39] b1_grad [ 0.07 -0.02  0.37] b2_grad [0.21]\n",
            "Epoch: 22 loss 0.0516 W1_grad [0.08 0.32 0.08 0.32 0.08 0.32] W2_grad [0.06 0.24 0.36] b1_grad [ 0.06 -0.03  0.35] b2_grad [0.2]\n",
            "Epoch: 23 loss 0.0491 W1_grad [0.06 0.3  0.07 0.3  0.07 0.31] W2_grad [0.05 0.22 0.33] b1_grad [ 0.05 -0.04  0.34] b2_grad [0.19]\n",
            "Epoch: 24 loss 0.0469 W1_grad [0.05 0.29 0.05 0.29 0.05 0.29] W2_grad [0.05 0.21 0.31] b1_grad [ 0.05 -0.04  0.34] b2_grad [0.17]\n",
            "Epoch: 25 loss 0.0449 W1_grad [0.04 0.27 0.04 0.27 0.04 0.28] W2_grad [0.04 0.19 0.29] b1_grad [ 0.04 -0.05  0.33] b2_grad [0.16]\n",
            "Epoch: 26 loss 0.0432 W1_grad [0.03 0.26 0.03 0.26 0.03 0.26] W2_grad [0.04 0.18 0.27] b1_grad [ 0.04 -0.06  0.32] b2_grad [0.15]\n",
            "Epoch: 27 loss 0.0417 W1_grad [0.03 0.25 0.03 0.25 0.03 0.25] W2_grad [0.04 0.17 0.25] b1_grad [ 0.03 -0.06  0.31] b2_grad [0.14]\n",
            "Epoch: 28 loss 0.0403 W1_grad [0.02 0.24 0.02 0.24 0.02 0.24] W2_grad [0.03 0.16 0.23] b1_grad [ 0.03 -0.07  0.3 ] b2_grad [0.14]\n",
            "Epoch: 29 loss 0.0391 W1_grad [0.01 0.23 0.01 0.23 0.01 0.23] W2_grad [0.03 0.14 0.21] b1_grad [ 0.02 -0.08  0.3 ] b2_grad [0.13]\n",
            "Epoch: 30 loss 0.038 W1_grad [0.   0.22 0.   0.22 0.   0.22] W2_grad [0.03 0.14 0.2 ] b1_grad [ 0.02 -0.08  0.29] b2_grad [0.12]\n",
            "Epoch: 31 loss 0.0371 W1_grad [-0.    0.21 -0.    0.21 -0.    0.21] W2_grad [0.03 0.13 0.18] b1_grad [ 0.02 -0.08  0.29] b2_grad [0.11]\n",
            "Epoch: 32 loss 0.0362 W1_grad [-0.01  0.2  -0.01  0.2  -0.01  0.2 ] W2_grad [0.02 0.12 0.17] b1_grad [ 0.01 -0.09  0.28] b2_grad [0.11]\n",
            "Epoch: 33 loss 0.0354 W1_grad [-0.01  0.2  -0.01  0.19 -0.01  0.19] W2_grad [0.02 0.11 0.16] b1_grad [ 0.01 -0.09  0.28] b2_grad [0.1]\n",
            "Epoch: 34 loss 0.0347 W1_grad [-0.02  0.19 -0.02  0.19 -0.02  0.19] W2_grad [0.02 0.1  0.14] b1_grad [ 0.01 -0.1   0.27] b2_grad [0.1]\n",
            "Epoch: 35 loss 0.034 W1_grad [-0.02  0.18 -0.02  0.18 -0.02  0.18] W2_grad [0.02 0.1  0.13] b1_grad [ 0.   -0.1   0.27] b2_grad [0.09]\n",
            "Epoch: 36 loss 0.0334 W1_grad [-0.03  0.18 -0.03  0.17 -0.03  0.17] W2_grad [0.02 0.09 0.12] b1_grad [ 0.   -0.1   0.26] b2_grad [0.09]\n",
            "Epoch: 37 loss 0.0329 W1_grad [-0.03  0.17 -0.03  0.17 -0.03  0.17] W2_grad [0.01 0.08 0.12] b1_grad [-0.   -0.1   0.26] b2_grad [0.08]\n",
            "Epoch: 38 loss 0.0324 W1_grad [-0.04  0.17 -0.04  0.16 -0.04  0.16] W2_grad [0.01 0.08 0.11] b1_grad [-0.   -0.11  0.25] b2_grad [0.08]\n",
            "Epoch: 39 loss 0.0319 W1_grad [-0.04  0.16 -0.04  0.16 -0.04  0.16] W2_grad [0.01 0.07 0.1 ] b1_grad [-0.   -0.11  0.25] b2_grad [0.07]\n",
            "Epoch: 40 loss 0.0315 W1_grad [-0.04  0.16 -0.04  0.15 -0.04  0.15] W2_grad [0.01 0.07 0.09] b1_grad [-0.01 -0.11  0.25] b2_grad [0.07]\n",
            "Epoch: 41 loss 0.0311 W1_grad [-0.05  0.15 -0.05  0.15 -0.05  0.15] W2_grad [0.01 0.07 0.08] b1_grad [-0.01 -0.11  0.24] b2_grad [0.07]\n",
            "Epoch: 42 loss 0.0308 W1_grad [-0.05  0.15 -0.05  0.14 -0.05  0.14] W2_grad [0.01 0.06 0.08] b1_grad [-0.01 -0.12  0.24] b2_grad [0.06]\n",
            "Epoch: 43 loss 0.0304 W1_grad [-0.05  0.14 -0.05  0.14 -0.05  0.14] W2_grad [0.01 0.06 0.07] b1_grad [-0.01 -0.12  0.24] b2_grad [0.06]\n",
            "Epoch: 44 loss 0.0301 W1_grad [-0.06  0.14 -0.05  0.14 -0.05  0.14] W2_grad [0.01 0.05 0.07] b1_grad [-0.01 -0.12  0.24] b2_grad [0.06]\n",
            "Epoch: 45 loss 0.0298 W1_grad [-0.06  0.14 -0.06  0.13 -0.06  0.13] W2_grad [0.01 0.05 0.06] b1_grad [-0.01 -0.12  0.23] b2_grad [0.05]\n",
            "Epoch: 46 loss 0.0295 W1_grad [-0.06  0.13 -0.06  0.13 -0.06  0.13] W2_grad [0.01 0.05 0.06] b1_grad [-0.01 -0.12  0.23] b2_grad [0.05]\n",
            "Epoch: 47 loss 0.0292 W1_grad [-0.06  0.13 -0.06  0.13 -0.06  0.13] W2_grad [0.   0.04 0.05] b1_grad [-0.02 -0.12  0.23] b2_grad [0.05]\n",
            "Epoch: 48 loss 0.029 W1_grad [-0.06  0.13 -0.06  0.12 -0.06  0.12] W2_grad [0.   0.04 0.05] b1_grad [-0.02 -0.13  0.23] b2_grad [0.05]\n",
            "Epoch: 49 loss 0.0287 W1_grad [-0.07  0.12 -0.06  0.12 -0.06  0.12] W2_grad [0.   0.04 0.04] b1_grad [-0.02 -0.13  0.22] b2_grad [0.04]\n",
            "Epoch: 50 loss 0.0285 W1_grad [-0.07  0.12 -0.07  0.12 -0.07  0.12] W2_grad [0.   0.03 0.04] b1_grad [-0.02 -0.13  0.22] b2_grad [0.04]\n",
            "Epoch: 51 loss 0.0283 W1_grad [-0.07  0.12 -0.07  0.11 -0.07  0.11] W2_grad [0.   0.03 0.04] b1_grad [-0.02 -0.13  0.22] b2_grad [0.04]\n",
            "Epoch: 52 loss 0.028 W1_grad [-0.07  0.12 -0.07  0.11 -0.07  0.11] W2_grad [0.   0.03 0.03] b1_grad [-0.02 -0.13  0.22] b2_grad [0.04]\n",
            "Epoch: 53 loss 0.0278 W1_grad [-0.07  0.11 -0.07  0.11 -0.07  0.11] W2_grad [0.   0.03 0.03] b1_grad [-0.02 -0.13  0.22] b2_grad [0.04]\n",
            "Epoch: 54 loss 0.0276 W1_grad [-0.07  0.11 -0.07  0.11 -0.07  0.11] W2_grad [0.   0.03 0.03] b1_grad [-0.02 -0.13  0.21] b2_grad [0.03]\n",
            "Epoch: 55 loss 0.0274 W1_grad [-0.07  0.11 -0.07  0.11 -0.07  0.11] W2_grad [0.   0.02 0.02] b1_grad [-0.02 -0.13  0.21] b2_grad [0.03]\n",
            "Epoch: 56 loss 0.0272 W1_grad [-0.08  0.11 -0.07  0.1  -0.07  0.1 ] W2_grad [-0.    0.02  0.02] b1_grad [-0.02 -0.13  0.21] b2_grad [0.03]\n",
            "Epoch: 57 loss 0.0271 W1_grad [-0.08  0.11 -0.07  0.1  -0.07  0.1 ] W2_grad [-0.    0.02  0.02] b1_grad [-0.02 -0.13  0.21] b2_grad [0.03]\n",
            "Epoch: 58 loss 0.0269 W1_grad [-0.08  0.1  -0.08  0.1  -0.07  0.1 ] W2_grad [-0.    0.02  0.02] b1_grad [-0.02 -0.13  0.21] b2_grad [0.03]\n",
            "Epoch: 59 loss 0.0267 W1_grad [-0.08  0.1  -0.08  0.1  -0.08  0.1 ] W2_grad [-0.    0.02  0.01] b1_grad [-0.02 -0.13  0.21] b2_grad [0.03]\n",
            "Epoch: 60 loss 0.0265 W1_grad [-0.08  0.1  -0.08  0.1  -0.08  0.1 ] W2_grad [-0.    0.02  0.01] b1_grad [-0.02 -0.13  0.21] b2_grad [0.03]\n",
            "Epoch: 61 loss 0.0263 W1_grad [-0.08  0.1  -0.08  0.1  -0.08  0.1 ] W2_grad [-0.    0.01  0.01] b1_grad [-0.02 -0.13  0.2 ] b2_grad [0.03]\n",
            "Epoch: 62 loss 0.0262 W1_grad [-0.08  0.1  -0.08  0.09 -0.08  0.09] W2_grad [-0.    0.01  0.01] b1_grad [-0.02 -0.13  0.2 ] b2_grad [0.02]\n",
            "Epoch: 63 loss 0.026 W1_grad [-0.08  0.1  -0.08  0.09 -0.08  0.09] W2_grad [-0.    0.01  0.01] b1_grad [-0.02 -0.14  0.2 ] b2_grad [0.02]\n",
            "Epoch: 64 loss 0.0258 W1_grad [-0.08  0.1  -0.08  0.09 -0.08  0.09] W2_grad [-0.    0.01  0.  ] b1_grad [-0.02 -0.14  0.2 ] b2_grad [0.02]\n",
            "Epoch: 65 loss 0.0257 W1_grad [-0.08  0.09 -0.08  0.09 -0.08  0.09] W2_grad [-0.    0.01  0.  ] b1_grad [-0.02 -0.14  0.2 ] b2_grad [0.02]\n",
            "Epoch: 66 loss 0.0255 W1_grad [-0.08  0.09 -0.08  0.09 -0.08  0.09] W2_grad [-0.    0.01  0.  ] b1_grad [-0.02 -0.14  0.2 ] b2_grad [0.02]\n",
            "Epoch: 67 loss 0.0254 W1_grad [-0.08  0.09 -0.08  0.09 -0.08  0.09] W2_grad [-0.    0.01 -0.  ] b1_grad [-0.02 -0.14  0.2 ] b2_grad [0.02]\n",
            "Epoch: 68 loss 0.0252 W1_grad [-0.08  0.09 -0.08  0.09 -0.08  0.09] W2_grad [-0.    0.01 -0.  ] b1_grad [-0.02 -0.14  0.2 ] b2_grad [0.02]\n",
            "Epoch: 69 loss 0.0251 W1_grad [-0.08  0.09 -0.08  0.09 -0.08  0.09] W2_grad [-0.01  0.   -0.  ] b1_grad [-0.02 -0.14  0.2 ] b2_grad [0.02]\n",
            "Epoch: 70 loss 0.0249 W1_grad [-0.08  0.09 -0.08  0.09 -0.08  0.09] W2_grad [-0.01  0.   -0.  ] b1_grad [-0.02 -0.14  0.2 ] b2_grad [0.02]\n",
            "Epoch: 71 loss 0.0248 W1_grad [-0.08  0.09 -0.08  0.08 -0.08  0.08] W2_grad [-0.01  0.   -0.01] b1_grad [-0.02 -0.14  0.19] b2_grad [0.02]\n",
            "Epoch: 72 loss 0.0246 W1_grad [-0.09  0.09 -0.08  0.08 -0.08  0.08] W2_grad [-0.01  0.   -0.01] b1_grad [-0.02 -0.14  0.19] b2_grad [0.02]\n",
            "Epoch: 73 loss 0.0245 W1_grad [-0.09  0.09 -0.08  0.08 -0.08  0.08] W2_grad [-0.01  0.   -0.01] b1_grad [-0.02 -0.14  0.19] b2_grad [0.02]\n",
            "Epoch: 74 loss 0.0243 W1_grad [-0.09  0.09 -0.08  0.08 -0.08  0.08] W2_grad [-0.01  0.   -0.01] b1_grad [-0.03 -0.14  0.19] b2_grad [0.02]\n",
            "Epoch: 75 loss 0.0242 W1_grad [-0.09  0.08 -0.08  0.08 -0.08  0.08] W2_grad [-0.01 -0.   -0.01] b1_grad [-0.02 -0.14  0.19] b2_grad [0.02]\n",
            "Epoch: 76 loss 0.024 W1_grad [-0.09  0.08 -0.08  0.08 -0.08  0.08] W2_grad [-0.01 -0.   -0.01] b1_grad [-0.02 -0.14  0.19] b2_grad [0.02]\n",
            "Epoch: 77 loss 0.0239 W1_grad [-0.09  0.08 -0.08  0.08 -0.08  0.08] W2_grad [-0.01 -0.   -0.01] b1_grad [-0.02 -0.14  0.19] b2_grad [0.01]\n",
            "Epoch: 78 loss 0.0238 W1_grad [-0.09  0.08 -0.08  0.08 -0.08  0.08] W2_grad [-0.01 -0.   -0.01] b1_grad [-0.02 -0.14  0.19] b2_grad [0.01]\n",
            "Epoch: 79 loss 0.0236 W1_grad [-0.09  0.08 -0.08  0.08 -0.08  0.08] W2_grad [-0.01 -0.   -0.01] b1_grad [-0.02 -0.14  0.19] b2_grad [0.01]\n",
            "Epoch: 80 loss 0.0235 W1_grad [-0.09  0.08 -0.08  0.08 -0.08  0.08] W2_grad [-0.01 -0.   -0.01] b1_grad [-0.02 -0.14  0.19] b2_grad [0.01]\n",
            "Epoch: 81 loss 0.0233 W1_grad [-0.09  0.08 -0.08  0.08 -0.08  0.08] W2_grad [-0.01 -0.   -0.01] b1_grad [-0.02 -0.14  0.19] b2_grad [0.01]\n",
            "Epoch: 82 loss 0.0232 W1_grad [-0.09  0.08 -0.08  0.08 -0.08  0.08] W2_grad [-0.01 -0.   -0.01] b1_grad [-0.02 -0.14  0.19] b2_grad [0.01]\n",
            "Epoch: 83 loss 0.0231 W1_grad [-0.09  0.08 -0.08  0.08 -0.08  0.08] W2_grad [-0.01 -0.   -0.02] b1_grad [-0.02 -0.14  0.18] b2_grad [0.01]\n",
            "Epoch: 84 loss 0.0229 W1_grad [-0.09  0.08 -0.08  0.08 -0.08  0.08] W2_grad [-0.01 -0.01 -0.02] b1_grad [-0.02 -0.14  0.18] b2_grad [0.01]\n",
            "Epoch: 85 loss 0.0228 W1_grad [-0.09  0.08 -0.08  0.08 -0.08  0.08] W2_grad [-0.01 -0.01 -0.02] b1_grad [-0.02 -0.14  0.18] b2_grad [0.01]\n",
            "Epoch: 86 loss 0.0227 W1_grad [-0.09  0.08 -0.08  0.07 -0.08  0.07] W2_grad [-0.01 -0.01 -0.02] b1_grad [-0.02 -0.14  0.18] b2_grad [0.01]\n",
            "Epoch: 87 loss 0.0225 W1_grad [-0.09  0.08 -0.08  0.07 -0.08  0.07] W2_grad [-0.01 -0.01 -0.02] b1_grad [-0.02 -0.14  0.18] b2_grad [0.01]\n",
            "Epoch: 88 loss 0.0224 W1_grad [-0.09  0.08 -0.08  0.07 -0.08  0.07] W2_grad [-0.01 -0.01 -0.02] b1_grad [-0.02 -0.14  0.18] b2_grad [0.01]\n",
            "Epoch: 89 loss 0.0223 W1_grad [-0.09  0.08 -0.08  0.07 -0.08  0.07] W2_grad [-0.01 -0.01 -0.02] b1_grad [-0.02 -0.13  0.18] b2_grad [0.01]\n",
            "Epoch: 90 loss 0.0221 W1_grad [-0.09  0.08 -0.08  0.07 -0.08  0.07] W2_grad [-0.01 -0.01 -0.02] b1_grad [-0.02 -0.13  0.18] b2_grad [0.01]\n",
            "Epoch: 91 loss 0.022 W1_grad [-0.09  0.08 -0.08  0.07 -0.08  0.07] W2_grad [-0.01 -0.01 -0.02] b1_grad [-0.02 -0.13  0.18] b2_grad [0.01]\n",
            "Epoch: 92 loss 0.0219 W1_grad [-0.09  0.08 -0.08  0.07 -0.08  0.07] W2_grad [-0.01 -0.01 -0.02] b1_grad [-0.02 -0.13  0.18] b2_grad [0.01]\n",
            "Epoch: 93 loss 0.0218 W1_grad [-0.09  0.07 -0.08  0.07 -0.08  0.07] W2_grad [-0.01 -0.01 -0.02] b1_grad [-0.02 -0.13  0.18] b2_grad [0.01]\n",
            "Epoch: 94 loss 0.0216 W1_grad [-0.09  0.07 -0.08  0.07 -0.08  0.07] W2_grad [-0.01 -0.01 -0.02] b1_grad [-0.02 -0.13  0.18] b2_grad [0.01]\n",
            "Epoch: 95 loss 0.0215 W1_grad [-0.09  0.07 -0.08  0.07 -0.08  0.07] W2_grad [-0.01 -0.01 -0.02] b1_grad [-0.02 -0.13  0.18] b2_grad [0.01]\n",
            "Epoch: 96 loss 0.0214 W1_grad [-0.09  0.07 -0.08  0.07 -0.08  0.07] W2_grad [-0.01 -0.01 -0.02] b1_grad [-0.02 -0.13  0.18] b2_grad [0.01]\n",
            "Epoch: 97 loss 0.0213 W1_grad [-0.09  0.07 -0.08  0.07 -0.08  0.07] W2_grad [-0.01 -0.01 -0.02] b1_grad [-0.02 -0.13  0.18] b2_grad [0.01]\n",
            "Epoch: 98 loss 0.0211 W1_grad [-0.09  0.07 -0.08  0.07 -0.08  0.07] W2_grad [-0.01 -0.01 -0.02] b1_grad [-0.02 -0.13  0.18] b2_grad [0.01]\n",
            "Epoch: 99 loss 0.021 W1_grad [-0.08  0.07 -0.08  0.07 -0.08  0.07] W2_grad [-0.01 -0.01 -0.02] b1_grad [-0.02 -0.13  0.17] b2_grad [0.01]\n",
            "Epoch: 100 loss 0.0209 W1_grad [-0.08  0.07 -0.08  0.07 -0.08  0.07] W2_grad [-0.01 -0.01 -0.02] b1_grad [-0.02 -0.13  0.17] b2_grad [0.01]\n"
          ]
        }
      ]
    }
  ]
}