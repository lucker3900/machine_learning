{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNITMgzpeGEiK0ggHmQR62f"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Ul2zWH6BsJTo"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#题目描述：\n",
        "\n",
        "#假设我们有一个简单的两层全连接神经网络，用于回归任务。网络结构如下：\n",
        "\n",
        "#输入层：2个神经元\n",
        "#隐藏层：3个神经元，使用 ReLU 激活函数\n",
        "#输出层：1个神经元，线性输出"
      ],
      "metadata": {
        "id": "YJz7DCutuIXn"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#relu函数\n",
        "def relu(a):\n",
        "  return np.maximum(0, a)"
      ],
      "metadata": {
        "id": "LRwXgHb7ty83"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#前向传播，求y_pred\n",
        "\n",
        "def forward(W1, W2, X, b1, b2):\n",
        "  Z = np.dot(X, W1.T) + b1.T\n",
        "  H = relu(Z)\n",
        "\n",
        "  y_pred = np.dot(H, W2.T) + b2\n",
        "  return Z, H, y_pred"
      ],
      "metadata": {
        "id": "P614PX_0uCWn"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#求损失函数loss的值\n",
        "\n",
        "def compute_loss(y_pred, y_true):\n",
        "  loss = np.mean((y_pred - y_true)**2) / 2\n",
        "  return loss"
      ],
      "metadata": {
        "id": "AYZukD83vEcI"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#反向传播,梯度计算\n",
        "def backward(y_pred, y_true, W2, H, Z, X):\n",
        "  y_diff = y_pred - y_true\n",
        "  b2_grad = np.mean((y_diff), axis=0, keepdims=True)\n",
        "\n",
        "  W2_grad = np.dot(y_diff.T, H)\n",
        "\n",
        "  dw2 = np.dot((y_pred-y_true), W2)\n",
        "  dh = dw2 * (Z>0)\n",
        "  b1_grad = np.mean(dh, axis=0, keepdims=True).T\n",
        "  W1_grad = np.dot(dh.T, X)\n",
        "\n",
        "  return W1_grad, W2_grad, b1_grad, b2_grad"
      ],
      "metadata": {
        "id": "vMwEhQvivpn5"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_params(W1, W2, b1, b2, W1_grad, W2_grad, b1_grad, b2_grad, learning_rate):\n",
        "  W1 -= learning_rate * W1_grad\n",
        "  W2 -= learning_rate * W2_grad\n",
        "\n",
        "  b1 -= learning_rate * b1_grad\n",
        "  b2 -= learning_rate * b2_grad\n",
        "  return W1, W2, b1, b2"
      ],
      "metadata": {
        "id": "B0nekePvThK5"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 数据\n",
        "X = np.array([[0.5, 0.3],\n",
        "        [0.8, 0.6],\n",
        "        [0.2, 0.9]])\n",
        "\n",
        "y_true = np.array([[0.8], [1.2], [0.5]])\n",
        "\n",
        "# 初始参数\n",
        "W1 = np.array([[0.1, 0.2],\n",
        "        [0.3, 0.4],\n",
        "        [0.5, 0.6]])\n",
        "\n",
        "b1 = np.array([[0.1], [0.2], [0.3]])\n",
        "W2 = np.array([[0.7, 0.8, 0.9]])\n",
        "b2 = np.array([[0.4]])\n",
        "\n",
        "# 学习率\n",
        "learning_rate = 0.01\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  Z, H, y_pred = forward(W1, W2, X, b1, b2)\n",
        "  loss = compute_loss(y_pred, y_true)\n",
        "  W1_grad, W2_grad, b1_grad, b2_grad = backward(y_pred, y_true, W2, H, Z, X)\n",
        "  W1, W2, b1, b2 = update_params(W1, W2, b1, b2, W1_grad, W2_grad, b1_grad, b2_grad, learning_rate)\n",
        "\n",
        "  print(\n",
        "    \"Epoch:\", epoch + 1,\n",
        "    \"loss\", np.round(loss, 4),\n",
        "    \"W1_grad\", np.round(W1_grad.flatten(), 2),\n",
        "    \"W2_grad\", np.round(W2_grad.flatten(), 2),\n",
        "    \"b1_grad\", np.round(b1_grad.flatten(), 2),\n",
        "    \"b2_grad\", np.round(b2_grad.flatten(), 2)\n",
        "  )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wn8WtT9zsPWA",
        "outputId": "39501065-e9cc-4967-c722-f6e5bf569ff9"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 loss 0.5902 W1_grad [0.98 1.46 1.13 1.67 1.27 1.88] W2_grad [0.87 1.88 2.9 ] b1_grad [0.73 0.84 0.94] b2_grad [1.05]\n",
            "Epoch: 2 loss 0.4821 W1_grad [0.86 1.3  0.97 1.47 1.08 1.64] W2_grad [0.72 1.62 2.52] b1_grad [0.65 0.73 0.82] b2_grad [0.94]\n",
            "Epoch: 3 loss 0.3991 W1_grad [0.76 1.17 0.84 1.31 0.93 1.45] W2_grad [0.6  1.41 2.21] b1_grad [0.58 0.65 0.72] b2_grad [0.85]\n",
            "Epoch: 4 loss 0.3344 W1_grad [0.67 1.06 0.74 1.18 0.81 1.29] W2_grad [0.51 1.23 1.96] b1_grad [0.52 0.58 0.63] b2_grad [0.77]\n",
            "Epoch: 5 loss 0.2831 W1_grad [0.59 0.97 0.65 1.06 0.71 1.16] W2_grad [0.43 1.09 1.74] b1_grad [0.47 0.52 0.56] b2_grad [0.7]\n",
            "Epoch: 6 loss 0.242 W1_grad [0.53 0.89 0.57 0.96 0.62 1.04] W2_grad [0.37 0.96 1.56] b1_grad [0.43 0.47 0.5 ] b2_grad [0.64]\n",
            "Epoch: 7 loss 0.2088 W1_grad [0.47 0.82 0.51 0.88 0.55 0.95] W2_grad [0.32 0.86 1.4 ] b1_grad [0.39 0.42 0.45] b2_grad [0.59]\n",
            "Epoch: 8 loss 0.1816 W1_grad [0.42 0.75 0.45 0.81 0.48 0.86] W2_grad [0.27 0.77 1.27] b1_grad [0.36 0.38 0.41] b2_grad [0.54]\n",
            "Epoch: 9 loss 0.1591 W1_grad [0.38 0.7  0.4  0.74 0.43 0.79] W2_grad [0.24 0.69 1.15] b1_grad [0.33 0.35 0.37] b2_grad [0.5]\n",
            "Epoch: 10 loss 0.1405 W1_grad [0.34 0.65 0.36 0.69 0.38 0.72] W2_grad [0.21 0.62 1.04] b1_grad [0.3  0.32 0.34] b2_grad [0.46]\n",
            "Epoch: 11 loss 0.125 W1_grad [0.3  0.6  0.32 0.63 0.33 0.67] W2_grad [0.18 0.57 0.95] b1_grad [0.28 0.29 0.31] b2_grad [0.43]\n",
            "Epoch: 12 loss 0.1119 W1_grad [0.27 0.56 0.28 0.59 0.3  0.62] W2_grad [0.16 0.51 0.87] b1_grad [0.26 0.27 0.28] b2_grad [0.39]\n",
            "Epoch: 13 loss 0.1009 W1_grad [0.24 0.53 0.25 0.55 0.26 0.57] W2_grad [0.14 0.47 0.8 ] b1_grad [0.24 0.25 0.26] b2_grad [0.37]\n",
            "Epoch: 14 loss 0.0915 W1_grad [0.22 0.49 0.22 0.51 0.23 0.53] W2_grad [0.12 0.43 0.73] b1_grad [0.22 0.23 0.24] b2_grad [0.34]\n",
            "Epoch: 15 loss 0.0835 W1_grad [0.19 0.47 0.2  0.48 0.2  0.49] W2_grad [0.11 0.39 0.67] b1_grad [0.21 0.21 0.22] b2_grad [0.32]\n",
            "Epoch: 16 loss 0.0767 W1_grad [0.17 0.44 0.18 0.45 0.18 0.46] W2_grad [0.09 0.36 0.62] b1_grad [0.19 0.2  0.2 ] b2_grad [0.29]\n",
            "Epoch: 17 loss 0.0708 W1_grad [0.15 0.41 0.16 0.42 0.16 0.43] W2_grad [0.08 0.33 0.57] b1_grad [0.18 0.18 0.19] b2_grad [0.27]\n",
            "Epoch: 18 loss 0.0657 W1_grad [0.13 0.39 0.14 0.4  0.14 0.41] W2_grad [0.07 0.3  0.53] b1_grad [0.17 0.17 0.17] b2_grad [0.26]\n",
            "Epoch: 19 loss 0.0614 W1_grad [0.12 0.37 0.12 0.38 0.12 0.38] W2_grad [0.07 0.28 0.49] b1_grad [0.15 0.16 0.16] b2_grad [0.24]\n",
            "Epoch: 20 loss 0.0576 W1_grad [0.1  0.35 0.1  0.35 0.1  0.36] W2_grad [0.06 0.26 0.45] b1_grad [0.14 0.15 0.15] b2_grad [0.22]\n",
            "Epoch: 21 loss 0.0543 W1_grad [0.09 0.33 0.09 0.34 0.09 0.34] W2_grad [0.05 0.24 0.42] b1_grad [0.14 0.14 0.14] b2_grad [0.21]\n",
            "Epoch: 22 loss 0.0514 W1_grad [0.08 0.32 0.08 0.32 0.08 0.32] W2_grad [0.05 0.22 0.39] b1_grad [0.13 0.13 0.13] b2_grad [0.2]\n",
            "Epoch: 23 loss 0.0489 W1_grad [0.06 0.3  0.06 0.3  0.06 0.3 ] W2_grad [0.04 0.2  0.36] b1_grad [0.12 0.12 0.12] b2_grad [0.18]\n",
            "Epoch: 24 loss 0.0466 W1_grad [0.05 0.29 0.05 0.29 0.05 0.29] W2_grad [0.04 0.19 0.33] b1_grad [0.11 0.11 0.11] b2_grad [0.17]\n",
            "Epoch: 25 loss 0.0447 W1_grad [0.04 0.27 0.04 0.27 0.04 0.27] W2_grad [0.03 0.17 0.31] b1_grad [0.1 0.1 0.1] b2_grad [0.16]\n",
            "Epoch: 26 loss 0.043 W1_grad [0.03 0.26 0.03 0.26 0.03 0.26] W2_grad [0.03 0.16 0.29] b1_grad [0.1 0.1 0.1] b2_grad [0.15]\n",
            "Epoch: 27 loss 0.0415 W1_grad [0.02 0.25 0.02 0.25 0.02 0.25] W2_grad [0.03 0.15 0.27] b1_grad [0.09 0.09 0.09] b2_grad [0.14]\n",
            "Epoch: 28 loss 0.0401 W1_grad [0.02 0.24 0.02 0.24 0.02 0.24] W2_grad [0.02 0.14 0.25] b1_grad [0.09 0.09 0.08] b2_grad [0.13]\n",
            "Epoch: 29 loss 0.0389 W1_grad [0.01 0.23 0.01 0.23 0.01 0.23] W2_grad [0.02 0.13 0.23] b1_grad [0.08 0.08 0.08] b2_grad [0.13]\n",
            "Epoch: 30 loss 0.0378 W1_grad [0.   0.22 0.   0.22 0.   0.22] W2_grad [0.02 0.12 0.22] b1_grad [0.08 0.08 0.07] b2_grad [0.12]\n",
            "Epoch: 31 loss 0.0369 W1_grad [-0.    0.21 -0.    0.21 -0.    0.21] W2_grad [0.02 0.11 0.2 ] b1_grad [0.07 0.07 0.07] b2_grad [0.11]\n",
            "Epoch: 32 loss 0.036 W1_grad [-0.01  0.2  -0.01  0.2  -0.01  0.2 ] W2_grad [0.01 0.1  0.19] b1_grad [0.07 0.07 0.07] b2_grad [0.11]\n",
            "Epoch: 33 loss 0.0352 W1_grad [-0.02  0.2  -0.02  0.19 -0.02  0.19] W2_grad [0.01 0.09 0.17] b1_grad [0.06 0.06 0.06] b2_grad [0.1]\n",
            "Epoch: 34 loss 0.0345 W1_grad [-0.02  0.19 -0.02  0.19 -0.02  0.18] W2_grad [0.01 0.09 0.16] b1_grad [0.06 0.06 0.06] b2_grad [0.09]\n",
            "Epoch: 35 loss 0.0339 W1_grad [-0.03  0.18 -0.03  0.18 -0.03  0.18] W2_grad [0.01 0.08 0.15] b1_grad [0.06 0.06 0.05] b2_grad [0.09]\n",
            "Epoch: 36 loss 0.0333 W1_grad [-0.03  0.18 -0.03  0.17 -0.03  0.17] W2_grad [0.01 0.08 0.14] b1_grad [0.05 0.05 0.05] b2_grad [0.08]\n",
            "Epoch: 37 loss 0.0328 W1_grad [-0.04  0.17 -0.03  0.17 -0.03  0.16] W2_grad [0.01 0.07 0.13] b1_grad [0.05 0.05 0.05] b2_grad [0.08]\n",
            "Epoch: 38 loss 0.0323 W1_grad [-0.04  0.16 -0.04  0.16 -0.04  0.16] W2_grad [0.01 0.06 0.12] b1_grad [0.05 0.05 0.05] b2_grad [0.07]\n",
            "Epoch: 39 loss 0.0318 W1_grad [-0.04  0.16 -0.04  0.16 -0.04  0.15] W2_grad [0.01 0.06 0.11] b1_grad [0.04 0.04 0.04] b2_grad [0.07]\n",
            "Epoch: 40 loss 0.0314 W1_grad [-0.05  0.15 -0.05  0.15 -0.04  0.15] W2_grad [0.   0.06 0.11] b1_grad [0.04 0.04 0.04] b2_grad [0.07]\n",
            "Epoch: 41 loss 0.031 W1_grad [-0.05  0.15 -0.05  0.15 -0.05  0.14] W2_grad [0.   0.05 0.1 ] b1_grad [0.04 0.04 0.04] b2_grad [0.06]\n",
            "Epoch: 42 loss 0.0307 W1_grad [-0.05  0.15 -0.05  0.14 -0.05  0.14] W2_grad [0.   0.05 0.09] b1_grad [0.04 0.04 0.04] b2_grad [0.06]\n",
            "Epoch: 43 loss 0.0303 W1_grad [-0.06  0.14 -0.05  0.14 -0.05  0.13] W2_grad [0.   0.04 0.08] b1_grad [0.04 0.04 0.03] b2_grad [0.06]\n",
            "Epoch: 44 loss 0.03 W1_grad [-0.06  0.14 -0.06  0.13 -0.06  0.13] W2_grad [0.   0.04 0.08] b1_grad [0.03 0.03 0.03] b2_grad [0.05]\n",
            "Epoch: 45 loss 0.0297 W1_grad [-0.06  0.13 -0.06  0.13 -0.06  0.13] W2_grad [0.   0.04 0.07] b1_grad [0.03 0.03 0.03] b2_grad [0.05]\n",
            "Epoch: 46 loss 0.0294 W1_grad [-0.06  0.13 -0.06  0.13 -0.06  0.12] W2_grad [0.   0.03 0.07] b1_grad [0.03 0.03 0.03] b2_grad [0.05]\n",
            "Epoch: 47 loss 0.0292 W1_grad [-0.07  0.13 -0.06  0.12 -0.06  0.12] W2_grad [0.   0.03 0.06] b1_grad [0.03 0.03 0.03] b2_grad [0.05]\n",
            "Epoch: 48 loss 0.0289 W1_grad [-0.07  0.12 -0.07  0.12 -0.06  0.12] W2_grad [-0.    0.03  0.06] b1_grad [0.03 0.03 0.03] b2_grad [0.04]\n",
            "Epoch: 49 loss 0.0287 W1_grad [-0.07  0.12 -0.07  0.12 -0.06  0.12] W2_grad [-0.    0.03  0.05] b1_grad [0.03 0.03 0.02] b2_grad [0.04]\n",
            "Epoch: 50 loss 0.0284 W1_grad [-0.07  0.12 -0.07  0.12 -0.07  0.11] W2_grad [-0.    0.02  0.05] b1_grad [0.03 0.02 0.02] b2_grad [0.04]\n",
            "Epoch: 51 loss 0.0282 W1_grad [-0.07  0.12 -0.07  0.11 -0.07  0.11] W2_grad [-0.    0.02  0.04] b1_grad [0.02 0.02 0.02] b2_grad [0.04]\n",
            "Epoch: 52 loss 0.028 W1_grad [-0.07  0.11 -0.07  0.11 -0.07  0.11] W2_grad [-0.    0.02  0.04] b1_grad [0.02 0.02 0.02] b2_grad [0.04]\n",
            "Epoch: 53 loss 0.0278 W1_grad [-0.07  0.11 -0.07  0.11 -0.07  0.11] W2_grad [-0.    0.02  0.04] b1_grad [0.02 0.02 0.02] b2_grad [0.03]\n",
            "Epoch: 54 loss 0.0276 W1_grad [-0.08  0.11 -0.07  0.11 -0.07  0.1 ] W2_grad [-0.    0.02  0.03] b1_grad [0.02 0.02 0.02] b2_grad [0.03]\n",
            "Epoch: 55 loss 0.0274 W1_grad [-0.08  0.11 -0.07  0.1  -0.07  0.1 ] W2_grad [-0.    0.01  0.03] b1_grad [0.02 0.02 0.02] b2_grad [0.03]\n",
            "Epoch: 56 loss 0.0272 W1_grad [-0.08  0.11 -0.08  0.1  -0.07  0.1 ] W2_grad [-0.    0.01  0.03] b1_grad [0.02 0.02 0.02] b2_grad [0.03]\n",
            "Epoch: 57 loss 0.027 W1_grad [-0.08  0.1  -0.08  0.1  -0.07  0.1 ] W2_grad [-0.    0.01  0.03] b1_grad [0.02 0.02 0.02] b2_grad [0.03]\n",
            "Epoch: 58 loss 0.0268 W1_grad [-0.08  0.1  -0.08  0.1  -0.08  0.1 ] W2_grad [-0.    0.01  0.02] b1_grad [0.02 0.02 0.02] b2_grad [0.03]\n",
            "Epoch: 59 loss 0.0267 W1_grad [-0.08  0.1  -0.08  0.1  -0.08  0.09] W2_grad [-0.    0.01  0.02] b1_grad [0.02 0.02 0.02] b2_grad [0.03]\n",
            "Epoch: 60 loss 0.0265 W1_grad [-0.08  0.1  -0.08  0.1  -0.08  0.09] W2_grad [-0.01  0.01  0.02] b1_grad [0.02 0.02 0.01] b2_grad [0.02]\n",
            "Epoch: 61 loss 0.0263 W1_grad [-0.08  0.1  -0.08  0.09 -0.08  0.09] W2_grad [-0.01  0.01  0.02] b1_grad [0.02 0.01 0.01] b2_grad [0.02]\n",
            "Epoch: 62 loss 0.0262 W1_grad [-0.08  0.1  -0.08  0.09 -0.08  0.09] W2_grad [-0.01  0.    0.01] b1_grad [0.01 0.01 0.01] b2_grad [0.02]\n",
            "Epoch: 63 loss 0.026 W1_grad [-0.08  0.1  -0.08  0.09 -0.08  0.09] W2_grad [-0.01  0.    0.01] b1_grad [0.01 0.01 0.01] b2_grad [0.02]\n",
            "Epoch: 64 loss 0.0258 W1_grad [-0.08  0.09 -0.08  0.09 -0.08  0.09] W2_grad [-0.01  0.    0.01] b1_grad [0.01 0.01 0.01] b2_grad [0.02]\n",
            "Epoch: 65 loss 0.0257 W1_grad [-0.08  0.09 -0.08  0.09 -0.08  0.09] W2_grad [-0.01  0.    0.01] b1_grad [0.01 0.01 0.01] b2_grad [0.02]\n",
            "Epoch: 66 loss 0.0255 W1_grad [-0.09  0.09 -0.08  0.09 -0.08  0.09] W2_grad [-0.01  0.    0.01] b1_grad [0.01 0.01 0.01] b2_grad [0.02]\n",
            "Epoch: 67 loss 0.0254 W1_grad [-0.09  0.09 -0.08  0.09 -0.08  0.08] W2_grad [-0.01 -0.    0.01] b1_grad [0.01 0.01 0.01] b2_grad [0.02]\n",
            "Epoch: 68 loss 0.0252 W1_grad [-0.09  0.09 -0.08  0.09 -0.08  0.08] W2_grad [-0.01 -0.    0.  ] b1_grad [0.01 0.01 0.01] b2_grad [0.02]\n",
            "Epoch: 69 loss 0.0251 W1_grad [-0.13 -0.09 -0.08  0.09 -0.08  0.08] W2_grad [-0.01 -0.    0.  ] b1_grad [-0.06  0.01  0.01] b2_grad [0.02]\n",
            "Epoch: 70 loss 0.025 W1_grad [-0.09  0.09 -0.08  0.09 -0.08  0.08] W2_grad [-0.01 -0.    0.  ] b1_grad [0.01 0.01 0.01] b2_grad [0.02]\n",
            "Epoch: 71 loss 0.0248 W1_grad [-0.09  0.09 -0.08  0.09 -0.08  0.08] W2_grad [-0.01 -0.    0.  ] b1_grad [0.01 0.01 0.01] b2_grad [0.02]\n",
            "Epoch: 72 loss 0.0247 W1_grad [-0.13 -0.09 -0.08  0.08 -0.08  0.08] W2_grad [-0.01 -0.    0.  ] b1_grad [-0.06  0.01  0.01] b2_grad [0.02]\n",
            "Epoch: 73 loss 0.0246 W1_grad [-0.09  0.09 -0.08  0.09 -0.08  0.08] W2_grad [-0.01 -0.    0.  ] b1_grad [0.01 0.01 0.01] b2_grad [0.02]\n",
            "Epoch: 74 loss 0.0244 W1_grad [-0.09  0.09 -0.08  0.08 -0.08  0.08] W2_grad [-0.01 -0.    0.  ] b1_grad [0.01 0.01 0.01] b2_grad [0.02]\n",
            "Epoch: 75 loss 0.0243 W1_grad [-0.13 -0.09 -0.08  0.08 -0.08  0.08] W2_grad [-0.01 -0.    0.  ] b1_grad [-0.06  0.01  0.01] b2_grad [0.02]\n",
            "Epoch: 76 loss 0.0242 W1_grad [-0.08  0.09 -0.08  0.08 -0.08  0.08] W2_grad [-0.01 -0.    0.  ] b1_grad [0.01 0.01 0.01] b2_grad [0.02]\n",
            "Epoch: 77 loss 0.024 W1_grad [-0.09  0.09 -0.08  0.08 -0.08  0.08] W2_grad [-0.01 -0.    0.  ] b1_grad [0.01 0.01 0.01] b2_grad [0.02]\n",
            "Epoch: 78 loss 0.0239 W1_grad [-0.09  0.09 -0.08  0.08 -0.08  0.08] W2_grad [-0.01 -0.01 -0.  ] b1_grad [0.01 0.01 0.01] b2_grad [0.02]\n",
            "Epoch: 79 loss 0.0238 W1_grad [-0.12 -0.09 -0.08  0.08 -0.08  0.08] W2_grad [-0.01 -0.01 -0.  ] b1_grad [-0.06  0.01  0.01] b2_grad [0.02]\n",
            "Epoch: 80 loss 0.0236 W1_grad [-0.08  0.09 -0.08  0.08 -0.08  0.08] W2_grad [-0.01 -0.01 -0.  ] b1_grad [0.01 0.01 0.01] b2_grad [0.02]\n",
            "Epoch: 81 loss 0.0235 W1_grad [-0.08  0.08 -0.08  0.08 -0.08  0.08] W2_grad [-0.01 -0.01 -0.  ] b1_grad [0.01 0.01 0.01] b2_grad [0.02]\n",
            "Epoch: 82 loss 0.0234 W1_grad [-0.12 -0.09 -0.08  0.08 -0.08  0.08] W2_grad [-0.01 -0.01 -0.  ] b1_grad [-0.05  0.01  0.01] b2_grad [0.02]\n",
            "Epoch: 83 loss 0.0233 W1_grad [-0.08  0.08 -0.08  0.08 -0.08  0.08] W2_grad [-0.01 -0.01 -0.  ] b1_grad [0.01 0.01 0.01] b2_grad [0.02]\n",
            "Epoch: 84 loss 0.0231 W1_grad [-0.08  0.08 -0.08  0.08 -0.08  0.08] W2_grad [-0.01 -0.01 -0.  ] b1_grad [0.01 0.01 0.01] b2_grad [0.02]\n",
            "Epoch: 85 loss 0.023 W1_grad [-0.12 -0.09 -0.08  0.08 -0.08  0.08] W2_grad [-0.01 -0.01 -0.  ] b1_grad [-0.05  0.01  0.01] b2_grad [0.02]\n",
            "Epoch: 86 loss 0.0229 W1_grad [-0.08  0.08 -0.08  0.08 -0.08  0.08] W2_grad [-0.01 -0.01 -0.  ] b1_grad [0.01 0.01 0.01] b2_grad [0.02]\n",
            "Epoch: 87 loss 0.0228 W1_grad [-0.08  0.08 -0.08  0.08 -0.08  0.08] W2_grad [-0.01 -0.01 -0.  ] b1_grad [0.01 0.01 0.01] b2_grad [0.02]\n",
            "Epoch: 88 loss 0.0226 W1_grad [-0.08  0.08 -0.08  0.08 -0.08  0.08] W2_grad [-0.01 -0.01 -0.  ] b1_grad [0.01 0.01 0.01] b2_grad [0.02]\n",
            "Epoch: 89 loss 0.0225 W1_grad [-0.12 -0.09 -0.08  0.08 -0.08  0.08] W2_grad [-0.01 -0.01 -0.01] b1_grad [-0.05  0.01  0.01] b2_grad [0.02]\n",
            "Epoch: 90 loss 0.0224 W1_grad [-0.08  0.08 -0.08  0.08 -0.08  0.08] W2_grad [-0.01 -0.01 -0.  ] b1_grad [0.01 0.01 0.01] b2_grad [0.02]\n",
            "Epoch: 91 loss 0.0223 W1_grad [-0.08  0.08 -0.08  0.08 -0.08  0.08] W2_grad [-0.01 -0.01 -0.  ] b1_grad [0.01 0.01 0.01] b2_grad [0.02]\n",
            "Epoch: 92 loss 0.0222 W1_grad [-0.12 -0.09 -0.08  0.08 -0.08  0.08] W2_grad [-0.01 -0.01 -0.01] b1_grad [-0.05  0.01  0.01] b2_grad [0.02]\n",
            "Epoch: 93 loss 0.022 W1_grad [-0.08  0.08 -0.08  0.08 -0.08  0.08] W2_grad [-0.01 -0.01 -0.  ] b1_grad [0.01 0.01 0.01] b2_grad [0.02]\n",
            "Epoch: 94 loss 0.0219 W1_grad [-0.08  0.08 -0.08  0.08 -0.08  0.08] W2_grad [-0.01 -0.01 -0.01] b1_grad [0.01 0.01 0.01] b2_grad [0.02]\n",
            "Epoch: 95 loss 0.0218 W1_grad [-0.12 -0.09 -0.08  0.08 -0.08  0.07] W2_grad [-0.01 -0.01 -0.01] b1_grad [-0.05  0.01  0.01] b2_grad [0.01]\n",
            "Epoch: 96 loss 0.0217 W1_grad [-0.08  0.08 -0.08  0.08 -0.08  0.08] W2_grad [-0.01 -0.01 -0.  ] b1_grad [0.01 0.01 0.01] b2_grad [0.02]\n",
            "Epoch: 97 loss 0.0216 W1_grad [-0.08  0.08 -0.08  0.08 -0.08  0.07] W2_grad [-0.01 -0.01 -0.01] b1_grad [0.01 0.01 0.01] b2_grad [0.02]\n",
            "Epoch: 98 loss 0.0214 W1_grad [-0.08  0.08 -0.08  0.08 -0.08  0.07] W2_grad [-0.01 -0.01 -0.01] b1_grad [0.01 0.01 0.01] b2_grad [0.01]\n",
            "Epoch: 99 loss 0.0213 W1_grad [-0.12 -0.09 -0.08  0.08 -0.08  0.07] W2_grad [-0.01 -0.01 -0.01] b1_grad [-0.05  0.01  0.01] b2_grad [0.01]\n",
            "Epoch: 100 loss 0.0212 W1_grad [-0.08  0.08 -0.08  0.08 -0.08  0.07] W2_grad [-0.01 -0.01 -0.01] b1_grad [0.01 0.01 0.01] b2_grad [0.02]\n"
          ]
        }
      ]
    }
  ]
}