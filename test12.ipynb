{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPM0qnsE2YJLqIrs5jS+gUn"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5W3dMidLQA-a"
      },
      "outputs": [],
      "source": [
        "#考虑一个用于回归任务的简单神经网络，结构如下：\n",
        "\n",
        "#输入层： 2 个神经元\n",
        "#隐藏层： 2 个神经元，使用 ReLU 激活函数\n",
        "#输出层： 1 个神经元，线性输出"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "5GTC9h-rQLR5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#relu激活函数\n",
        "def relu(a):\n",
        "  return np.maximum(0, a)"
      ],
      "metadata": {
        "id": "hbGOXC3LQ-0c"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#前向传播，输出y_pred\n",
        "def forward(W1, W2, b1, b2, X):\n",
        "  Z = np.dot(X, W1.T) + b1 #shape (2,2)\n",
        "  H = relu(Z) #shape (2,2)\n",
        "\n",
        "  y_pred = np.dot(H, W2.T) + b2 #shape (2,1)\n",
        "\n",
        "  return H, Z, y_pred"
      ],
      "metadata": {
        "id": "M2bIknjJRjMb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#计算损失loss函数\n",
        "def compute_loss(y_pred, y_true):\n",
        "  loss = (y_pred - y_true)**2 / 2\n",
        "  return loss"
      ],
      "metadata": {
        "id": "Hlbr8KgVTB1N"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#计算反向传播,梯度计算\n",
        "def backward(y_pred, y_true, W2, H, Z, X):\n",
        "  y_diff = y_pred - y_true #shape (2,1)\n",
        "\n",
        "  b2_grad = np.mean(y_diff, axis=0, keepdims=True) #shape (1,1)\n",
        "  W2_grad = np.dot(y_diff.T, H) #shepe (2,1)\n",
        "\n",
        "  dh = np.dot(y_diff, W2) #shape (2,2)\n",
        "  dz = dh * (Z>0)\n",
        "  b1_grad = np.mean(dz, axis=0, keepdims=True).T #shape (2,1)\n",
        "  W1_grad = np.dot(dz, X.T) #shape (2,2)\n",
        "\n",
        "  return W1_grad, W2_grad, b1_grad, b2_grad"
      ],
      "metadata": {
        "id": "O-kFE2F5TjjK"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#更新参数weight, bias的值\n",
        "def update_params(W1, W2, b1, b2, W1_grad, W2_grad, b1_grad, b2_grad):\n",
        "  W1 -= learning_rate * W1_grad\n",
        "  W2 -= learning_rate * W2_grad\n",
        "  b1 -= learning_rate * b1_grad\n",
        "  b2 -= learning_rate * b2_grad\n",
        "  return W1, W2, b1, b2"
      ],
      "metadata": {
        "id": "en-qXDiVXD3c"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([[1.0, 2.0],\n",
        "        [2.0, 1.0]])\n",
        "\n",
        "y_true = np.array([[3.0],\n",
        "        [2.5]])\n",
        "\n",
        "W1 = np.array([[0.1, 0.2],\n",
        "        [0.3, 0.4]]) #(输入层到隐藏层) ndarray (2,2)\n",
        "\n",
        "b1 = np.array([[0.1],\n",
        "        [0.2]]) #(隐藏层偏置) ndarray (2,1)\n",
        "\n",
        "W2 = np.array([[0.5, 0.6]]) #(隐藏层到输出层) ndarray (1,2)\n",
        "\n",
        "b2 = np.array([[0.3]]) #(输出层偏置) ndarray (1,1)\n",
        "\n",
        "\n",
        "learning_rate = 0.05\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  H, Z, y_pred = forward(W1, W2, b1, b2, X)\n",
        "  loss = compute_loss(y_pred, y_true)\n",
        "  W1_grad, W2_grad, b1_grad, b2_grad = backward(y_pred, y_true, W2, H, Z, X)\n",
        "  W1, W2, b1, b2 = update_params(W1, W2, b1, b2, W1_grad, W2_grad, b1_grad, b2_grad)\n",
        "\n",
        "  print(\"Epoch:\", epoch + 1,\n",
        "    \"loss:\", np.round(np.mean(loss), 4),\n",
        "    \"W1_grad:\", np.round(W1_grad.flatten(), 2),\n",
        "    \"W2_grad:\", np.round(W2_grad.flatten(), 2),\n",
        "    \"b1_grad:\", np.round(b1_grad.flatten(), 2),\n",
        "    \"b2_grad:\", np.round(b2_grad.flatten(), 2)\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GUsSBkdQPjw",
        "outputId": "1d9d1351-f01d-4979-c704-116d1a21a839"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 loss: 1.0537 W1_grad: [-2.86 -2.69 -2.01 -1.89] W2_grad: [-1.72 -3.43] b1_grad: [-0.72 -0.86] b2_grad: [-1.43]\n",
            "Epoch: 2 loss: 0.2012 W1_grad: [-1.78 -1.63 -0.68 -0.62] W2_grad: [-1.22 -1.77] b1_grad: [-0.34 -0.45] b2_grad: [-0.58]\n",
            "Epoch: 3 loss: 0.0386 W1_grad: [-0.8  -0.73  0.47  0.42] W2_grad: [-0.18 -0.23] b1_grad: [-0.05 -0.06] b2_grad: [-0.07]\n",
            "Epoch: 4 loss: 0.0365 W1_grad: [-0.68 -0.62  0.61  0.56] W2_grad: [-0.04 -0.04] b1_grad: [-0.01 -0.01] b2_grad: [-0.02]\n",
            "Epoch: 5 loss: 0.0365 W1_grad: [-0.7  -0.63  0.6   0.55] W2_grad: [-0.05 -0.05] b1_grad: [-0.01 -0.02] b2_grad: [-0.02]\n",
            "Epoch: 6 loss: 0.0365 W1_grad: [-0.7  -0.63  0.6   0.55] W2_grad: [-0.05 -0.05] b1_grad: [-0.01 -0.02] b2_grad: [-0.02]\n",
            "Epoch: 7 loss: 0.0366 W1_grad: [-0.7  -0.64  0.61  0.55] W2_grad: [-0.06 -0.05] b1_grad: [-0.01 -0.02] b2_grad: [-0.02]\n",
            "Epoch: 8 loss: 0.0366 W1_grad: [-0.7  -0.64  0.61  0.55] W2_grad: [-0.06 -0.04] b1_grad: [-0.01 -0.02] b2_grad: [-0.02]\n",
            "Epoch: 9 loss: 0.0366 W1_grad: [-0.7  -0.64  0.61  0.56] W2_grad: [-0.06 -0.04] b1_grad: [-0.01 -0.02] b2_grad: [-0.02]\n",
            "Epoch: 10 loss: 0.0366 W1_grad: [-0.7  -0.64  0.61  0.56] W2_grad: [-0.06 -0.04] b1_grad: [-0.01 -0.02] b2_grad: [-0.02]\n",
            "Epoch: 11 loss: 0.0366 W1_grad: [-0.71 -0.64  0.62  0.56] W2_grad: [-0.06 -0.03] b1_grad: [-0.01 -0.02] b2_grad: [-0.02]\n",
            "Epoch: 12 loss: 0.0366 W1_grad: [-0.71 -0.65  0.62  0.57] W2_grad: [-0.06 -0.03] b1_grad: [-0.01 -0.02] b2_grad: [-0.02]\n",
            "Epoch: 13 loss: 0.0367 W1_grad: [-0.71 -0.65  0.62  0.57] W2_grad: [-0.06 -0.03] b1_grad: [-0.01 -0.02] b2_grad: [-0.02]\n",
            "Epoch: 14 loss: 0.0367 W1_grad: [-0.71 -0.65  0.63  0.57] W2_grad: [-0.07 -0.03] b1_grad: [-0.01 -0.02] b2_grad: [-0.02]\n",
            "Epoch: 15 loss: 0.0367 W1_grad: [-0.71 -0.65  0.63  0.58] W2_grad: [-0.07 -0.02] b1_grad: [-0.01 -0.01] b2_grad: [-0.02]\n",
            "Epoch: 16 loss: 0.0367 W1_grad: [-0.71 -0.65  0.63  0.58] W2_grad: [-0.07 -0.02] b1_grad: [-0.01 -0.01] b2_grad: [-0.02]\n",
            "Epoch: 17 loss: 0.0367 W1_grad: [-0.71 -0.65  0.63  0.58] W2_grad: [-0.06 -0.02] b1_grad: [-0.01 -0.01] b2_grad: [-0.02]\n",
            "Epoch: 18 loss: 0.0368 W1_grad: [-0.71 -0.65  0.64  0.59] W2_grad: [-0.06 -0.01] b1_grad: [-0.01 -0.01] b2_grad: [-0.01]\n",
            "Epoch: 19 loss: 0.0368 W1_grad: [-0.71 -0.66  0.64  0.59] W2_grad: [-0.06 -0.01] b1_grad: [-0.01 -0.01] b2_grad: [-0.01]\n",
            "Epoch: 20 loss: 0.0368 W1_grad: [-0.71 -0.66  0.64  0.59] W2_grad: [-0.06 -0.01] b1_grad: [-0.01 -0.01] b2_grad: [-0.01]\n",
            "Epoch: 21 loss: 0.0368 W1_grad: [-0.71 -0.66  0.64  0.6 ] W2_grad: [-0.06 -0.01] b1_grad: [-0.01 -0.01] b2_grad: [-0.01]\n",
            "Epoch: 22 loss: 0.0397 W1_grad: [-0.2 -0.4  0.2  0.4] W2_grad: [0.02 0.  ] b1_grad: [-0.  0.] b2_grad: [-0.]\n",
            "Epoch: 23 loss: 0.0391 W1_grad: [-0.18 -0.35  0.22  0.43] W2_grad: [0.21 0.  ] b1_grad: [0.02 0.  ] b2_grad: [0.03]\n",
            "Epoch: 24 loss: 0.038 W1_grad: [-0.18 -0.36  0.21  0.41] W2_grad: [0.15 0.  ] b1_grad: [0.01 0.  ] b2_grad: [0.02]\n",
            "Epoch: 25 loss: 0.0372 W1_grad: [-0.17 -0.35  0.2   0.4 ] W2_grad: [0.16 0.  ] b1_grad: [0.01 0.  ] b2_grad: [0.02]\n",
            "Epoch: 26 loss: 0.0364 W1_grad: [-0.17 -0.34  0.2   0.39] W2_grad: [0.15 0.  ] b1_grad: [0.01 0.  ] b2_grad: [0.02]\n",
            "Epoch: 27 loss: 0.0356 W1_grad: [-0.17 -0.33  0.19  0.38] W2_grad: [0.14 0.  ] b1_grad: [0.01 0.  ] b2_grad: [0.02]\n",
            "Epoch: 28 loss: 0.0349 W1_grad: [-0.16 -0.33  0.19  0.38] W2_grad: [0.14 0.  ] b1_grad: [0.01 0.  ] b2_grad: [0.02]\n",
            "Epoch: 29 loss: 0.0342 W1_grad: [-0.16 -0.32  0.18  0.37] W2_grad: [0.13 0.  ] b1_grad: [0.01 0.  ] b2_grad: [0.02]\n",
            "Epoch: 30 loss: 0.0335 W1_grad: [-0.16 -0.32  0.18  0.36] W2_grad: [0.13 0.  ] b1_grad: [0.01 0.  ] b2_grad: [0.02]\n",
            "Epoch: 31 loss: 0.0328 W1_grad: [-0.16 -0.31  0.18  0.35] W2_grad: [0.12 0.  ] b1_grad: [0.01 0.  ] b2_grad: [0.02]\n",
            "Epoch: 32 loss: 0.0322 W1_grad: [-0.15 -0.31  0.17  0.35] W2_grad: [0.12 0.  ] b1_grad: [0.01 0.  ] b2_grad: [0.02]\n",
            "Epoch: 33 loss: 0.0316 W1_grad: [-0.15 -0.3   0.17  0.34] W2_grad: [0.11 0.  ] b1_grad: [0.01 0.  ] b2_grad: [0.02]\n",
            "Epoch: 34 loss: 0.0311 W1_grad: [-0.15 -0.3   0.17  0.33] W2_grad: [0.11 0.  ] b1_grad: [0.01 0.  ] b2_grad: [0.01]\n",
            "Epoch: 35 loss: 0.0305 W1_grad: [-0.15 -0.29  0.16  0.33] W2_grad: [0.11 0.  ] b1_grad: [0.01 0.  ] b2_grad: [0.01]\n",
            "Epoch: 36 loss: 0.03 W1_grad: [-0.14 -0.29  0.16  0.32] W2_grad: [0.1 0. ] b1_grad: [0.01 0.  ] b2_grad: [0.01]\n",
            "Epoch: 37 loss: 0.0295 W1_grad: [-0.14 -0.28  0.16  0.32] W2_grad: [0.1 0. ] b1_grad: [0.01 0.  ] b2_grad: [0.01]\n",
            "Epoch: 38 loss: 0.029 W1_grad: [-0.14 -0.28  0.16  0.31] W2_grad: [0.1 0. ] b1_grad: [0.01 0.  ] b2_grad: [0.01]\n",
            "Epoch: 39 loss: 0.0286 W1_grad: [-0.14 -0.27  0.15  0.3 ] W2_grad: [0.09 0.  ] b1_grad: [0.01 0.  ] b2_grad: [0.01]\n",
            "Epoch: 40 loss: 0.0281 W1_grad: [-0.13 -0.27  0.15  0.3 ] W2_grad: [0.09 0.  ] b1_grad: [0.01 0.  ] b2_grad: [0.01]\n",
            "Epoch: 41 loss: 0.0277 W1_grad: [-0.13 -0.27  0.15  0.3 ] W2_grad: [0.09 0.  ] b1_grad: [0.01 0.  ] b2_grad: [0.01]\n",
            "Epoch: 42 loss: 0.0273 W1_grad: [-0.13 -0.26  0.15  0.29] W2_grad: [0.08 0.  ] b1_grad: [0.01 0.  ] b2_grad: [0.01]\n",
            "Epoch: 43 loss: 0.0269 W1_grad: [-0.13 -0.26  0.14  0.29] W2_grad: [0.08 0.  ] b1_grad: [0.01 0.  ] b2_grad: [0.01]\n",
            "Epoch: 44 loss: 0.0265 W1_grad: [-0.13 -0.25  0.14  0.28] W2_grad: [0.08 0.  ] b1_grad: [0.01 0.  ] b2_grad: [0.01]\n",
            "Epoch: 45 loss: 0.0261 W1_grad: [-0.13 -0.25  0.14  0.28] W2_grad: [0.08 0.  ] b1_grad: [0.01 0.  ] b2_grad: [0.01]\n",
            "Epoch: 46 loss: 0.0258 W1_grad: [-0.12 -0.25  0.14  0.27] W2_grad: [0.08 0.  ] b1_grad: [0.01 0.  ] b2_grad: [0.01]\n",
            "Epoch: 47 loss: 0.0254 W1_grad: [-0.12 -0.24  0.14  0.27] W2_grad: [0.07 0.  ] b1_grad: [0.01 0.  ] b2_grad: [0.01]\n",
            "Epoch: 48 loss: 0.0251 W1_grad: [-0.12 -0.24  0.13  0.27] W2_grad: [0.07 0.  ] b1_grad: [0.01 0.  ] b2_grad: [0.01]\n",
            "Epoch: 49 loss: 0.0248 W1_grad: [-0.12 -0.24  0.13  0.26] W2_grad: [0.07 0.  ] b1_grad: [0.01 0.  ] b2_grad: [0.01]\n",
            "Epoch: 50 loss: 0.0245 W1_grad: [-0.12 -0.24  0.13  0.26] W2_grad: [0.07 0.  ] b1_grad: [0.01 0.  ] b2_grad: [0.01]\n",
            "Epoch: 51 loss: 0.0242 W1_grad: [-0.12 -0.23  0.13  0.26] W2_grad: [0.07 0.  ] b1_grad: [0.01 0.  ] b2_grad: [0.01]\n",
            "Epoch: 52 loss: 0.0239 W1_grad: [-0.12 -0.23  0.13  0.25] W2_grad: [0.06 0.  ] b1_grad: [0.01 0.  ] b2_grad: [0.01]\n",
            "Epoch: 53 loss: 0.0236 W1_grad: [-0.11 -0.23  0.13  0.25] W2_grad: [0.06 0.  ] b1_grad: [0.01 0.  ] b2_grad: [0.01]\n",
            "Epoch: 54 loss: 0.0233 W1_grad: [-0.11 -0.23  0.12  0.25] W2_grad: [0.06 0.  ] b1_grad: [0.01 0.  ] b2_grad: [0.01]\n",
            "Epoch: 55 loss: 0.023 W1_grad: [-0.11 -0.22  0.12  0.24] W2_grad: [0.06 0.  ] b1_grad: [0.01 0.  ] b2_grad: [0.01]\n",
            "Epoch: 56 loss: 0.0227 W1_grad: [-0.11 -0.22  0.12  0.24] W2_grad: [0.06 0.  ] b1_grad: [0.01 0.  ] b2_grad: [0.01]\n",
            "Epoch: 57 loss: 0.0225 W1_grad: [-0.11 -0.22  0.12  0.24] W2_grad: [0.06 0.  ] b1_grad: [0.01 0.  ] b2_grad: [0.01]\n",
            "Epoch: 58 loss: 0.0222 W1_grad: [-0.11 -0.22  0.12  0.24] W2_grad: [0.06 0.  ] b1_grad: [0.01 0.  ] b2_grad: [0.01]\n",
            "Epoch: 59 loss: 0.022 W1_grad: [-0.11 -0.21  0.12  0.23] W2_grad: [0.06 0.  ] b1_grad: [0.01 0.  ] b2_grad: [0.01]\n",
            "Epoch: 60 loss: 0.0218 W1_grad: [-0.11 -0.21  0.12  0.23] W2_grad: [0.05 0.  ] b1_grad: [0.01 0.  ] b2_grad: [0.01]\n",
            "Epoch: 61 loss: 0.0215 W1_grad: [-0.1  -0.21  0.11  0.23] W2_grad: [0.05 0.  ] b1_grad: [0.01 0.  ] b2_grad: [0.01]\n",
            "Epoch: 62 loss: 0.0213 W1_grad: [-0.1  -0.21  0.11  0.23] W2_grad: [0.05 0.  ] b1_grad: [0.01 0.  ] b2_grad: [0.01]\n",
            "Epoch: 63 loss: 0.0211 W1_grad: [-0.1  -0.2   0.11  0.22] W2_grad: [0.05 0.  ] b1_grad: [0. 0.] b2_grad: [0.01]\n",
            "Epoch: 64 loss: 0.0209 W1_grad: [-0.1  -0.2   0.11  0.22] W2_grad: [0.05 0.  ] b1_grad: [0. 0.] b2_grad: [0.01]\n",
            "Epoch: 65 loss: 0.0207 W1_grad: [-0.1  -0.2   0.11  0.22] W2_grad: [0.05 0.  ] b1_grad: [0. 0.] b2_grad: [0.01]\n",
            "Epoch: 66 loss: 0.0204 W1_grad: [-0.1  -0.2   0.11  0.22] W2_grad: [0.05 0.  ] b1_grad: [0. 0.] b2_grad: [0.01]\n",
            "Epoch: 67 loss: 0.0202 W1_grad: [-0.1  -0.2   0.11  0.22] W2_grad: [0.05 0.  ] b1_grad: [0. 0.] b2_grad: [0.01]\n",
            "Epoch: 68 loss: 0.0201 W1_grad: [-0.1  -0.19  0.11  0.21] W2_grad: [0.05 0.  ] b1_grad: [0. 0.] b2_grad: [0.01]\n",
            "Epoch: 69 loss: 0.0199 W1_grad: [-0.1  -0.19  0.11  0.21] W2_grad: [0.05 0.  ] b1_grad: [0. 0.] b2_grad: [0.01]\n",
            "Epoch: 70 loss: 0.0197 W1_grad: [-0.1  -0.19  0.1   0.21] W2_grad: [0.04 0.  ] b1_grad: [0. 0.] b2_grad: [0.01]\n",
            "Epoch: 71 loss: 0.0195 W1_grad: [-0.09 -0.19  0.1   0.21] W2_grad: [0.04 0.  ] b1_grad: [0. 0.] b2_grad: [0.01]\n",
            "Epoch: 72 loss: 0.0193 W1_grad: [-0.09 -0.19  0.1   0.21] W2_grad: [0.04 0.  ] b1_grad: [0. 0.] b2_grad: [0.01]\n",
            "Epoch: 73 loss: 0.0191 W1_grad: [-0.09 -0.19  0.1   0.2 ] W2_grad: [0.04 0.  ] b1_grad: [0. 0.] b2_grad: [0.01]\n",
            "Epoch: 74 loss: 0.019 W1_grad: [-0.09 -0.18  0.1   0.2 ] W2_grad: [0.04 0.  ] b1_grad: [0. 0.] b2_grad: [0.01]\n",
            "Epoch: 75 loss: 0.0188 W1_grad: [-0.09 -0.18  0.1   0.2 ] W2_grad: [0.04 0.  ] b1_grad: [0. 0.] b2_grad: [0.01]\n",
            "Epoch: 76 loss: 0.0186 W1_grad: [-0.09 -0.18  0.1   0.2 ] W2_grad: [0.04 0.  ] b1_grad: [0. 0.] b2_grad: [0.01]\n",
            "Epoch: 77 loss: 0.0185 W1_grad: [-0.09 -0.18  0.1   0.2 ] W2_grad: [0.04 0.  ] b1_grad: [0. 0.] b2_grad: [0.01]\n",
            "Epoch: 78 loss: 0.0183 W1_grad: [-0.09 -0.18  0.1   0.2 ] W2_grad: [0.04 0.  ] b1_grad: [0. 0.] b2_grad: [0.01]\n",
            "Epoch: 79 loss: 0.0182 W1_grad: [-0.09 -0.18  0.1   0.19] W2_grad: [0.04 0.  ] b1_grad: [0. 0.] b2_grad: [0.01]\n",
            "Epoch: 80 loss: 0.018 W1_grad: [-0.09 -0.18  0.1   0.19] W2_grad: [0.04 0.  ] b1_grad: [0. 0.] b2_grad: [0.01]\n",
            "Epoch: 81 loss: 0.0179 W1_grad: [-0.09 -0.17  0.1   0.19] W2_grad: [0.04 0.  ] b1_grad: [0. 0.] b2_grad: [0.01]\n",
            "Epoch: 82 loss: 0.0177 W1_grad: [-0.09 -0.17  0.09  0.19] W2_grad: [0.04 0.  ] b1_grad: [0. 0.] b2_grad: [0.01]\n",
            "Epoch: 83 loss: 0.0176 W1_grad: [-0.09 -0.17  0.09  0.19] W2_grad: [0.04 0.  ] b1_grad: [0. 0.] b2_grad: [0.01]\n",
            "Epoch: 84 loss: 0.0174 W1_grad: [-0.09 -0.17  0.09  0.19] W2_grad: [0.03 0.  ] b1_grad: [0. 0.] b2_grad: [0.01]\n",
            "Epoch: 85 loss: 0.0173 W1_grad: [-0.08 -0.17  0.09  0.19] W2_grad: [0.03 0.  ] b1_grad: [0. 0.] b2_grad: [0.01]\n",
            "Epoch: 86 loss: 0.0172 W1_grad: [-0.08 -0.17  0.09  0.18] W2_grad: [0.03 0.  ] b1_grad: [0. 0.] b2_grad: [0.01]\n",
            "Epoch: 87 loss: 0.017 W1_grad: [-0.08 -0.17  0.09  0.18] W2_grad: [0.03 0.  ] b1_grad: [0. 0.] b2_grad: [0.01]\n",
            "Epoch: 88 loss: 0.0169 W1_grad: [-0.08 -0.17  0.09  0.18] W2_grad: [0.03 0.  ] b1_grad: [0. 0.] b2_grad: [0.01]\n",
            "Epoch: 89 loss: 0.0168 W1_grad: [-0.08 -0.16  0.09  0.18] W2_grad: [0.03 0.  ] b1_grad: [0. 0.] b2_grad: [0.01]\n",
            "Epoch: 90 loss: 0.0167 W1_grad: [-0.08 -0.16  0.09  0.18] W2_grad: [0.03 0.  ] b1_grad: [0. 0.] b2_grad: [0.01]\n",
            "Epoch: 91 loss: 0.0165 W1_grad: [-0.08 -0.16  0.09  0.18] W2_grad: [0.03 0.  ] b1_grad: [0. 0.] b2_grad: [0.01]\n",
            "Epoch: 92 loss: 0.0164 W1_grad: [-0.08 -0.16  0.09  0.18] W2_grad: [0.03 0.  ] b1_grad: [0. 0.] b2_grad: [0.01]\n",
            "Epoch: 93 loss: 0.0163 W1_grad: [-0.08 -0.16  0.09  0.17] W2_grad: [0.03 0.  ] b1_grad: [0. 0.] b2_grad: [0.01]\n",
            "Epoch: 94 loss: 0.0162 W1_grad: [-0.08 -0.16  0.09  0.17] W2_grad: [0.03 0.  ] b1_grad: [0. 0.] b2_grad: [0.01]\n",
            "Epoch: 95 loss: 0.0161 W1_grad: [-0.08 -0.16  0.09  0.17] W2_grad: [0.03 0.  ] b1_grad: [0. 0.] b2_grad: [0.01]\n",
            "Epoch: 96 loss: 0.0159 W1_grad: [-0.08 -0.16  0.09  0.17] W2_grad: [0.03 0.  ] b1_grad: [0. 0.] b2_grad: [0.01]\n",
            "Epoch: 97 loss: 0.0158 W1_grad: [-0.08 -0.15  0.09  0.17] W2_grad: [0.03 0.  ] b1_grad: [0. 0.] b2_grad: [0.01]\n",
            "Epoch: 98 loss: 0.0157 W1_grad: [-0.08 -0.15  0.08  0.17] W2_grad: [0.02 0.  ] b1_grad: [0. 0.] b2_grad: [0.01]\n",
            "Epoch: 99 loss: 0.0156 W1_grad: [-0.08 -0.15  0.08  0.17] W2_grad: [0.05 0.  ] b1_grad: [0. 0.] b2_grad: [0.01]\n",
            "Epoch: 100 loss: 0.0155 W1_grad: [-0.08 -0.15  0.08  0.16] W2_grad: [-0.  0.] b1_grad: [0. 0.] b2_grad: [0.01]\n"
          ]
        }
      ]
    }
  ]
}