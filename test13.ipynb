{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNGsSWGKRPUF30L07fJa8lT"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISr8P0x6cC3U"
      },
      "outputs": [],
      "source": [
        "#假设有一个简单的神经网络，\n",
        "#包含一个输入层（2个神经元）\n",
        "#一个隐藏层（3个神经元，使用 ReLU 激活函数）\n",
        "#一个输出层（1个神经元，使用线性激活函数）。\n",
        "\n",
        "#我们用均方误差（Mean Squared Error, MSE）作为损失函数。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "ClD9dBjTcja-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#求relu激活函数\n",
        "def relu(a):\n",
        "  return np.maximum(0, a)"
      ],
      "metadata": {
        "id": "-NszRB2Dd-eN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#前向传播，求输出y_pred\n",
        "\n",
        "def forward(W1, W2, b1, b2, X):\n",
        "  Z = np.dot(X, W1) + b1 #shape (1,3)\n",
        "  H = relu(Z) #shape (3,)\n",
        "\n",
        "  y_pred = np.mean(W2 * H)\n",
        "  return Z, H, y_pred"
      ],
      "metadata": {
        "id": "P9e_h3a2eVhW"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#求损失函数, loss值\n",
        "def compute_loss(y_pred, y_true):\n",
        "  loss = (y_pred - y_true)**2 / 2\n",
        "  return loss"
      ],
      "metadata": {
        "id": "ocj-M_Azj9dP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#反向传播，求梯度函数\n",
        "def backward(y_pred, y_true, H, Z, W2, X):\n",
        "  b2_grad = y_pred - y_true\n",
        "  W2_grad = b2_grad * H\n",
        "\n",
        "  b1_grad = b2_grad * W2 * (Z>0)\n",
        "  W1_grad = np.outer(X, b1_grad)\n",
        "\n",
        "  return W1_grad, W2_grad, b1_grad, b2_grad"
      ],
      "metadata": {
        "id": "u1ao42KDk84P"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#更新参数，weight,bias\n",
        "def update_params(W1, W2, b1, b2, W1_grad, W2_grad, b1_grad, b2_grad, lr):\n",
        "  W1 -= lr * W1_grad\n",
        "  W2 -= lr * W2_grad\n",
        "  b1 -= lr * b1_grad\n",
        "  b2 -= lr * b2_grad\n",
        "  return W1, W2, b1, b2"
      ],
      "metadata": {
        "id": "57cwWCulpADo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([1.0, 2.0]) #ndarray(2,)（一个样本，2个特征）\n",
        "\n",
        "y_true = 3.0\n",
        "\n",
        "W1 = np.array([[0.5, 0.1, -0.3],\n",
        "        [0.2, 0.4, 0.6]]) #ndarray(2,3)\n",
        "\n",
        "b1 = np.array([0.1, 0.2, 0.3]) #ndarray(3,)\n",
        "\n",
        "W2 = np.array([0.3, 0.7, -0.5]) #ndarray(3,) （形状为 3×1）\n",
        "\n",
        "lr = 0.01\n",
        "\n",
        "b2 = 0.1\n",
        "\n",
        "epochs = 50\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  Z, H, y_pred = forward(W1, W2, b1, b2, X)\n",
        "  loss = compute_loss(y_pred, y_true)\n",
        "  W1_grad, W2_grad, b1_grad, b2_grad = backward(y_pred, y_true, H, Z, W2, X)\n",
        "  W1, W2, b1, b2 = update_params(W1, W2, b1, b2, W1_grad, W2_grad, b1_grad, b2_grad, lr)\n",
        "\n",
        "  print(\"Epoch:\", epoch + 1,\n",
        "    \"loss:\", np.round(loss, 4),\n",
        "    \"W1_grad\", np.round(W1_grad.flatten(), 2),\n",
        "    \"W2_grad\", np.round(W2_grad.flatten(), 2),\n",
        "    \"b1_grad\", np.round(b1_grad.flatten(), 2),\n",
        "    \"b2_grad\", np.round(b2_grad.flatten(), 2)\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-s5JDu_VcnlW",
        "outputId": "74027831-3872-4edd-f9c4-26620423a8b8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 loss: 4.0423 W1_grad [-0.85 -1.99  1.42 -1.71 -3.98  2.84] W2_grad [-2.84 -3.13 -3.41] b1_grad [-0.85 -1.99  1.42] b2_grad [-2.84]\n",
            "Epoch: 2 loss: 3.811 W1_grad [-0.91 -2.02  1.29 -1.81 -4.04  2.57] W2_grad [-2.9  -3.37 -3.08] b1_grad [-0.91 -2.02  1.29] b2_grad [-2.76]\n",
            "Epoch: 3 loss: 3.583 W1_grad [-0.96 -2.05  1.16 -1.91 -4.1   2.33] W2_grad [-2.96 -3.59 -2.78] b1_grad [-0.96 -2.05  1.16] b2_grad [-2.68]\n",
            "Epoch: 4 loss: 3.3558 W1_grad [-1.   -2.07  1.06 -2.01 -4.15  2.11] W2_grad [-3.01 -3.79 -2.51] b1_grad [-1.   -2.07  1.06] b2_grad [-2.59]\n",
            "Epoch: 5 loss: 3.128 W1_grad [-1.04 -2.1   0.96 -2.09 -4.2   1.91] W2_grad [-3.06 -3.97 -2.26] b1_grad [-1.04 -2.1   0.96] b2_grad [-2.5]\n",
            "Epoch: 6 loss: 2.8988 W1_grad [-1.08 -2.12  0.87 -2.16 -4.23  1.73] W2_grad [-3.1  -4.13 -2.04] b1_grad [-1.08 -2.12  0.87] b2_grad [-2.41]\n",
            "Epoch: 7 loss: 2.6683 W1_grad [-1.11 -2.12  0.78 -2.21 -4.25  1.57] W2_grad [-3.12 -4.25 -1.84] b1_grad [-1.11 -2.12  0.78] b2_grad [-2.31]\n",
            "Epoch: 8 loss: 2.4374 W1_grad [-1.13 -2.12  0.71 -2.25 -4.25  1.42] W2_grad [-3.13 -4.35 -1.65] b1_grad [-1.13 -2.12  0.71] b2_grad [-2.21]\n",
            "Epoch: 9 loss: 2.2077 W1_grad [-1.14 -2.11  0.64 -2.27 -4.23  1.28] W2_grad [-3.12 -4.4  -1.48] b1_grad [-1.14 -2.11  0.64] b2_grad [-2.1]\n",
            "Epoch: 10 loss: 1.981 W1_grad [-1.14 -2.09  0.58 -2.28 -4.18  1.15] W2_grad [-3.09 -4.42 -1.33] b1_grad [-1.14 -2.09  0.58] b2_grad [-1.99]\n",
            "Epoch: 11 loss: 1.7599 W1_grad [-1.13 -2.05  0.52 -2.26 -4.1   1.04] W2_grad [-3.04 -4.4  -1.19] b1_grad [-1.13 -2.05  0.52] b2_grad [-1.88]\n",
            "Epoch: 12 loss: 1.5469 W1_grad [-1.11 -2.    0.47 -2.23 -4.    0.93] W2_grad [-2.97 -4.35 -1.06] b1_grad [-1.11 -2.    0.47] b2_grad [-1.76]\n",
            "Epoch: 13 loss: 1.3445 W1_grad [-1.09 -1.94  0.42 -2.18 -3.87  0.83] W2_grad [-2.88 -4.25 -0.94] b1_grad [-1.09 -1.94  0.42] b2_grad [-1.64]\n",
            "Epoch: 14 loss: 1.1552 W1_grad [-1.05 -1.86  0.37 -2.1  -3.72  0.74] W2_grad [-2.77 -4.11 -0.83] b1_grad [-1.05 -1.86  0.37] b2_grad [-1.52]\n",
            "Epoch: 15 loss: 0.9807 W1_grad [-1.01 -1.77  0.33 -2.02 -3.54  0.66] W2_grad [-2.64 -3.95 -0.74] b1_grad [-1.01 -1.77  0.33] b2_grad [-1.4]\n",
            "Epoch: 16 loss: 0.8228 W1_grad [-0.96 -1.67  0.29 -1.91 -3.35  0.59] W2_grad [-2.49 -3.75 -0.65] b1_grad [-0.96 -1.67  0.29] b2_grad [-1.28]\n",
            "Epoch: 17 loss: 0.682 W1_grad [-0.9  -1.57  0.26 -1.8  -3.14  0.52] W2_grad [-2.34 -3.53 -0.57] b1_grad [-0.9  -1.57  0.26] b2_grad [-1.17]\n",
            "Epoch: 18 loss: 0.5588 W1_grad [-0.84 -1.46  0.23 -1.68 -2.91  0.46] W2_grad [-2.17 -3.3  -0.5 ] b1_grad [-0.84 -1.46  0.23] b2_grad [-1.06]\n",
            "Epoch: 19 loss: 0.4526 W1_grad [-0.78 -1.34  0.2  -1.55 -2.68  0.4 ] W2_grad [-2.   -3.05 -0.44] b1_grad [-0.78 -1.34  0.2 ] b2_grad [-0.95]\n",
            "Epoch: 20 loss: 0.3626 W1_grad [-0.71 -1.23  0.18 -1.42 -2.45  0.35] W2_grad [-1.83 -2.8  -0.38] b1_grad [-0.71 -1.23  0.18] b2_grad [-0.85]\n",
            "Epoch: 21 loss: 0.2874 W1_grad [-0.65 -1.11  0.15 -1.3  -2.23  0.31] W2_grad [-1.66 -2.55 -0.33] b1_grad [-0.65 -1.11  0.15] b2_grad [-0.76]\n",
            "Epoch: 22 loss: 0.2256 W1_grad [-0.59 -1.    0.13 -1.17 -2.01  0.27] W2_grad [-1.5  -2.3  -0.29] b1_grad [-0.59 -1.    0.13] b2_grad [-0.67]\n",
            "Epoch: 23 loss: 0.1755 W1_grad [-0.53 -0.9   0.12 -1.05 -1.8   0.23] W2_grad [-1.34 -2.07 -0.25] b1_grad [-0.53 -0.9   0.12] b2_grad [-0.59]\n",
            "Epoch: 24 loss: 0.1354 W1_grad [-0.47 -0.8   0.1  -0.94 -1.6   0.2 ] W2_grad [-1.2  -1.84 -0.21] b1_grad [-0.47 -0.8   0.1 ] b2_grad [-0.52]\n",
            "Epoch: 25 loss: 0.1036 W1_grad [-0.42 -0.71  0.09 -0.83 -1.42  0.18] W2_grad [-1.06 -1.63 -0.19] b1_grad [-0.42 -0.71  0.09] b2_grad [-0.46]\n",
            "Epoch: 26 loss: 0.0788 W1_grad [-0.37 -0.62  0.08 -0.73 -1.25  0.15] W2_grad [-0.93 -1.44 -0.16] b1_grad [-0.37 -0.62  0.08] b2_grad [-0.4]\n",
            "Epoch: 27 loss: 0.0595 W1_grad [-0.32 -0.55  0.07 -0.64 -1.1   0.13] W2_grad [-0.82 -1.27 -0.14] b1_grad [-0.32 -0.55  0.07] b2_grad [-0.35]\n",
            "Epoch: 28 loss: 0.0447 W1_grad [-0.28 -0.48  0.06 -0.56 -0.96  0.11] W2_grad [-0.72 -1.11 -0.12] b1_grad [-0.28 -0.48  0.06] b2_grad [-0.3]\n",
            "Epoch: 29 loss: 0.0334 W1_grad [-0.24 -0.42  0.05 -0.49 -0.83  0.1 ] W2_grad [-0.62 -0.96 -0.1 ] b1_grad [-0.24 -0.42  0.05] b2_grad [-0.26]\n",
            "Epoch: 30 loss: 0.0249 W1_grad [-0.21 -0.36  0.04 -0.43 -0.72  0.08] W2_grad [-0.54 -0.84 -0.09] b1_grad [-0.21 -0.36  0.04] b2_grad [-0.22]\n",
            "Epoch: 31 loss: 0.0184 W1_grad [-0.18 -0.31  0.04 -0.37 -0.63  0.07] W2_grad [-0.47 -0.73 -0.07] b1_grad [-0.18 -0.31  0.04] b2_grad [-0.19]\n",
            "Epoch: 32 loss: 0.0136 W1_grad [-0.16 -0.27  0.03 -0.32 -0.54  0.06] W2_grad [-0.4  -0.63 -0.06] b1_grad [-0.16 -0.27  0.03] b2_grad [-0.17]\n",
            "Epoch: 33 loss: 0.01 W1_grad [-0.14 -0.23  0.03 -0.27 -0.47  0.05] W2_grad [-0.35 -0.54 -0.05] b1_grad [-0.14 -0.23  0.03] b2_grad [-0.14]\n",
            "Epoch: 34 loss: 0.0074 W1_grad [-0.12 -0.2   0.02 -0.24 -0.4   0.04] W2_grad [-0.3  -0.46 -0.05] b1_grad [-0.12 -0.2   0.02] b2_grad [-0.12]\n",
            "Epoch: 35 loss: 0.0054 W1_grad [-0.1  -0.17  0.02 -0.2  -0.34  0.04] W2_grad [-0.26 -0.4  -0.04] b1_grad [-0.1  -0.17  0.02] b2_grad [-0.1]\n",
            "Epoch: 36 loss: 0.004 W1_grad [-0.09 -0.15  0.02 -0.17 -0.29  0.03] W2_grad [-0.22 -0.34 -0.03] b1_grad [-0.09 -0.15  0.02] b2_grad [-0.09]\n",
            "Epoch: 37 loss: 0.0029 W1_grad [-0.07 -0.13  0.01 -0.15 -0.25  0.03] W2_grad [-0.19 -0.29 -0.03] b1_grad [-0.07 -0.13  0.01] b2_grad [-0.08]\n",
            "Epoch: 38 loss: 0.0021 W1_grad [-0.06 -0.11  0.01 -0.13 -0.22  0.02] W2_grad [-0.16 -0.25 -0.02] b1_grad [-0.06 -0.11  0.01] b2_grad [-0.06]\n",
            "Epoch: 39 loss: 0.0015 W1_grad [-0.05 -0.09  0.01 -0.11 -0.18  0.02] W2_grad [-0.14 -0.21 -0.02] b1_grad [-0.05 -0.09  0.01] b2_grad [-0.06]\n",
            "Epoch: 40 loss: 0.0011 W1_grad [-0.05 -0.08  0.01 -0.09 -0.16  0.02] W2_grad [-0.12 -0.18 -0.02] b1_grad [-0.05 -0.08  0.01] b2_grad [-0.05]\n",
            "Epoch: 41 loss: 0.0008 W1_grad [-0.04 -0.07  0.01 -0.08 -0.13  0.01] W2_grad [-0.1  -0.16 -0.02] b1_grad [-0.04 -0.07  0.01] b2_grad [-0.04]\n",
            "Epoch: 42 loss: 0.0006 W1_grad [-0.03 -0.06  0.01 -0.07 -0.12  0.01] W2_grad [-0.09 -0.13 -0.01] b1_grad [-0.03 -0.06  0.01] b2_grad [-0.03]\n",
            "Epoch: 43 loss: 0.0004 W1_grad [-0.03 -0.05  0.01 -0.06 -0.1   0.01] W2_grad [-0.07 -0.11 -0.01] b1_grad [-0.03 -0.05  0.01] b2_grad [-0.03]\n",
            "Epoch: 44 loss: 0.0003 W1_grad [-0.02 -0.04  0.   -0.05 -0.08  0.01] W2_grad [-0.06 -0.1  -0.01] b1_grad [-0.02 -0.04  0.  ] b2_grad [-0.02]\n",
            "Epoch: 45 loss: 0.0002 W1_grad [-0.02 -0.04  0.   -0.04 -0.07  0.01] W2_grad [-0.05 -0.08 -0.01] b1_grad [-0.02 -0.04  0.  ] b2_grad [-0.02]\n",
            "Epoch: 46 loss: 0.0002 W1_grad [-0.02 -0.03  0.   -0.04 -0.06  0.01] W2_grad [-0.05 -0.07 -0.01] b1_grad [-0.02 -0.03  0.  ] b2_grad [-0.02]\n",
            "Epoch: 47 loss: 0.0001 W1_grad [-0.02 -0.03  0.   -0.03 -0.05  0.01] W2_grad [-0.04 -0.06 -0.01] b1_grad [-0.02 -0.03  0.  ] b2_grad [-0.02]\n",
            "Epoch: 48 loss: 0.0001 W1_grad [-0.01 -0.02  0.   -0.03 -0.04  0.  ] W2_grad [-0.03 -0.05 -0.  ] b1_grad [-0.01 -0.02  0.  ] b2_grad [-0.01]\n",
            "Epoch: 49 loss: 0.0001 W1_grad [-0.01 -0.02  0.   -0.02 -0.04  0.  ] W2_grad [-0.03 -0.04 -0.  ] b1_grad [-0.01 -0.02  0.  ] b2_grad [-0.01]\n",
            "Epoch: 50 loss: 0.0 W1_grad [-0.01 -0.02  0.   -0.02 -0.03  0.  ] W2_grad [-0.02 -0.04 -0.  ] b1_grad [-0.01 -0.02  0.  ] b2_grad [-0.01]\n"
          ]
        }
      ]
    }
  ]
}