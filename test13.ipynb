{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqLzNsx0vCHwPCpN1tykJ5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ISr8P0x6cC3U"
      },
      "outputs": [],
      "source": [
        "#假设有一个简单的神经网络，\n",
        "#包含一个输入层（2个神经元）\n",
        "#一个隐藏层（3个神经元，使用 ReLU 激活函数）\n",
        "#一个输出层（1个神经元，使用线性激活函数）。\n",
        "\n",
        "#我们用均方误差（Mean Squared Error, MSE）作为损失函数。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "ClD9dBjTcja-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#求relu激活函数\n",
        "def relu(a):\n",
        "  return np.maximum(0, a)"
      ],
      "metadata": {
        "id": "-NszRB2Dd-eN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#前向传播，求输出y_pred\n",
        "\n",
        "def forward(W1, W2, b1, b2, X):\n",
        "  Z = np.outer(X, W2) + b1 #shape (2,3)\n",
        "  H = np.mean(relu(Z), axis=0, keepdims=True).flatten() #shape (3,)\n",
        "\n",
        "  y_temp = np.mean((np.outer(W2, H) + b2), axis=0, keepdims=True).flatten()\n",
        "\n",
        "  y_pred = np.mean(y_temp)\n",
        "  return Z, H, y_pred"
      ],
      "metadata": {
        "id": "P9e_h3a2eVhW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#求损失函数, loss值\n",
        "def compute_loss(y_pred, y_true):\n",
        "  loss = (y_pred - y_true)**2 / 2\n",
        "  return loss"
      ],
      "metadata": {
        "id": "ocj-M_Azj9dP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#反向传播，求梯度函数\n",
        "def backward(y_pred, y_true, H, Z, W2, X):\n",
        "  b2_grad = y_pred - y_true\n",
        "  W2_grad = b2_grad * H\n",
        "\n",
        "  Z = np.mean(Z, axis=0, keepdims=True).flatten()\n",
        "  b1_grad = b2_grad * W2 * (Z>0)\n",
        "  W1_grad = np.outer(X, b1_grad)\n",
        "\n",
        "  return W1_grad, W2_grad, b1_grad, b2_grad"
      ],
      "metadata": {
        "id": "u1ao42KDk84P"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#更新参数，weight,bias\n",
        "def update_params(W1, W2, b1, b2, W1_grad, W2_grad, b1_grad, b2_grad, lr):\n",
        "  W1 -= lr * W1_grad\n",
        "  W2 -= lr * W2_grad\n",
        "  b1 -= lr * b1_grad\n",
        "  b2 -= lr * b2_grad\n",
        "  return W1, W2, b1, b2"
      ],
      "metadata": {
        "id": "57cwWCulpADo"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([1.0, 2.0]) #ndarray(2,)（一个样本，2个特征）\n",
        "\n",
        "y_true = 3.0\n",
        "\n",
        "W1 = np.array([[0.5, 0.1, -0.3],\n",
        "        [0.2, 0.4, 0.6]]) #ndarray(2,3)\n",
        "\n",
        "b1 = np.array([0.1, 0.2, 0.3]) #ndarray(3,)\n",
        "\n",
        "W2 = np.array([0.3, 0.7, -0.5]) #ndarray(3,) （形状为 3×1）\n",
        "\n",
        "lr = 0.01\n",
        "\n",
        "b2 = 0.1\n",
        "\n",
        "epochs = 50\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  Z, H, y_pred = forward(W1, W2, b1, b2, X)\n",
        "  loss = compute_loss(y_pred, y_true)\n",
        "  W1_grad, W2_grad, b1_grad, b2_grad = backward(y_pred, y_true, H, Z, W2, X)\n",
        "  W1, W2, b1, b2 = update_params(W1, W2, b1, b2, W1_grad, W2_grad, b1_grad, b2_grad, lr)\n",
        "\n",
        "  print(\"Epoch:\", epoch + 1,\n",
        "    \"loss:\", np.round(loss, 4),\n",
        "    \"W1_grad\", np.round(W1_grad.flatten(), 2),\n",
        "    \"W2_grad\", np.round(W2_grad.flatten(), 2),\n",
        "    \"b1_grad\", np.round(b1_grad.flatten(), 2),\n",
        "    \"b2_grad\", np.round(b2_grad.flatten(), 2)\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-s5JDu_VcnlW",
        "outputId": "34d45b37-217a-4f49-86b1-30d3bf8ce0b7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 loss: 3.92 W1_grad [-0.84 -1.96  0.   -1.68 -3.92  0.  ] W2_grad [-1.54 -3.5  -0.  ] b1_grad [-0.84 -1.96  0.  ] b2_grad [-2.8]\n",
            "Epoch: 2 loss: 3.7966 W1_grad [-0.87 -2.03  0.   -1.74 -4.05  0.  ] W2_grad [-1.6  -3.64 -0.  ] b1_grad [-0.87 -2.03  0.  ] b2_grad [-2.76]\n",
            "Epoch: 3 loss: 3.6713 W1_grad [-0.9  -2.09  0.   -1.8  -4.18  0.  ] W2_grad [-1.66 -3.79 -0.  ] b1_grad [-0.9  -2.09  0.  ] b2_grad [-2.71]\n",
            "Epoch: 4 loss: 3.5439 W1_grad [-0.93 -2.15  0.   -1.85 -4.31  0.  ] W2_grad [-1.73 -3.93 -0.  ] b1_grad [-0.93 -2.15  0.  ] b2_grad [-2.66]\n",
            "Epoch: 5 loss: 3.4143 W1_grad [-0.95 -2.22  0.   -1.91 -4.43  0.  ] W2_grad [-1.79 -4.06 -0.  ] b1_grad [-0.95 -2.22  0.  ] b2_grad [-2.61]\n",
            "Epoch: 6 loss: 3.2825 W1_grad [-0.98 -2.28  0.   -1.96 -4.56  0.  ] W2_grad [-1.84 -4.2  -0.  ] b1_grad [-0.98 -2.28  0.  ] b2_grad [-2.56]\n",
            "Epoch: 7 loss: 3.1485 W1_grad [-1.01 -2.34  0.   -2.02 -4.67  0.  ] W2_grad [-1.9  -4.33 -0.  ] b1_grad [-1.01 -2.34  0.  ] b2_grad [-2.51]\n",
            "Epoch: 8 loss: 3.0123 W1_grad [-1.03 -2.39  0.   -2.06 -4.78  0.  ] W2_grad [-1.95 -4.45 -0.  ] b1_grad [-1.03 -2.39  0.  ] b2_grad [-2.45]\n",
            "Epoch: 9 loss: 2.8741 W1_grad [-1.06 -2.44  0.   -2.11 -4.89  0.  ] W2_grad [-2.   -4.56 -0.  ] b1_grad [-1.06 -2.44  0.  ] b2_grad [-2.4]\n",
            "Epoch: 10 loss: 2.734 W1_grad [-1.08 -2.49  0.   -2.15 -4.98  0.  ] W2_grad [-2.05 -4.67 -0.  ] b1_grad [-1.08 -2.49  0.  ] b2_grad [-2.34]\n",
            "Epoch: 11 loss: 2.5925 W1_grad [-1.09 -2.53  0.   -2.19 -5.06  0.  ] W2_grad [-2.09 -4.76 -0.  ] b1_grad [-1.09 -2.53  0.  ] b2_grad [-2.28]\n",
            "Epoch: 12 loss: 2.4497 W1_grad [-1.11 -2.56  0.   -2.22 -5.13  0.  ] W2_grad [-2.12 -4.84 -0.  ] b1_grad [-1.11 -2.56  0.  ] b2_grad [-2.21]\n",
            "Epoch: 13 loss: 2.3062 W1_grad [-1.12 -2.59  0.   -2.25 -5.19  0.  ] W2_grad [-2.15 -4.91 -0.  ] b1_grad [-1.12 -2.59  0.  ] b2_grad [-2.15]\n",
            "Epoch: 14 loss: 2.1626 W1_grad [-1.13 -2.61  0.   -2.26 -5.23  0.  ] W2_grad [-2.18 -4.96 -0.  ] b1_grad [-1.13 -2.61  0.  ] b2_grad [-2.08]\n",
            "Epoch: 15 loss: 2.0194 W1_grad [-1.14 -2.62  0.   -2.28 -5.25  0.  ] W2_grad [-2.19 -5.   -0.  ] b1_grad [-1.14 -2.62  0.  ] b2_grad [-2.01]\n",
            "Epoch: 16 loss: 1.8773 W1_grad [-1.14 -2.63  0.   -2.28 -5.25  0.  ] W2_grad [-2.2  -5.01 -0.  ] b1_grad [-1.14 -2.63  0.  ] b2_grad [-1.94]\n",
            "Epoch: 17 loss: 1.7371 W1_grad [-1.14 -2.62  0.   -2.27 -5.24  0.  ] W2_grad [-2.2  -5.01 -0.  ] b1_grad [-1.14 -2.62  0.  ] b2_grad [-1.86]\n",
            "Epoch: 18 loss: 1.5996 W1_grad [-1.13 -2.6   0.   -2.26 -5.21  0.  ] W2_grad [-2.19 -4.99 -0.  ] b1_grad [-1.13 -2.6   0.  ] b2_grad [-1.79]\n",
            "Epoch: 19 loss: 1.4655 W1_grad [-1.12 -2.58  0.   -2.24 -5.16  0.  ] W2_grad [-2.17 -4.95 -0.  ] b1_grad [-1.12 -2.58  0.  ] b2_grad [-1.71]\n",
            "Epoch: 20 loss: 1.3355 W1_grad [-1.1  -2.54  0.   -2.21 -5.08  0.  ] W2_grad [-2.14 -4.89 -0.  ] b1_grad [-1.1  -2.54  0.  ] b2_grad [-1.63]\n",
            "Epoch: 21 loss: 1.2106 W1_grad [-1.08 -2.5   0.   -2.17 -4.99  0.  ] W2_grad [-2.11 -4.81 -0.  ] b1_grad [-1.08 -2.5   0.  ] b2_grad [-1.56]\n",
            "Epoch: 22 loss: 1.0912 W1_grad [-1.06 -2.44  0.   -2.12 -4.88  0.  ] W2_grad [-2.06 -4.71 -0.  ] b1_grad [-1.06 -2.44  0.  ] b2_grad [-1.48]\n",
            "Epoch: 23 loss: 0.9781 W1_grad [-1.03 -2.38  0.   -2.07 -4.75  0.  ] W2_grad [-2.01 -4.59 -0.  ] b1_grad [-1.03 -2.38  0.  ] b2_grad [-1.4]\n",
            "Epoch: 24 loss: 0.8717 W1_grad [-1.   -2.3   0.   -2.   -4.61  0.  ] W2_grad [-1.95 -4.46 -0.  ] b1_grad [-1.  -2.3  0. ] b2_grad [-1.32]\n",
            "Epoch: 25 loss: 0.7725 W1_grad [-0.97 -2.22  0.   -1.93 -4.45  0.  ] W2_grad [-1.89 -4.31 -0.  ] b1_grad [-0.97 -2.22  0.  ] b2_grad [-1.24]\n",
            "Epoch: 26 loss: 0.6806 W1_grad [-0.93 -2.14  0.   -1.86 -4.28  0.  ] W2_grad [-1.82 -4.14 -0.  ] b1_grad [-0.93 -2.14  0.  ] b2_grad [-1.17]\n",
            "Epoch: 27 loss: 0.5963 W1_grad [-0.89 -2.05  0.   -1.78 -4.09  0.  ] W2_grad [-1.74 -3.97 -0.  ] b1_grad [-0.89 -2.05  0.  ] b2_grad [-1.09]\n",
            "Epoch: 28 loss: 0.5194 W1_grad [-0.85 -1.95  0.   -1.7  -3.9   0.  ] W2_grad [-1.66 -3.79 -0.  ] b1_grad [-0.85 -1.95  0.  ] b2_grad [-1.02]\n",
            "Epoch: 29 loss: 0.45 W1_grad [-0.81 -1.85  0.   -1.61 -3.7   0.  ] W2_grad [-1.58 -3.6  -0.  ] b1_grad [-0.81 -1.85  0.  ] b2_grad [-0.95]\n",
            "Epoch: 30 loss: 0.3878 W1_grad [-0.76 -1.75  0.   -1.52 -3.5   0.  ] W2_grad [-1.49 -3.4  -0.  ] b1_grad [-0.76 -1.75  0.  ] b2_grad [-0.88]\n",
            "Epoch: 31 loss: 0.3324 W1_grad [-0.72 -1.65  0.   -1.43 -3.3   0.  ] W2_grad [-1.4  -3.21 -0.  ] b1_grad [-0.72 -1.65  0.  ] b2_grad [-0.82]\n",
            "Epoch: 32 loss: 0.2835 W1_grad [-0.67 -1.55  0.   -1.35 -3.09  0.  ] W2_grad [-1.32 -3.01 -0.  ] b1_grad [-0.67 -1.55  0.  ] b2_grad [-0.75]\n",
            "Epoch: 33 loss: 0.2406 W1_grad [-0.63 -1.45  0.   -1.26 -2.89  0.  ] W2_grad [-1.23 -2.81 -0.  ] b1_grad [-0.63 -1.45  0.  ] b2_grad [-0.69]\n",
            "Epoch: 34 loss: 0.2033 W1_grad [-0.59 -1.35  0.   -1.17 -2.69  0.  ] W2_grad [-1.15 -2.62 -0.  ] b1_grad [-0.59 -1.35  0.  ] b2_grad [-0.64]\n",
            "Epoch: 35 loss: 0.171 W1_grad [-0.54 -1.25  0.   -1.09 -2.5   0.  ] W2_grad [-1.07 -2.44 -0.  ] b1_grad [-0.54 -1.25  0.  ] b2_grad [-0.58]\n",
            "Epoch: 36 loss: 0.1432 W1_grad [-0.5  -1.16  0.   -1.01 -2.32  0.  ] W2_grad [-0.99 -2.26 -0.  ] b1_grad [-0.5  -1.16  0.  ] b2_grad [-0.54]\n",
            "Epoch: 37 loss: 0.1195 W1_grad [-0.47 -1.07  0.   -0.93 -2.14  0.  ] W2_grad [-0.91 -2.08 -0.  ] b1_grad [-0.47 -1.07  0.  ] b2_grad [-0.49]\n",
            "Epoch: 38 loss: 0.0993 W1_grad [-0.43 -0.98  0.   -0.86 -1.97  0.  ] W2_grad [-0.84 -1.92 -0.  ] b1_grad [-0.43 -0.98  0.  ] b2_grad [-0.45]\n",
            "Epoch: 39 loss: 0.0823 W1_grad [-0.39 -0.9   0.   -0.79 -1.81  0.  ] W2_grad [-0.77 -1.76 -0.  ] b1_grad [-0.39 -0.9   0.  ] b2_grad [-0.41]\n",
            "Epoch: 40 loss: 0.068 W1_grad [-0.36 -0.83  0.   -0.72 -1.65  0.  ] W2_grad [-0.71 -1.61 -0.  ] b1_grad [-0.36 -0.83  0.  ] b2_grad [-0.37]\n",
            "Epoch: 41 loss: 0.056 W1_grad [-0.33 -0.76  0.   -0.66 -1.51  0.  ] W2_grad [-0.65 -1.48 -0.  ] b1_grad [-0.33 -0.76  0.  ] b2_grad [-0.33]\n",
            "Epoch: 42 loss: 0.046 W1_grad [-0.3  -0.69  0.   -0.6  -1.38  0.  ] W2_grad [-0.59 -1.35 -0.  ] b1_grad [-0.3  -0.69  0.  ] b2_grad [-0.3]\n",
            "Epoch: 43 loss: 0.0376 W1_grad [-0.27 -0.63  0.   -0.55 -1.26  0.  ] W2_grad [-0.54 -1.23 -0.  ] b1_grad [-0.27 -0.63  0.  ] b2_grad [-0.27]\n",
            "Epoch: 44 loss: 0.0308 W1_grad [-0.25 -0.57  0.   -0.5  -1.14  0.  ] W2_grad [-0.49 -1.11 -0.  ] b1_grad [-0.25 -0.57  0.  ] b2_grad [-0.25]\n",
            "Epoch: 45 loss: 0.0251 W1_grad [-0.23 -0.52  0.   -0.45 -1.04  0.  ] W2_grad [-0.44 -1.01 -0.  ] b1_grad [-0.23 -0.52  0.  ] b2_grad [-0.22]\n",
            "Epoch: 46 loss: 0.0204 W1_grad [-0.2  -0.47  0.   -0.41 -0.94  0.  ] W2_grad [-0.4  -0.92 -0.  ] b1_grad [-0.2  -0.47  0.  ] b2_grad [-0.2]\n",
            "Epoch: 47 loss: 0.0166 W1_grad [-0.18 -0.42  0.   -0.37 -0.85  0.  ] W2_grad [-0.36 -0.83 -0.  ] b1_grad [-0.18 -0.42  0.  ] b2_grad [-0.18]\n",
            "Epoch: 48 loss: 0.0134 W1_grad [-0.17 -0.38  0.   -0.33 -0.77  0.  ] W2_grad [-0.33 -0.75 -0.  ] b1_grad [-0.17 -0.38  0.  ] b2_grad [-0.16]\n",
            "Epoch: 49 loss: 0.0109 W1_grad [-0.15 -0.35  0.   -0.3  -0.69  0.  ] W2_grad [-0.3  -0.68 -0.  ] b1_grad [-0.15 -0.35  0.  ] b2_grad [-0.15]\n",
            "Epoch: 50 loss: 0.0088 W1_grad [-0.14 -0.31  0.   -0.27 -0.62  0.  ] W2_grad [-0.27 -0.61 -0.  ] b1_grad [-0.14 -0.31  0.  ] b2_grad [-0.13]\n"
          ]
        }
      ]
    }
  ]
}