{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrSKoleAuAbcaJbLi5wYIv"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "hohjWweZp1_a"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#计算relu函数\n",
        "def relu(a):\n",
        "  a = np.maximum(0, a)\n",
        "  return a"
      ],
      "metadata": {
        "id": "B4K0XHh2p8D1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#计算前向传播输出y_pred\n",
        "def forward(X, W1, W2, B1, B2):\n",
        "  Z = np.dot(W1, X) + B1\n",
        "  H = relu(Z)\n",
        "\n",
        "  y_pred = np.dot(H, W2.flatten()) + B2\n",
        "  return H, Z, y_pred"
      ],
      "metadata": {
        "id": "sMLkLc12qWPj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#计算损失函数loss\n",
        "def compute_loss(y_pred, y_true):\n",
        "  loss = (y_pred - y_true) ** 2 / 2\n",
        "  return loss"
      ],
      "metadata": {
        "id": "BPvMKQOyrqKb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#反向传导计算梯度值\n",
        "def backward(y_pred, y_true, X, H, Z):\n",
        "  B2_grad = y_pred - y_true\n",
        "  W2_grad = np.outer(B2_grad, H)\n",
        "\n",
        "  B1_grad = B2_grad * W2.flatten() * (Z>0)\n",
        "  W1_grad = np.outer(B1_grad, X)\n",
        "\n",
        "  return W1_grad, W2_grad, B1_grad, B2_grad"
      ],
      "metadata": {
        "id": "TEzWCEUXs7VB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#更新参数weight, bias\n",
        "def update_param(learning_rate, W1, W2, B1, B2, W1_grad, W2_grad, B1_grad, B2_grad):\n",
        "  W1 -= learning_rate * W1_grad\n",
        "  W2 -= learning_rate * W2_grad\n",
        "\n",
        "  B1 -= learning_rate * B1_grad\n",
        "  B2 -= learning_rate * B2_grad\n",
        "\n",
        "  return W1, W2, B1, B2"
      ],
      "metadata": {
        "id": "D8v4rhzww5Qa"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVHk22D_p0Lu",
        "outputId": "c807a4f5-19c7-4be1-9a73-a4d8197e1bb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss 1.7766125 W1_grad [ 0.57 -1.13 -0.85 -1.32  2.64  1.98] W2_grad [-0.19 -2.73] B1_grad [-0.57  1.32] B2_grad [-1.89]\n",
            "Epoch 2 Loss 1.51209452 W1_grad [ 0.52 -1.05 -0.79 -1.17  2.34  1.75] W2_grad [-0.26 -2.33] B1_grad [-0.52  1.17] B2_grad [-1.74]\n",
            "Epoch 3 Loss 1.302699 W1_grad [ 0.49 -0.98 -0.74 -1.05  2.1   1.57] W2_grad [-0.31 -2.01] B1_grad [-0.49  1.05] B2_grad [-1.61]\n",
            "Epoch 4 Loss 1.13329253 W1_grad [ 0.46 -0.93 -0.69 -0.95  1.89  1.42] W2_grad [-0.35 -1.74] B1_grad [-0.46  0.95] B2_grad [-1.51]\n",
            "Epoch 5 Loss 0.99373155 W1_grad [ 0.44 -0.88 -0.66 -0.86  1.73  1.29] W2_grad [-0.38 -1.52] B1_grad [-0.44  0.86] B2_grad [-1.41]\n",
            "Epoch 6 Loss 0.87698841 W1_grad [ 0.42 -0.83 -0.63 -0.79  1.58  1.19] W2_grad [-0.4  -1.34] B1_grad [-0.42  0.79] B2_grad [-1.32]\n",
            "Epoch 7 Loss 0.77805649 W1_grad [ 0.4  -0.8  -0.6  -0.73  1.46  1.09] W2_grad [-0.42 -1.18] B1_grad [-0.4   0.73] B2_grad [-1.25]\n",
            "Epoch 8 Loss 0.69328268 W1_grad [ 0.38 -0.76 -0.57 -0.67  1.35  1.01] W2_grad [-0.44 -1.04] B1_grad [-0.38  0.67] B2_grad [-1.18]\n",
            "Epoch 9 Loss 0.61994553 W1_grad [ 0.36 -0.73 -0.55 -0.62  1.25  0.94] W2_grad [-0.45 -0.92] B1_grad [-0.36  0.62] B2_grad [-1.11]\n",
            "Epoch 10 Loss 0.5559803 W1_grad [ 0.35 -0.7  -0.52 -0.58  1.16  0.87] W2_grad [-0.46 -0.82] B1_grad [-0.35  0.58] B2_grad [-1.05]\n",
            "Epoch 11 Loss 0.49979478 W1_grad [ 0.34 -0.67 -0.5  -0.54  1.09  0.82] W2_grad [-0.46 -0.73] B1_grad [-0.34  0.54] B2_grad [-1.]\n",
            "Epoch 12 Loss 0.45014308 W1_grad [ 0.32 -0.65 -0.49 -0.51  1.02  0.76] W2_grad [-0.47 -0.65] B1_grad [-0.32  0.51] B2_grad [-0.95]\n",
            "Epoch 13 Loss 0.40603713 W1_grad [ 0.31 -0.62 -0.47 -0.48  0.95  0.72] W2_grad [-0.47 -0.58] B1_grad [-0.31  0.48] B2_grad [-0.9]\n",
            "Epoch 14 Loss 0.36668368 W1_grad [ 0.3  -0.6  -0.45 -0.45  0.9   0.67] W2_grad [-0.46 -0.52] B1_grad [-0.3   0.45] B2_grad [-0.86]\n",
            "Epoch 15 Loss 0.33143849 W1_grad [ 0.29 -0.58 -0.43 -0.42  0.84  0.63] W2_grad [-0.46 -0.46] B1_grad [-0.29  0.42] B2_grad [-0.81]\n",
            "Epoch 16 Loss 0.29977271 W1_grad [ 0.28 -0.56 -0.42 -0.4   0.8   0.6 ] W2_grad [-0.46 -0.41] B1_grad [-0.28  0.4 ] B2_grad [-0.77]\n",
            "Epoch 17 Loss 0.2712477 W1_grad [ 0.27 -0.54 -0.4  -0.38  0.75  0.56] W2_grad [-0.45 -0.37] B1_grad [-0.27  0.38] B2_grad [-0.74]\n",
            "Epoch 18 Loss 0.24549611 W1_grad [ 0.26 -0.52 -0.39 -0.35  0.71  0.53] W2_grad [-0.45 -0.33] B1_grad [-0.26  0.35] B2_grad [-0.7]\n",
            "Epoch 19 Loss 0.22220742 W1_grad [ 0.25 -0.5  -0.37 -0.34  0.67  0.5 ] W2_grad [-0.44 -0.29] B1_grad [-0.25  0.34] B2_grad [-0.67]\n",
            "Epoch 20 Loss 0.20111674 W1_grad [ 0.24 -0.48 -0.36 -0.32  0.63  0.48] W2_grad [-0.43 -0.26] B1_grad [-0.24  0.32] B2_grad [-0.63]\n",
            "Epoch 21 Loss 0.18199623 W1_grad [ 0.23 -0.46 -0.35 -0.3   0.6   0.45] W2_grad [-0.42 -0.23] B1_grad [-0.23  0.3 ] B2_grad [-0.6]\n",
            "Epoch 22 Loss 0.16464829 W1_grad [ 0.22 -0.44 -0.33 -0.28  0.57  0.43] W2_grad [-0.41 -0.21] B1_grad [-0.22  0.28] B2_grad [-0.57]\n",
            "Epoch 23 Loss 0.14890025 W1_grad [ 0.21 -0.43 -0.32 -0.27  0.54  0.4 ] W2_grad [-0.4  -0.18] B1_grad [-0.21  0.27] B2_grad [-0.55]\n",
            "Epoch 24 Loss 0.13460011 W1_grad [ 0.2  -0.41 -0.31 -0.26  0.51  0.38] W2_grad [-0.39 -0.16] B1_grad [-0.2   0.26] B2_grad [-0.52]\n",
            "Epoch 25 Loss 0.12161323 W1_grad [ 0.2  -0.39 -0.29 -0.24  0.48  0.36] W2_grad [-0.38 -0.14] B1_grad [-0.2   0.24] B2_grad [-0.49]\n",
            "Epoch 26 Loss 0.10981959 W1_grad [ 0.19 -0.38 -0.28 -0.23  0.46  0.34] W2_grad [-0.37 -0.13] B1_grad [-0.19  0.23] B2_grad [-0.47]\n",
            "Epoch 27 Loss 0.09911161 W1_grad [ 0.18 -0.36 -0.27 -0.22  0.43  0.33] W2_grad [-0.36 -0.11] B1_grad [-0.18  0.22] B2_grad [-0.45]\n",
            "Epoch 28 Loss 0.08939245 W1_grad [ 0.17 -0.35 -0.26 -0.21  0.41  0.31] W2_grad [-0.35 -0.1 ] B1_grad [-0.17  0.21] B2_grad [-0.42]\n",
            "Epoch 29 Loss 0.08057452 W1_grad [ 0.17 -0.33 -0.25 -0.19  0.39  0.29] W2_grad [-0.33 -0.09] B1_grad [-0.17  0.19] B2_grad [-0.4]\n",
            "Epoch 30 Loss 0.07257834 W1_grad [ 0.16 -0.32 -0.24 -0.18  0.37  0.28] W2_grad [-0.32 -0.08] B1_grad [-0.16  0.18] B2_grad [-0.38]\n",
            "Epoch 31 Loss 0.06533158 W1_grad [ 0.15 -0.3  -0.23 -0.17  0.35  0.26] W2_grad [-0.31 -0.07] B1_grad [-0.15  0.17] B2_grad [-0.36]\n",
            "Epoch 32 Loss 0.05876824 W1_grad [ 0.14 -0.29 -0.22 -0.17  0.33  0.25] W2_grad [-0.3  -0.06] B1_grad [-0.14  0.17] B2_grad [-0.34]\n",
            "Epoch 33 Loss 0.05282799 W1_grad [ 0.14 -0.28 -0.21 -0.16  0.31  0.24] W2_grad [-0.29 -0.05] B1_grad [-0.14  0.16] B2_grad [-0.33]\n",
            "Epoch 34 Loss 0.04745563 W1_grad [ 0.13 -0.26 -0.2  -0.15  0.3   0.22] W2_grad [-0.28 -0.04] B1_grad [-0.13  0.15] B2_grad [-0.31]\n",
            "Epoch 35 Loss 0.04260056 W1_grad [ 0.13 -0.25 -0.19 -0.14  0.28  0.21] W2_grad [-0.26 -0.04] B1_grad [-0.13  0.14] B2_grad [-0.29]\n",
            "Epoch 36 Loss 0.03821641 W1_grad [ 0.12 -0.24 -0.18 -0.13  0.27  0.2 ] W2_grad [-0.25 -0.03] B1_grad [-0.12  0.13] B2_grad [-0.28]\n",
            "Epoch 37 Loss 0.03426066 W1_grad [ 0.11 -0.23 -0.17 -0.13  0.25  0.19] W2_grad [-0.24 -0.03] B1_grad [-0.11  0.13] B2_grad [-0.26]\n",
            "Epoch 38 Loss 0.03069429 W1_grad [ 0.11 -0.22 -0.16 -0.12  0.24  0.18] W2_grad [-0.23 -0.02] B1_grad [-0.11  0.12] B2_grad [-0.25]\n",
            "Epoch 39 Loss 0.02748158 W1_grad [ 0.1  -0.21 -0.16 -0.11  0.23  0.17] W2_grad [-0.22 -0.02] B1_grad [-0.1   0.11] B2_grad [-0.23]\n",
            "Epoch 40 Loss 0.02458978 W1_grad [ 0.1  -0.2  -0.15 -0.11  0.21  0.16] W2_grad [-0.21 -0.02] B1_grad [-0.1   0.11] B2_grad [-0.22]\n",
            "Epoch 41 Loss 0.02198889 W1_grad [ 0.09 -0.19 -0.14 -0.1   0.2   0.15] W2_grad [-0.2  -0.02] B1_grad [-0.09  0.1 ] B2_grad [-0.21]\n",
            "Epoch 42 Loss 0.01965148 W1_grad [ 0.09 -0.18 -0.13 -0.1   0.19  0.14] W2_grad [-0.19 -0.01] B1_grad [-0.09  0.1 ] B2_grad [-0.2]\n",
            "Epoch 43 Loss 0.01755248 W1_grad [ 0.08 -0.17 -0.13 -0.09  0.18  0.13] W2_grad [-0.18 -0.01] B1_grad [-0.08  0.09] B2_grad [-0.19]\n",
            "Epoch 44 Loss 0.01566899 W1_grad [ 0.08 -0.16 -0.12 -0.08  0.17  0.13] W2_grad [-0.17 -0.01] B1_grad [-0.08  0.08] B2_grad [-0.18]\n",
            "Epoch 45 Loss 0.01398013 W1_grad [ 0.08 -0.15 -0.11 -0.08  0.16  0.12] W2_grad [-0.17 -0.01] B1_grad [-0.08  0.08] B2_grad [-0.17]\n",
            "Epoch 46 Loss 0.01246687 W1_grad [ 0.07 -0.14 -0.11 -0.08  0.15  0.11] W2_grad [-0.16 -0.01] B1_grad [-0.07  0.08] B2_grad [-0.16]\n",
            "Epoch 47 Loss 0.0111119 W1_grad [ 0.07 -0.14 -0.1  -0.07  0.14  0.11] W2_grad [-0.15 -0.  ] B1_grad [-0.07  0.07] B2_grad [-0.15]\n",
            "Epoch 48 Loss 0.00989947 W1_grad [ 0.06 -0.13 -0.1  -0.07  0.13  0.1 ] W2_grad [-0.14 -0.  ] B1_grad [-0.06  0.07] B2_grad [-0.14]\n",
            "Epoch 49 Loss 0.0088153 W1_grad [ 0.06 -0.12 -0.09 -0.06  0.13  0.1 ] W2_grad [-0.13 -0.  ] B1_grad [-0.06  0.06] B2_grad [-0.13]\n",
            "Epoch 50 Loss 0.00784642 W1_grad [ 0.06 -0.12 -0.09 -0.06  0.12  0.09] W2_grad [-0.13 -0.  ] B1_grad [-0.06  0.06] B2_grad [-0.13]\n",
            "Epoch 51 Loss 0.0069811 W1_grad [ 0.05 -0.11 -0.08 -0.06  0.11  0.08] W2_grad [-0.12 -0.  ] B1_grad [-0.05  0.06] B2_grad [-0.12]\n",
            "Epoch 52 Loss 0.00620872 W1_grad [ 0.05 -0.1  -0.08 -0.05  0.11  0.08] W2_grad [-0.11 -0.  ] B1_grad [-0.05  0.05] B2_grad [-0.11]\n",
            "Epoch 53 Loss 0.00557725 W1_grad [ 0.05 -0.1  -0.07 -0.    0.    0.  ] W2_grad [-0.11 -0.  ] B1_grad [-0.05  0.  ] B2_grad [-0.11]\n",
            "Epoch 54 Loss 0.00515738 W1_grad [ 0.05 -0.09 -0.07 -0.    0.    0.  ] W2_grad [-0.1 -0. ] B1_grad [-0.05  0.  ] B2_grad [-0.1]\n",
            "Epoch 55 Loss 0.00476748 W1_grad [ 0.05 -0.09 -0.07 -0.    0.    0.  ] W2_grad [-0.1 -0. ] B1_grad [-0.05  0.  ] B2_grad [-0.1]\n",
            "Epoch 56 Loss 0.00440559 W1_grad [ 0.04 -0.09 -0.07 -0.    0.    0.  ] W2_grad [-0.1 -0. ] B1_grad [-0.04  0.  ] B2_grad [-0.09]\n",
            "Epoch 57 Loss 0.00406986 W1_grad [ 0.04 -0.08 -0.06 -0.    0.    0.  ] W2_grad [-0.09 -0.  ] B1_grad [-0.04  0.  ] B2_grad [-0.09]\n",
            "Epoch 58 Loss 0.00375855 W1_grad [ 0.04 -0.08 -0.06 -0.    0.    0.  ] W2_grad [-0.09 -0.  ] B1_grad [-0.04  0.  ] B2_grad [-0.09]\n",
            "Epoch 59 Loss 0.00347 W1_grad [ 0.04 -0.08 -0.06 -0.    0.    0.  ] W2_grad [-0.09 -0.  ] B1_grad [-0.04  0.  ] B2_grad [-0.08]\n",
            "Epoch 60 Loss 0.00320268 W1_grad [ 0.04 -0.08 -0.06 -0.    0.    0.  ] W2_grad [-0.08 -0.  ] B1_grad [-0.04  0.  ] B2_grad [-0.08]\n",
            "Epoch 61 Loss 0.00295512 W1_grad [ 0.04 -0.07 -0.05 -0.    0.    0.  ] W2_grad [-0.08 -0.  ] B1_grad [-0.04  0.  ] B2_grad [-0.08]\n",
            "Epoch 62 Loss 0.00272595 W1_grad [ 0.03 -0.07 -0.05 -0.    0.    0.  ] W2_grad [-0.08 -0.  ] B1_grad [-0.03  0.  ] B2_grad [-0.07]\n",
            "Epoch 63 Loss 0.00251391 W1_grad [ 0.03 -0.07 -0.05 -0.    0.    0.  ] W2_grad [-0.08 -0.  ] B1_grad [-0.03  0.  ] B2_grad [-0.07]\n",
            "Epoch 64 Loss 0.00231777 W1_grad [ 0.03 -0.06 -0.05 -0.    0.    0.  ] W2_grad [-0.07 -0.  ] B1_grad [-0.03  0.  ] B2_grad [-0.07]\n",
            "Epoch 65 Loss 0.00213641 W1_grad [ 0.03 -0.06 -0.05 -0.    0.    0.  ] W2_grad [-0.07 -0.  ] B1_grad [-0.03  0.  ] B2_grad [-0.07]\n",
            "Epoch 66 Loss 0.00196879 W1_grad [ 0.03 -0.06 -0.04 -0.    0.    0.  ] W2_grad [-0.07 -0.  ] B1_grad [-0.03  0.  ] B2_grad [-0.06]\n",
            "Epoch 67 Loss 0.0018139 W1_grad [ 0.03 -0.06 -0.04 -0.    0.    0.  ] W2_grad [-0.06 -0.  ] B1_grad [-0.03  0.  ] B2_grad [-0.06]\n",
            "Epoch 68 Loss 0.00167084 W1_grad [ 0.03 -0.06 -0.04 -0.    0.    0.  ] W2_grad [-0.06 -0.  ] B1_grad [-0.03  0.  ] B2_grad [-0.06]\n",
            "Epoch 69 Loss 0.00153873 W1_grad [ 0.03 -0.05 -0.04 -0.    0.    0.  ] W2_grad [-0.06 -0.  ] B1_grad [-0.03  0.  ] B2_grad [-0.06]\n",
            "Epoch 70 Loss 0.00141679 W1_grad [ 0.03 -0.05 -0.04 -0.    0.    0.  ] W2_grad [-0.06 -0.  ] B1_grad [-0.03  0.  ] B2_grad [-0.05]\n",
            "Epoch 71 Loss 0.00130425 W1_grad [ 0.02 -0.05 -0.04 -0.    0.    0.  ] W2_grad [-0.06 -0.  ] B1_grad [-0.02  0.  ] B2_grad [-0.05]\n",
            "Epoch 72 Loss 0.00120043 W1_grad [ 0.02 -0.05 -0.04 -0.    0.    0.  ] W2_grad [-0.05 -0.  ] B1_grad [-0.02  0.  ] B2_grad [-0.05]\n",
            "Epoch 73 Loss 0.00110467 W1_grad [ 0.02 -0.05 -0.03 -0.    0.    0.  ] W2_grad [-0.05 -0.  ] B1_grad [-0.02  0.  ] B2_grad [-0.05]\n",
            "Epoch 74 Loss 0.00101638 W1_grad [ 0.02 -0.04 -0.03 -0.    0.    0.  ] W2_grad [-0.05 -0.  ] B1_grad [-0.02  0.  ] B2_grad [-0.05]\n",
            "Epoch 75 Loss 0.00093499 W1_grad [ 0.02 -0.04 -0.03 -0.    0.    0.  ] W2_grad [-0.05 -0.  ] B1_grad [-0.02  0.  ] B2_grad [-0.04]\n",
            "Epoch 76 Loss 0.00085997 W1_grad [ 0.02 -0.04 -0.03 -0.    0.    0.  ] W2_grad [-0.05 -0.  ] B1_grad [-0.02  0.  ] B2_grad [-0.04]\n",
            "Epoch 77 Loss 0.00079085 W1_grad [ 0.02 -0.04 -0.03 -0.    0.    0.  ] W2_grad [-0.04 -0.  ] B1_grad [-0.02  0.  ] B2_grad [-0.04]\n",
            "Epoch 78 Loss 0.00072718 W1_grad [ 0.02 -0.04 -0.03 -0.    0.    0.  ] W2_grad [-0.04 -0.  ] B1_grad [-0.02  0.  ] B2_grad [-0.04]\n",
            "Epoch 79 Loss 0.00066854 W1_grad [ 0.02 -0.04 -0.03 -0.    0.    0.  ] W2_grad [-0.04 -0.  ] B1_grad [-0.02  0.  ] B2_grad [-0.04]\n",
            "Epoch 80 Loss 0.00061455 W1_grad [ 0.02 -0.03 -0.03 -0.    0.    0.  ] W2_grad [-0.04 -0.  ] B1_grad [-0.02  0.  ] B2_grad [-0.04]\n",
            "Epoch 81 Loss 0.00056484 W1_grad [ 0.02 -0.03 -0.02 -0.    0.    0.  ] W2_grad [-0.04 -0.  ] B1_grad [-0.02  0.  ] B2_grad [-0.03]\n",
            "Epoch 82 Loss 0.00051908 W1_grad [ 0.02 -0.03 -0.02 -0.    0.    0.  ] W2_grad [-0.04 -0.  ] B1_grad [-0.02  0.  ] B2_grad [-0.03]\n",
            "Epoch 83 Loss 0.00047698 W1_grad [ 0.01 -0.03 -0.02 -0.    0.    0.  ] W2_grad [-0.03 -0.  ] B1_grad [-0.01  0.  ] B2_grad [-0.03]\n",
            "Epoch 84 Loss 0.00043824 W1_grad [ 0.01 -0.03 -0.02 -0.    0.    0.  ] W2_grad [-0.03 -0.  ] B1_grad [-0.01  0.  ] B2_grad [-0.03]\n",
            "Epoch 85 Loss 0.0004026 W1_grad [ 0.01 -0.03 -0.02 -0.    0.    0.  ] W2_grad [-0.03 -0.  ] B1_grad [-0.01  0.  ] B2_grad [-0.03]\n",
            "Epoch 86 Loss 0.00036982 W1_grad [ 0.01 -0.03 -0.02 -0.    0.    0.  ] W2_grad [-0.03 -0.  ] B1_grad [-0.01  0.  ] B2_grad [-0.03]\n",
            "Epoch 87 Loss 0.00033967 W1_grad [ 0.01 -0.03 -0.02 -0.    0.    0.  ] W2_grad [-0.03 -0.  ] B1_grad [-0.01  0.  ] B2_grad [-0.03]\n",
            "Epoch 88 Loss 0.00031195 W1_grad [ 0.01 -0.02 -0.02 -0.    0.    0.  ] W2_grad [-0.03 -0.  ] B1_grad [-0.01  0.  ] B2_grad [-0.02]\n",
            "Epoch 89 Loss 0.00028646 W1_grad [ 0.01 -0.02 -0.02 -0.    0.    0.  ] W2_grad [-0.03 -0.  ] B1_grad [-0.01  0.  ] B2_grad [-0.02]\n",
            "Epoch 90 Loss 0.00026303 W1_grad [ 0.01 -0.02 -0.02 -0.    0.    0.  ] W2_grad [-0.03 -0.  ] B1_grad [-0.01  0.  ] B2_grad [-0.02]\n",
            "Epoch 91 Loss 0.0002415 W1_grad [ 0.01 -0.02 -0.02 -0.    0.    0.  ] W2_grad [-0.02 -0.  ] B1_grad [-0.01  0.  ] B2_grad [-0.02]\n",
            "Epoch 92 Loss 0.00022171 W1_grad [ 0.01 -0.02 -0.02 -0.    0.    0.  ] W2_grad [-0.02 -0.  ] B1_grad [-0.01  0.  ] B2_grad [-0.02]\n",
            "Epoch 93 Loss 0.00020353 W1_grad [ 0.01 -0.02 -0.01 -0.    0.    0.  ] W2_grad [-0.02 -0.  ] B1_grad [-0.01  0.  ] B2_grad [-0.02]\n",
            "Epoch 94 Loss 0.00018682 W1_grad [ 0.01 -0.02 -0.01 -0.    0.    0.  ] W2_grad [-0.02 -0.  ] B1_grad [-0.01  0.  ] B2_grad [-0.02]\n",
            "Epoch 95 Loss 0.00017147 W1_grad [ 0.01 -0.02 -0.01 -0.    0.    0.  ] W2_grad [-0.02 -0.  ] B1_grad [-0.01  0.  ] B2_grad [-0.02]\n",
            "Epoch 96 Loss 0.00015737 W1_grad [ 0.01 -0.02 -0.01 -0.    0.    0.  ] W2_grad [-0.02 -0.  ] B1_grad [-0.01  0.  ] B2_grad [-0.02]\n",
            "Epoch 97 Loss 0.00014442 W1_grad [ 0.01 -0.02 -0.01 -0.    0.    0.  ] W2_grad [-0.02 -0.  ] B1_grad [-0.01  0.  ] B2_grad [-0.02]\n",
            "Epoch 98 Loss 0.00013253 W1_grad [ 0.01 -0.02 -0.01 -0.    0.    0.  ] W2_grad [-0.02 -0.  ] B1_grad [-0.01  0.  ] B2_grad [-0.02]\n",
            "Epoch 99 Loss 0.00012161 W1_grad [ 0.01 -0.02 -0.01 -0.    0.    0.  ] W2_grad [-0.02 -0.  ] B1_grad [-0.01  0.  ] B2_grad [-0.02]\n",
            "Epoch 100 Loss 0.00011158 W1_grad [ 0.01 -0.01 -0.01 -0.    0.    0.  ] W2_grad [-0.02 -0.  ] B1_grad [-0.01  0.  ] B2_grad [-0.01]\n",
            "Epoch 101 Loss 0.00010238 W1_grad [ 0.01 -0.01 -0.01 -0.    0.    0.  ] W2_grad [-0.02 -0.  ] B1_grad [-0.01  0.  ] B2_grad [-0.01]\n",
            "Epoch 102 Loss 9.392e-05 W1_grad [ 0.01 -0.01 -0.01 -0.    0.    0.  ] W2_grad [-0.02 -0.  ] B1_grad [-0.01  0.  ] B2_grad [-0.01]\n",
            "Epoch 103 Loss 8.616e-05 W1_grad [ 0.01 -0.01 -0.01 -0.    0.    0.  ] W2_grad [-0.01 -0.  ] B1_grad [-0.01  0.  ] B2_grad [-0.01]\n",
            "Epoch 104 Loss 7.904e-05 W1_grad [ 0.01 -0.01 -0.01 -0.    0.    0.  ] W2_grad [-0.01 -0.  ] B1_grad [-0.01  0.  ] B2_grad [-0.01]\n",
            "Epoch 105 Loss 7.25e-05 W1_grad [ 0.01 -0.01 -0.01 -0.    0.    0.  ] W2_grad [-0.01 -0.  ] B1_grad [-0.01  0.  ] B2_grad [-0.01]\n",
            "Epoch 106 Loss 6.651e-05 W1_grad [ 0.01 -0.01 -0.01 -0.    0.    0.  ] W2_grad [-0.01 -0.  ] B1_grad [-0.01  0.  ] B2_grad [-0.01]\n",
            "Epoch 107 Loss 6.1e-05 W1_grad [ 0.01 -0.01 -0.01 -0.    0.    0.  ] W2_grad [-0.01 -0.  ] B1_grad [-0.01  0.  ] B2_grad [-0.01]\n",
            "Epoch 108 Loss 5.595e-05 W1_grad [ 0.01 -0.01 -0.01 -0.    0.    0.  ] W2_grad [-0.01 -0.  ] B1_grad [-0.01  0.  ] B2_grad [-0.01]\n",
            "Epoch 109 Loss 5.131e-05 W1_grad [ 0.   -0.01 -0.01 -0.    0.    0.  ] W2_grad [-0.01 -0.  ] B1_grad [-0.  0.] B2_grad [-0.01]\n",
            "Epoch 110 Loss 4.706e-05 W1_grad [ 0.   -0.01 -0.01 -0.    0.    0.  ] W2_grad [-0.01 -0.  ] B1_grad [-0.  0.] B2_grad [-0.01]\n",
            "Epoch 111 Loss 4.315e-05 W1_grad [ 0.   -0.01 -0.01 -0.    0.    0.  ] W2_grad [-0.01 -0.  ] B1_grad [-0.  0.] B2_grad [-0.01]\n",
            "Epoch 112 Loss 3.957e-05 W1_grad [ 0.   -0.01 -0.01 -0.    0.    0.  ] W2_grad [-0.01 -0.  ] B1_grad [-0.  0.] B2_grad [-0.01]\n",
            "Epoch 113 Loss 3.629e-05 W1_grad [ 0.   -0.01 -0.01 -0.    0.    0.  ] W2_grad [-0.01 -0.  ] B1_grad [-0.  0.] B2_grad [-0.01]\n",
            "Epoch 114 Loss 3.328e-05 W1_grad [ 0.   -0.01 -0.01 -0.    0.    0.  ] W2_grad [-0.01 -0.  ] B1_grad [-0.  0.] B2_grad [-0.01]\n",
            "Epoch 115 Loss 3.051e-05 W1_grad [ 0.   -0.01 -0.01 -0.    0.    0.  ] W2_grad [-0.01 -0.  ] B1_grad [-0.  0.] B2_grad [-0.01]\n",
            "Epoch 116 Loss 2.798e-05 W1_grad [ 0.   -0.01 -0.01 -0.    0.    0.  ] W2_grad [-0.01 -0.  ] B1_grad [-0.  0.] B2_grad [-0.01]\n",
            "Epoch 117 Loss 2.565e-05 W1_grad [ 0.   -0.01 -0.01 -0.    0.    0.  ] W2_grad [-0.01 -0.  ] B1_grad [-0.  0.] B2_grad [-0.01]\n",
            "Epoch 118 Loss 2.352e-05 W1_grad [ 0.   -0.01 -0.01 -0.    0.    0.  ] W2_grad [-0.01 -0.  ] B1_grad [-0.  0.] B2_grad [-0.01]\n",
            "Epoch 119 Loss 2.156e-05 W1_grad [ 0.   -0.01 -0.   -0.    0.    0.  ] W2_grad [-0.01 -0.  ] B1_grad [-0.  0.] B2_grad [-0.01]\n",
            "Epoch 120 Loss 1.977e-05 W1_grad [ 0.   -0.01 -0.   -0.    0.    0.  ] W2_grad [-0.01 -0.  ] B1_grad [-0.  0.] B2_grad [-0.01]\n",
            "Epoch 121 Loss 1.812e-05 W1_grad [ 0.   -0.01 -0.   -0.    0.    0.  ] W2_grad [-0.01 -0.  ] B1_grad [-0.  0.] B2_grad [-0.01]\n",
            "Epoch 122 Loss 1.662e-05 W1_grad [ 0.   -0.01 -0.   -0.    0.    0.  ] W2_grad [-0.01 -0.  ] B1_grad [-0.  0.] B2_grad [-0.01]\n",
            "Epoch 123 Loss 1.523e-05 W1_grad [ 0.   -0.01 -0.   -0.    0.    0.  ] W2_grad [-0.01 -0.  ] B1_grad [-0.  0.] B2_grad [-0.01]\n",
            "Epoch 124 Loss 1.396e-05 W1_grad [ 0.   -0.01 -0.   -0.    0.    0.  ] W2_grad [-0.01 -0.  ] B1_grad [-0.  0.] B2_grad [-0.01]\n",
            "Epoch 125 Loss 1.28e-05 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0.01 -0.  ] B1_grad [-0.  0.] B2_grad [-0.01]\n",
            "Epoch 126 Loss 1.173e-05 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0.01 -0.  ] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 127 Loss 1.076e-05 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0.01 -0.  ] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 128 Loss 9.86e-06 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0.01 -0.  ] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 129 Loss 9.04e-06 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 130 Loss 8.28e-06 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 131 Loss 7.59e-06 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 132 Loss 6.96e-06 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 133 Loss 6.38e-06 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 134 Loss 5.85e-06 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 135 Loss 5.36e-06 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 136 Loss 4.91e-06 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 137 Loss 4.5e-06 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 138 Loss 4.13e-06 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 139 Loss 3.78e-06 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 140 Loss 3.47e-06 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 141 Loss 3.18e-06 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 142 Loss 2.91e-06 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 143 Loss 2.67e-06 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 144 Loss 2.45e-06 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 145 Loss 2.24e-06 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 146 Loss 2.05e-06 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 147 Loss 1.88e-06 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 148 Loss 1.72e-06 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 149 Loss 1.58e-06 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 150 Loss 1.45e-06 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 151 Loss 1.33e-06 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 152 Loss 1.22e-06 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 153 Loss 1.11e-06 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 154 Loss 1.02e-06 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 155 Loss 9.4e-07 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 156 Loss 8.6e-07 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 157 Loss 7.9e-07 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 158 Loss 7.2e-07 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 159 Loss 6.6e-07 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 160 Loss 6e-07 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 161 Loss 5.5e-07 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 162 Loss 5.1e-07 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 163 Loss 4.7e-07 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 164 Loss 4.3e-07 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 165 Loss 3.9e-07 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 166 Loss 3.6e-07 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 167 Loss 3.3e-07 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 168 Loss 3e-07 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 169 Loss 2.8e-07 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 170 Loss 2.5e-07 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 171 Loss 2.3e-07 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 172 Loss 2.1e-07 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 173 Loss 1.9e-07 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 174 Loss 1.8e-07 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 175 Loss 1.6e-07 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 176 Loss 1.5e-07 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 177 Loss 1.4e-07 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 178 Loss 1.3e-07 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 179 Loss 1.2e-07 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 180 Loss 1.1e-07 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 181 Loss 1e-07 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 182 Loss 9e-08 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 183 Loss 8e-08 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 184 Loss 7e-08 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 185 Loss 7e-08 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 186 Loss 6e-08 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 187 Loss 6e-08 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 188 Loss 5e-08 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 189 Loss 5e-08 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 190 Loss 4e-08 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 191 Loss 4e-08 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 192 Loss 4e-08 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 193 Loss 3e-08 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 194 Loss 3e-08 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 195 Loss 3e-08 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 196 Loss 3e-08 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 197 Loss 2e-08 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 198 Loss 2e-08 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 199 Loss 2e-08 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 200 Loss 2e-08 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 201 Loss 2e-08 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 202 Loss 2e-08 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 203 Loss 1e-08 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 204 Loss 1e-08 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 205 Loss 1e-08 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 206 Loss 1e-08 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 207 Loss 1e-08 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 208 Loss 1e-08 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 209 Loss 1e-08 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 210 Loss 1e-08 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 211 Loss 1e-08 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 212 Loss 1e-08 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 213 Loss 1e-08 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 214 Loss 1e-08 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 215 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 216 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 217 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 218 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 219 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 220 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 221 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 222 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 223 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 224 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 225 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 226 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 227 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 228 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 229 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 230 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 231 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 232 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 233 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 234 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 235 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 236 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 237 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 238 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 239 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 240 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 241 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 242 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 243 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 244 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 245 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 246 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 247 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 248 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 249 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 250 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 251 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 252 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 253 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 254 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 255 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 256 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 257 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 258 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 259 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 260 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 261 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 262 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 263 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 264 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 265 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 266 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 267 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 268 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 269 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 270 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 271 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 272 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 273 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 274 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 275 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 276 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 277 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 278 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 279 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 280 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 281 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 282 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 283 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 284 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 285 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 286 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 287 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 288 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 289 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 290 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 291 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 292 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 293 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 294 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 295 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 296 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 297 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 298 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 299 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 300 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 301 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 302 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 303 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 304 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 305 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 306 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 307 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 308 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 309 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 310 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 311 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 312 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 313 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 314 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 315 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 316 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 317 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 318 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 319 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 320 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 321 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 322 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 323 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 324 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 325 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 326 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 327 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 328 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 329 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 330 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 331 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 332 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 333 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 334 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 335 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 336 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 337 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 338 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 339 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 340 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 341 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 342 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 343 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 344 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 345 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 346 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 347 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 348 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 349 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 350 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 351 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 352 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 353 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 354 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 355 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 356 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 357 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 358 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 359 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 360 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 361 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 362 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 363 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 364 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 365 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 366 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 367 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 368 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 369 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 370 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 371 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 372 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 373 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 374 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 375 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 376 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 377 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 378 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 379 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 380 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 381 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 382 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 383 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 384 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 385 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 386 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 387 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 388 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 389 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 390 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 391 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 392 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 393 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 394 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 395 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 396 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 397 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 398 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 399 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 400 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 401 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 402 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 403 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 404 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 405 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 406 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 407 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 408 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 409 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 410 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 411 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 412 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 413 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 414 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 415 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 416 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 417 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 418 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 419 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 420 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 421 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 422 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 423 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 424 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 425 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 426 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 427 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 428 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 429 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 430 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 431 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 432 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 433 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 434 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 435 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 436 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 437 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 438 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 439 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 440 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 441 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 442 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 443 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 444 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 445 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 446 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 447 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 448 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 449 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 450 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 451 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 452 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 453 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 454 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 455 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 456 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 457 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 458 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 459 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 460 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 461 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 462 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 463 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 464 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 465 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 466 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 467 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 468 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 469 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 470 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 471 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 472 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 473 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 474 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 475 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 476 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 477 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 478 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 479 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 480 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 481 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 482 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 483 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 484 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 485 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 486 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 487 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 488 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 489 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 490 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 491 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 492 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 493 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 494 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 495 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 496 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 497 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 498 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 499 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n",
            "Epoch 500 Loss 0.0 W1_grad [ 0. -0. -0. -0.  0.  0.] W2_grad [-0. -0.] B1_grad [-0.  0.] B2_grad [-0.]\n"
          ]
        }
      ],
      "source": [
        "X = np.array([-1.0, 2.0, 1.5])      # 输入向量 (3,)\n",
        "W1 = np.array([[0.1, -0.2, 0.4],    # 隐藏层权重 (2,3)\n",
        "        [-0.5, 0.3, 0.2]])\n",
        "\n",
        "B1 = np.array([0.0, 0.05])          # 隐藏层偏置 (2,)\n",
        "\n",
        "W2 = np.array([[0.3, -0.7]])        # 输出层权重 (1,2)\n",
        "B2 = np.array([0.1])                # 输出层偏置 (1,)\n",
        "\n",
        "y_true = np.array([1.0])            # 标签\n",
        "learning_rate = 0.01\n",
        "\n",
        "gradian_history = []\n",
        "epochs = 500\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  H,Z, y_pred = forward(X, W1, W2, B1, B2)\n",
        "  #print(f\"隐藏节点输出:{H}, 输出Y结果:{y_pred}\")\n",
        "\n",
        "  loss = compute_loss(y_pred, y_true)\n",
        "  #print(f\"求损失函数结果,loss:{loss}\")\n",
        "\n",
        "  W1_grad, W2_grad, B1_grad, B2_grad = backward(y_pred, y_true, X, H, Z)\n",
        "  #print(f\"求各个参数的梯度:W1梯度{W1_grad.flatten()}, W2梯度{W2_grad}, B1梯度{B1_grad}, B2梯度{B2_grad}\")\n",
        "\n",
        "  gradian_history.append({\n",
        "   \"Epoch\": epoch + 1,\n",
        "   \"W1_grad\": W1_grad.flatten(),\n",
        "   \"W2_grad\": W2_grad.flatten(),\n",
        "   \"B1_grad\": B1_grad.flatten(),\n",
        "   \"B2_grad\": B2_grad.flatten()\n",
        "  })\n",
        "\n",
        "  W1, W2, B1, B2 = update_param(learning_rate, W1, W2, B1, B2, W1_grad, W2_grad, B1_grad, B2_grad)\n",
        "  #print(f\"更新参数W1:{W1},W2:{W2},B1{B1},B2{B2}\")\n",
        "\n",
        "  print(\n",
        "    \"Epoch\", epoch + 1,\n",
        "    \"Loss\", round(float(loss), 8),\n",
        "    \"W1_grad\", np.round(W1_grad.flatten(), 2),\n",
        "    \"W2_grad\", np.round(W2_grad.flatten(), 2),\n",
        "    \"B1_grad\", np.round(B1_grad.flatten(), 2),\n",
        "    \"B2_grad\", np.round(B2_grad.flatten(), 2)\n",
        "  )\n",
        ""
      ]
    }
  ]
}