{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgtuGhG0fbk26J9WxHbO/f"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "hohjWweZp1_a"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#计算relu函数\n",
        "def relu(a):\n",
        "  a = np.maximum(0, a)\n",
        "  return a"
      ],
      "metadata": {
        "id": "B4K0XHh2p8D1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#计算前向传播输出y_pred\n",
        "def forward(X, W1, W2, B1, B2):\n",
        "  Z = np.dot(W1, X) + B1\n",
        "  H = relu(Z)\n",
        "\n",
        "  y_pred = np.dot(H, W2.flatten()) + B2\n",
        "  return H, Z, y_pred"
      ],
      "metadata": {
        "id": "sMLkLc12qWPj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#计算损失函数loss\n",
        "def compute_loss(y_pred, y_true):\n",
        "  loss = (y_pred - y_true) ** 2 / 2\n",
        "  return loss"
      ],
      "metadata": {
        "id": "BPvMKQOyrqKb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#反向传导计算梯度值\n",
        "def backward(y_pred, y_true, X, H, Z):\n",
        "  B2_grad = y_pred - y_true\n",
        "  W2_grad = np.outer(B2_grad, H)\n",
        "\n",
        "  B1_grad = B2_grad * W2.flatten() * (Z>0)\n",
        "  W1_grad = np.outer(B1_grad, X)\n",
        "\n",
        "  return W1_grad, W2_grad, B1_grad, B2_grad"
      ],
      "metadata": {
        "id": "TEzWCEUXs7VB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#更新参数weight, bias\n",
        "def update_param(learning_rate, W1, W2, B1, B2, W1_grad, W2_grad, B1_grad, B2_grad):\n",
        "  W1 -= learning_rate * W1_grad\n",
        "  W2 -= learning_rate * W2_grad\n",
        "\n",
        "  B1 -= learning_rate * B1_grad\n",
        "  B2 -= learning_rate * B2_grad\n",
        "\n",
        "  return W1, W2, B1, B2"
      ],
      "metadata": {
        "id": "D8v4rhzww5Qa"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "zVHk22D_p0Lu",
        "outputId": "a761176f-df12-4df8-bea9-581707d0b181"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unsupported format string passed to numpy.ndarray.__format__",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-f7803accd1f1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m   \u001b[0;31m#df = df.applymap(lambda x: np.array2string(x, precision=2) if isinstance(x, np.ndarray) else x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m   \u001b[0;31m# 格式化打印当前轮次的数据（单行显示）\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m   print(f\"Epoch {epoch + 1:2d}, \" \n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;34mf\"W1_grad: {w1_str}, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;34mf\"W2_grad: {w2_str}, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported format string passed to numpy.ndarray.__format__"
          ]
        }
      ],
      "source": [
        "X = np.array([-1.0, 2.0, 1.5])      # 输入向量 (3,)\n",
        "W1 = np.array([[0.1, -0.2, 0.4],    # 隐藏层权重 (2,3)\n",
        "        [-0.5, 0.3, 0.2]])\n",
        "\n",
        "B1 = np.array([0.0, 0.05])          # 隐藏层偏置 (2,)\n",
        "\n",
        "W2 = np.array([[0.3, -0.7]])        # 输出层权重 (1,2)\n",
        "B2 = np.array([0.1])                # 输出层偏置 (1,)\n",
        "\n",
        "y_true = np.array([1.0])            # 标签\n",
        "learning_rate = 0.01\n",
        "\n",
        "gradian_history = []\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  H,Z, y_pred = forward(X, W1, W2, B1, B2)\n",
        "  #print(f\"隐藏节点输出:{H}, 输出Y结果:{y_pred}\")\n",
        "\n",
        "  loss = compute_loss(y_pred, y_true)\n",
        "  #print(f\"求损失函数结果,loss:{loss}\")\n",
        "\n",
        "  W1_grad, W2_grad, B1_grad, B2_grad = backward(y_pred, y_true, X, H, Z)\n",
        "  #print(f\"求各个参数的梯度:W1梯度{W1_grad.flatten()}, W2梯度{W2_grad}, B1梯度{B1_grad}, B2梯度{B2_grad}\")\n",
        "\n",
        "  gradian_history.append({\n",
        "   \"Epoch\": epoch + 1,\n",
        "   \"W1_grad\": W1_grad.flatten(),\n",
        "   \"W2_grad\": W2_grad.flatten(),\n",
        "   \"B1_grad\": B1_grad.flatten(),\n",
        "   \"B2_grad\": B2_grad.flatten()\n",
        "  })\n",
        "\n",
        "  W1, W2, B1, B2 = update_param(learning_rate, W1, W2, B1, B2, W1_grad, W2_grad, B1_grad, B2_grad)\n",
        "  #print(f\"更新参数W1:{W1},W2:{W2},B1{B1},B2{B2}\")\n",
        "\n",
        "  w1_str = np.array2string(W1_grad.flatten(), precision=2, separator=' ')\n",
        "  w2_str = np.array2string(W2_grad.flatten(), precision=2, separator=' ')\n",
        "\n",
        "  b1_str = np.array2string(B1_grad.flatten(), precision=2, separator=' ')\n",
        "  b2_str = np.array2string(B2_grad.flatten(), precision=2, separator=' ')\n",
        "  #df = pd.DataFrame(gradian_history)\n",
        "  #df = df.applymap(lambda x: np.array2string(x, precision=2) if isinstance(x, np.ndarray) else x)\n",
        "  # 格式化打印当前轮次的数据（单行显示）\n",
        "  print(f\"Epoch {epoch + 1:2d}, \"\n",
        "        f\"W1_grad: {w1_str}, \"\n",
        "        f\"W2_grad: {w2_str}, \"\n",
        "        f\"B1_grad: {b1_str}, \"\n",
        "        f\"B2_grad: {b2_str}, \"\n",
        "        f\"Loss: {loss:.4f}\"\n",
        "    )\n",
        "\n",
        "  #pd.set_option(\"display.max_columns\", None)\n",
        "  #pd.set_option(\"display.width\", 200)\n",
        "  #print(df)\n"
      ]
    }
  ]
}