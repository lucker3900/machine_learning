{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNo7BnieTkZ8goZ1S7MK0Mj"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "hohjWweZp1_a"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#计算relu函数\n",
        "def relu(a):\n",
        "  a = np.maximum(0, a)\n",
        "  return a"
      ],
      "metadata": {
        "id": "B4K0XHh2p8D1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#计算前向传播输出y_pred\n",
        "def forward(X, W1, W2, B1, B2):\n",
        "  Z = np.dot(W1, X) + B1\n",
        "  H = relu(Z)\n",
        "\n",
        "  y_pred = np.dot(H, W2.flatten()) + B2\n",
        "  return H, Z, y_pred"
      ],
      "metadata": {
        "id": "sMLkLc12qWPj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#计算损失函数loss\n",
        "def compute_loss(y_pred, y_true):\n",
        "  loss = (y_pred - y_true) ** 2 / 2\n",
        "  return loss"
      ],
      "metadata": {
        "id": "BPvMKQOyrqKb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#反向传导计算梯度值\n",
        "def backward(y_pred, y_true, X, H, Z):\n",
        "  B2_grad = y_pred - y_true\n",
        "  W2_grad = np.outer(B2_grad, H)\n",
        "\n",
        "  B1_grad = B2_grad * W2.flatten() * (Z>0)\n",
        "  W1_grad = np.outer(B1_grad, X)\n",
        "\n",
        "  return W1_grad, W2_grad, B1_grad, B2_grad"
      ],
      "metadata": {
        "id": "TEzWCEUXs7VB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#更新参数weight, bias\n",
        "def update_param(learning_rate, W1, W2, B1, B2, W1_grad, W2_grad, B1_grad, B2_grad):\n",
        "  W1 -= learning_rate * W1_grad\n",
        "  W2 -= learning_rate * W2_grad\n",
        "\n",
        "  B1 -= learning_rate * B1_grad\n",
        "  B2 -= learning_rate * B2_grad\n",
        "\n",
        "  return W1, W2, B1, B2"
      ],
      "metadata": {
        "id": "D8v4rhzww5Qa"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVHk22D_p0Lu",
        "outputId": "cb7e80d7-92f9-452e-ecde-fc31005f1733"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "隐藏节点输出:[0.1  1.45], 输出Y结果:[-0.885]\n",
            "求损失函数结果,loss:[1.7766125]\n",
            "求各个参数的梯度:W1梯度[ 0.5655  -1.131   -0.84825 -1.3195   2.639    1.97925], W2梯度[[-0.1885  -2.73325]], B1梯度[-0.5655  1.3195], B2梯度[-1.885]\n",
            "更新参数W1:[[ 0.094345  -0.18869    0.4084825]\n",
            " [-0.486805   0.27361    0.1802075]],W2:[[ 0.301885  -0.6726675]],B1[0.005655 0.036805],B2[0.11885]\n",
            "隐藏节点输出:[0.14665375 1.34114125], 输出Y结果:[-0.73901956]\n",
            "求损失函数结果,loss:[1.51209452]\n",
            "求各个参数的梯度:W1梯度[ 0.52498392 -1.04996784 -0.78747588 -1.16978194  2.33956389  1.75467291], W2梯度[[-0.25503374 -2.33227087]], B1梯度[-0.52498392  1.16978194], B2梯度[-1.73901956]\n",
            "更新参数W1:[[ 0.08909516 -0.17819032  0.41635726]\n",
            " [-0.47510718  0.25021436  0.16266077]],W2:[[ 0.30443534 -0.64934479]],B1[0.01090484 0.02510718],B2[0.1362402]\n",
            "隐藏节点输出:[0.18996492 1.24463424], 输出Y结果:[-0.61412453]\n",
            "求损失函数结果,loss:[1.302699]\n",
            "求各个参数的梯度:W1梯度[ 0.49139655 -0.98279309 -0.73709482 -1.04812336  2.09624671  1.57218503], W2梯度[[-0.30662704 -2.00899466]], B1梯度[-0.49139655  1.04812336], B2梯度[-1.61412453]\n",
            "更新参数W1:[[ 0.0841812  -0.16836239  0.42372821]\n",
            " [-0.46462595  0.22925189  0.14693892]],W2:[[ 0.30750161 -0.62925484]],B1[0.0158188  0.01462595],B2[0.15238144]\n",
            "隐藏节点输出:[0.23050514 1.15816406], 输出Y结果:[-0.50551821]\n",
            "求损失函数结果,loss:[1.13329253]\n",
            "求各个参数的梯度:W1梯度[ 0.46294927 -0.92589854 -0.6944239  -0.94735462  1.89470925  1.42103194], W2梯度[[-0.34702968 -1.74363708]], B1梯度[-0.46294927  0.94735462], B2梯度[-1.50551821]\n",
            "更新参数W1:[[ 0.0795517  -0.15910341  0.43067245]\n",
            " [-0.4551524   0.2103048   0.1327286 ]],W2:[[ 0.3109719  -0.61181847]],B1[0.0204483 0.0051524],B2[0.16743662]\n",
            "隐藏节点输出:[0.26869845 1.08000731], 输出Y结果:[-0.40977413]\n",
            "求损失函数结果,loss:[0.99373155]\n",
            "求各个参数的梯度:W1梯度[ 0.43840015 -0.87680029 -0.65760022 -0.86252586  1.72505171  1.29378878], W2梯度[[-0.37880413 -1.52256636]], B1梯度[-0.43840015  0.86252586], B2梯度[-1.40977413]\n",
            "更新参数W1:[[ 0.0751677  -0.1503354   0.43724845]\n",
            " [-0.44652714  0.19305428  0.11979071]],W2:[[ 0.31475995 -0.59659281]],B1[ 0.0248323  -0.00347286],B2[0.18153436]\n",
            "隐藏节点输出:[0.30486647 1.00884892], 输出Y结果:[-0.3243779]\n",
            "求损失函数结果,loss:[0.87698841]\n",
            "求各个参数的梯度:W1梯度[ 0.41686112 -0.83372223 -0.62529167 -0.79011433  1.58022866  1.1851715 ], W2梯度[[-0.40375841 -1.33609722]], B1梯度[-0.41686112  0.79011433], B2梯度[-1.3243779]\n",
            "更新参数W1:[[ 0.07099909 -0.14199818  0.44350136]\n",
            " [-0.438626    0.177252    0.107939  ]],W2:[[ 0.31879753 -0.58323184]],B1[ 0.02900091 -0.011374  ],B2[0.19477814]\n",
            "隐藏节点输出:[0.33925751 0.94366449], 输出Y结果:[-0.24744258]\n",
            "求损失函数结果,loss:[0.77805649]\n",
            "求各个参数的梯度:W1梯度[ 0.39768161 -0.79536322 -0.59652242 -0.72754823  1.45509645  1.09132234], W2梯度[[-0.42320426 -1.17716726]], B1梯度[-0.39768161  0.72754823], B2梯度[-1.24744258]\n",
            "更新参数W1:[[ 0.06702227 -0.13404455  0.44946659]\n",
            " [-0.43135052  0.16270103  0.09702577]],W2:[[ 0.32302957 -0.57146017]],B1[ 0.03297773 -0.01864948],B2[0.20725257]\n",
            "隐藏节点输出:[0.37206624 0.88364176], 输出Y结果:[-0.1775251]\n",
            "求损失函数结果,loss:[0.69328268]\n",
            "求各个参数的梯度:W1梯度[ 0.38037543 -0.76075086 -0.57056314 -0.67290869  1.34581738  1.00936303], W2梯度[[-0.43811734 -1.04051035]], B1梯度[-0.38037543  0.67290869], B2梯度[-1.1775251]\n",
            "更新参数W1:[[ 0.06321852 -0.12643704  0.45517222]\n",
            " [-0.42462143  0.14924286  0.08693214]],W2:[[ 0.32741075 -0.56105506]],B1[ 0.03678148 -0.02537857],B2[0.21902782]\n",
            "隐藏节点输出:[0.40344721 0.8281268 ], 输出Y结果:[-0.11350396]\n",
            "求损失函数结果,loss:[0.61994553]\n",
            "求各个参数的梯度:W1梯度[ 0.36457316 -0.72914632 -0.54685974 -0.62473703  1.24947406  0.93710555], W2梯度[[-0.44924007 -0.92212246]], B1梯度[-0.36457316  0.62473703], B2梯度[-1.11350396]\n",
            "更新参数W1:[[ 0.05957279 -0.11914558  0.46064082]\n",
            " [-0.41837406  0.13674812  0.07756109]],W2:[[ 0.33190315 -0.55183384]],B1[ 0.04042721 -0.03162594],B2[0.23016286]\n",
            "隐藏节点输出:[0.4335245  0.77658599], 输出Y结果:[-0.05449542]\n",
            "求损失函数结果,loss:[0.5559803]\n",
            "求各个参数的梯度:W1梯度[ 0.34999035 -0.6999807  -0.52498552 -0.58190626  1.16381251  0.87285938], W2梯度[[-0.4571496  -0.81890637]], B1梯度[-0.34999035  0.58190626], B2梯度[-1.05449542]\n",
            "更新参数W1:[[ 0.05607288 -0.11214577  0.46589067]\n",
            " [-0.412555    0.12510999  0.0688325 ]],W2:[[ 0.33647464 -0.54364477]],B1[ 0.04392712 -0.037445  ],B2[0.24070781]\n"
          ]
        }
      ],
      "source": [
        "X = np.array([-1.0, 2.0, 1.5])      # 输入向量 (3,)\n",
        "W1 = np.array([[0.1, -0.2, 0.4],    # 隐藏层权重 (2,3)\n",
        "        [-0.5, 0.3, 0.2]])\n",
        "\n",
        "B1 = np.array([0.0, 0.05])          # 隐藏层偏置 (2,)\n",
        "\n",
        "W2 = np.array([[0.3, -0.7]])        # 输出层权重 (1,2)\n",
        "B2 = np.array([0.1])                # 输出层偏置 (1,)\n",
        "\n",
        "y_true = np.array([1.0])            # 标签\n",
        "learning_rate = 0.01\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  H,Z, y_pred = forward(X, W1, W2, B1, B2)\n",
        "  print(f\"隐藏节点输出:{H}, 输出Y结果:{y_pred}\")\n",
        "\n",
        "  loss = compute_loss(y_pred, y_true)\n",
        "  print(f\"求损失函数结果,loss:{loss}\")\n",
        "\n",
        "  W1_grad, W2_grad, B1_grad, B2_grad = backward(y_pred, y_true, X, H, Z)\n",
        "  print(f\"求各个参数的梯度:W1梯度{W1_grad.flatten()}, W2梯度{W2_grad}, B1梯度{B1_grad}, B2梯度{B2_grad}\")\n",
        "\n",
        "  W1, W2, B1, B2 = update_param(learning_rate, W1, W2, B1, B2, W1_grad, W2_grad, B1_grad, B2_grad)\n",
        "  print(f\"更新参数W1:{W1},W2:{W2},B1{B1},B2{B2}\")"
      ]
    }
  ]
}