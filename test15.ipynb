{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMV5CDGwVEx8KSQxR2mQZ8G"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eccFwRwFlocS"
      },
      "outputs": [],
      "source": [
        "#考虑一个单隐藏层的神经网络，结构如下：\n",
        "\n",
        "#输入层：2 个神经元。\n",
        "#隐藏层：2 个神经元，使用 Sigmoid 激活函数。\n",
        "#输出层：1 个神经元，使用线性激活函数（即无激活函数）。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "s7l8VllHozfr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sigmoid激活函数\n",
        "def sigmoid(a):\n",
        "  return 1 / 1 + np.exp(-a)"
      ],
      "metadata": {
        "id": "mzz0GkVzqtgV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#前向传播, 输出y_pred\n",
        "def forward(W1, W2, b1, b2, X):\n",
        "  Z = np.dot(X, W1) + b1 #ndarray (2,)\n",
        "  H = sigmoid(Z) #ndarray (2,)\n",
        "\n",
        "  y_pred = np.dot(H, W2.T) + b2\n",
        "\n",
        "  return Z, H, y_pred"
      ],
      "metadata": {
        "id": "ARLX6r7WrGY8"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#求损失函数loss值\n",
        "\n",
        "def compute_loss(y_pred, y_true):\n",
        "  loss = (y_pred - y_true)**2 / 2\n",
        "  return loss"
      ],
      "metadata": {
        "id": "9oRsWBGMtX9G"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#反向传播函数,求梯度变化\n",
        "def backward(y_pred, y_true, W2, H, X):\n",
        "  b2_grad = y_pred - y_true\n",
        "  W2_grad = b2_grad * H\n",
        "\n",
        "  bh = b2_grad * W2 # ndarray (2,)\n",
        "  bz = bh * H * (1-H) # ndarray (2,)\n",
        "  b1_grad = bz\n",
        "  W1_grad = np.outer(bz, X)\n",
        "\n",
        "  return W1_grad, W2_grad, b1_grad, b2_grad"
      ],
      "metadata": {
        "id": "edKtKfZCtv29"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#更新参数weight, bias\n",
        "def update_params(W1, W2, b1, b2, W1_grad, W2_grad, b1_grad, b2_grad, learning_rate):\n",
        "  W1 -= learning_rate * W1_grad\n",
        "  W2 -= learning_rate * W2_grad\n",
        "  b1 -= learning_rate * b1_grad\n",
        "  b2 -= learning_rate * b2_grad\n",
        "  return W1, W2, b1, b2"
      ],
      "metadata": {
        "id": "495vhhBvvSde"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#初始化\n",
        "X = np.array([0.5, 1.0]) # ndarray (2,)\n",
        "\n",
        "W1 = np.array([[0.1, 0.3],\n",
        "        [0.2, 0.4]]) # ndarray (2,2)\n",
        "\n",
        "b1 = np.array([0.1, -0.1]) # ndarray (2,)\n",
        "W2 = np.array([0.5, 0.7]) # ndarray (2,)\n",
        "\n",
        "b2 = 0.2\n",
        "\n",
        "y_true = 2.0\n",
        "learning_rate = 0.01\n",
        "\n",
        "epochs = 40\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  Z, H, y_pred = forward(W1, W2, b1, b2, X)\n",
        "  loss = compute_loss(y_pred, y_true)\n",
        "  W1_grad, W2_grad, b1_grad, b2_grad = backward(y_pred, y_true, W2, H, X)\n",
        "  W1, W2, b1, b2 = update_params(W1, W2, b1, b2, W1_grad, W2_grad, b1_grad, b2_grad, learning_rate)\n",
        "\n",
        "  print(\"Epoch:\", epoch + 1,\n",
        "    \"loss:\", np.round(np.mean(loss), 4),\n",
        "    \"W1_grad:\", np.round(W1_grad.flatten(), 2),\n",
        "    \"W2_grad:\", np.round(W2_grad.flatten(), 2),\n",
        "    \"b1_grad:\", np.round(b1_grad.flatten(), 2),\n",
        "    \"b2_grad:\", np.round(b2_grad.flatten(), 2),\n",
        "    \"w1:\", np.round(W1, 2),\n",
        "    \"w2:\", np.round(W2, 2),\n",
        "    \"b1:\", np.round(b1, 2),\n",
        "    \"b2:\", np.round(b2, 2),\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1nYLJfSqoX0",
        "outputId": "9443681e-cd04-4299-e5a1-b65906a67c75"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 loss: 0.0197 W1_grad: [-0.06 -0.12 -0.07 -0.15] W2_grad: [0.34 0.33] b1_grad: [-0.12 -0.15] b2_grad: [0.2] w1: [[0.1 0.3]\n",
            " [0.2 0.4]] w2: [0.5 0.7] b1: [ 0.1 -0.1] b2: 0.2\n",
            "Epoch: 2 loss: 0.0168 W1_grad: [-0.05 -0.11 -0.07 -0.13] W2_grad: [0.31 0.3 ] b1_grad: [-0.11 -0.13] b2_grad: [0.18] w1: [[0.1 0.3]\n",
            " [0.2 0.4]] w2: [0.49 0.69] b1: [ 0.1 -0.1] b2: 0.2\n",
            "Epoch: 3 loss: 0.0143 W1_grad: [-0.05 -0.1  -0.06 -0.12] W2_grad: [0.29 0.28] b1_grad: [-0.1  -0.12] b2_grad: [0.17] w1: [[0.1 0.3]\n",
            " [0.2 0.4]] w2: [0.49 0.69] b1: [ 0.1 -0.1] b2: 0.19\n",
            "Epoch: 4 loss: 0.0122 W1_grad: [-0.05 -0.09 -0.06 -0.11] W2_grad: [0.27 0.25] b1_grad: [-0.09 -0.11] b2_grad: [0.16] w1: [[0.1  0.3 ]\n",
            " [0.2  0.41]] w2: [0.49 0.69] b1: [ 0.1  -0.09] b2: 0.19\n",
            "Epoch: 5 loss: 0.0104 W1_grad: [-0.04 -0.08 -0.05 -0.1 ] W2_grad: [0.24 0.23] b1_grad: [-0.08 -0.1 ] b2_grad: [0.14] w1: [[0.1  0.31]\n",
            " [0.2  0.41]] w2: [0.49 0.69] b1: [ 0.11 -0.09] b2: 0.19\n",
            "Epoch: 6 loss: 0.0089 W1_grad: [-0.04 -0.08 -0.05 -0.09] W2_grad: [0.23 0.22] b1_grad: [-0.08 -0.09] b2_grad: [0.13] w1: [[0.1  0.31]\n",
            " [0.2  0.41]] w2: [0.48 0.68] b1: [ 0.11 -0.09] b2: 0.19\n",
            "Epoch: 7 loss: 0.0076 W1_grad: [-0.04 -0.07 -0.04 -0.09] W2_grad: [0.21 0.2 ] b1_grad: [-0.07 -0.09] b2_grad: [0.12] w1: [[0.1  0.31]\n",
            " [0.2  0.41]] w2: [0.48 0.68] b1: [ 0.11 -0.09] b2: 0.19\n",
            "Epoch: 8 loss: 0.0065 W1_grad: [-0.03 -0.06 -0.04 -0.08] W2_grad: [0.19 0.18] b1_grad: [-0.06 -0.08] b2_grad: [0.11] w1: [[0.1  0.31]\n",
            " [0.2  0.41]] w2: [0.48 0.68] b1: [ 0.11 -0.09] b2: 0.19\n",
            "Epoch: 9 loss: 0.0055 W1_grad: [-0.03 -0.06 -0.04 -0.07] W2_grad: [0.18 0.17] b1_grad: [-0.06 -0.07] b2_grad: [0.1] w1: [[0.1  0.31]\n",
            " [0.2  0.41]] w2: [0.48 0.68] b1: [ 0.11 -0.09] b2: 0.19\n",
            "Epoch: 10 loss: 0.0047 W1_grad: [-0.03 -0.05 -0.03 -0.07] W2_grad: [0.16 0.16] b1_grad: [-0.05 -0.07] b2_grad: [0.1] w1: [[0.1  0.31]\n",
            " [0.21 0.41]] w2: [0.48 0.68] b1: [ 0.11 -0.09] b2: 0.19\n",
            "Epoch: 11 loss: 0.004 W1_grad: [-0.03 -0.05 -0.03 -0.06] W2_grad: [0.15 0.15] b1_grad: [-0.05 -0.06] b2_grad: [0.09] w1: [[0.1  0.31]\n",
            " [0.21 0.41]] w2: [0.47 0.68] b1: [ 0.11 -0.09] b2: 0.18\n",
            "Epoch: 12 loss: 0.0034 W1_grad: [-0.02 -0.05 -0.03 -0.06] W2_grad: [0.14 0.13] b1_grad: [-0.05 -0.06] b2_grad: [0.08] w1: [[0.1  0.31]\n",
            " [0.21 0.41]] w2: [0.47 0.67] b1: [ 0.11 -0.09] b2: 0.18\n",
            "Epoch: 13 loss: 0.0029 W1_grad: [-0.02 -0.04 -0.03 -0.05] W2_grad: [0.13 0.12] b1_grad: [-0.04 -0.05] b2_grad: [0.08] w1: [[0.1  0.31]\n",
            " [0.21 0.41]] w2: [0.47 0.67] b1: [ 0.11 -0.09] b2: 0.18\n",
            "Epoch: 14 loss: 0.0025 W1_grad: [-0.02 -0.04 -0.02 -0.05] W2_grad: [0.12 0.11] b1_grad: [-0.04 -0.05] b2_grad: [0.07] w1: [[0.11 0.31]\n",
            " [0.21 0.41]] w2: [0.47 0.67] b1: [ 0.11 -0.09] b2: 0.18\n",
            "Epoch: 15 loss: 0.0022 W1_grad: [-0.02 -0.04 -0.02 -0.04] W2_grad: [0.11 0.11] b1_grad: [-0.04 -0.04] b2_grad: [0.07] w1: [[0.11 0.31]\n",
            " [0.21 0.41]] w2: [0.47 0.67] b1: [ 0.11 -0.09] b2: 0.18\n",
            "Epoch: 16 loss: 0.0018 W1_grad: [-0.02 -0.03 -0.02 -0.04] W2_grad: [0.1 0.1] b1_grad: [-0.03 -0.04] b2_grad: [0.06] w1: [[0.11 0.31]\n",
            " [0.21 0.41]] w2: [0.47 0.67] b1: [ 0.11 -0.09] b2: 0.18\n",
            "Epoch: 17 loss: 0.0016 W1_grad: [-0.02 -0.03 -0.02 -0.04] W2_grad: [0.09 0.09] b1_grad: [-0.03 -0.04] b2_grad: [0.06] w1: [[0.11 0.31]\n",
            " [0.21 0.41]] w2: [0.47 0.67] b1: [ 0.11 -0.09] b2: 0.18\n",
            "Epoch: 18 loss: 0.0014 W1_grad: [-0.01 -0.03 -0.02 -0.03] W2_grad: [0.09 0.08] b1_grad: [-0.03 -0.03] b2_grad: [0.05] w1: [[0.11 0.31]\n",
            " [0.21 0.41]] w2: [0.47 0.67] b1: [ 0.11 -0.09] b2: 0.18\n",
            "Epoch: 19 loss: 0.0012 W1_grad: [-0.01 -0.03 -0.02 -0.03] W2_grad: [0.08 0.08] b1_grad: [-0.03 -0.03] b2_grad: [0.05] w1: [[0.11 0.31]\n",
            " [0.21 0.41]] w2: [0.47 0.67] b1: [ 0.11 -0.09] b2: 0.18\n",
            "Epoch: 20 loss: 0.001 W1_grad: [-0.01 -0.02 -0.01 -0.03] W2_grad: [0.08 0.07] b1_grad: [-0.02 -0.03] b2_grad: [0.04] w1: [[0.11 0.31]\n",
            " [0.21 0.41]] w2: [0.46 0.67] b1: [ 0.11 -0.09] b2: 0.18\n",
            "Epoch: 21 loss: 0.0008 W1_grad: [-0.01 -0.02 -0.01 -0.03] W2_grad: [0.07 0.07] b1_grad: [-0.02 -0.03] b2_grad: [0.04] w1: [[0.11 0.31]\n",
            " [0.21 0.41]] w2: [0.46 0.67] b1: [ 0.11 -0.09] b2: 0.18\n",
            "Epoch: 22 loss: 0.0007 W1_grad: [-0.01 -0.02 -0.01 -0.03] W2_grad: [0.06 0.06] b1_grad: [-0.02 -0.03] b2_grad: [0.04] w1: [[0.11 0.31]\n",
            " [0.21 0.41]] w2: [0.46 0.67] b1: [ 0.11 -0.09] b2: 0.18\n",
            "Epoch: 23 loss: 0.0006 W1_grad: [-0.01 -0.02 -0.01 -0.02] W2_grad: [0.06 0.06] b1_grad: [-0.02 -0.02] b2_grad: [0.04] w1: [[0.11 0.31]\n",
            " [0.21 0.42]] w2: [0.46 0.66] b1: [ 0.11 -0.08] b2: 0.18\n",
            "Epoch: 24 loss: 0.0005 W1_grad: [-0.01 -0.02 -0.01 -0.02] W2_grad: [0.06 0.05] b1_grad: [-0.02 -0.02] b2_grad: [0.03] w1: [[0.11 0.31]\n",
            " [0.21 0.42]] w2: [0.46 0.66] b1: [ 0.11 -0.08] b2: 0.18\n",
            "Epoch: 25 loss: 0.0005 W1_grad: [-0.01 -0.02 -0.01 -0.02] W2_grad: [0.05 0.05] b1_grad: [-0.02 -0.02] b2_grad: [0.03] w1: [[0.11 0.31]\n",
            " [0.21 0.42]] w2: [0.46 0.66] b1: [ 0.11 -0.08] b2: 0.18\n",
            "Epoch: 26 loss: 0.0004 W1_grad: [-0.01 -0.02 -0.01 -0.02] W2_grad: [0.05 0.05] b1_grad: [-0.02 -0.02] b2_grad: [0.03] w1: [[0.11 0.31]\n",
            " [0.21 0.42]] w2: [0.46 0.66] b1: [ 0.11 -0.08] b2: 0.18\n",
            "Epoch: 27 loss: 0.0003 W1_grad: [-0.01 -0.01 -0.01 -0.02] W2_grad: [0.04 0.04] b1_grad: [-0.01 -0.02] b2_grad: [0.03] w1: [[0.11 0.31]\n",
            " [0.21 0.42]] w2: [0.46 0.66] b1: [ 0.11 -0.08] b2: 0.18\n",
            "Epoch: 28 loss: 0.0003 W1_grad: [-0.01 -0.01 -0.01 -0.02] W2_grad: [0.04 0.04] b1_grad: [-0.01 -0.02] b2_grad: [0.02] w1: [[0.11 0.31]\n",
            " [0.21 0.42]] w2: [0.46 0.66] b1: [ 0.11 -0.08] b2: 0.18\n",
            "Epoch: 29 loss: 0.0002 W1_grad: [-0.01 -0.01 -0.01 -0.01] W2_grad: [0.04 0.04] b1_grad: [-0.01 -0.01] b2_grad: [0.02] w1: [[0.11 0.31]\n",
            " [0.21 0.42]] w2: [0.46 0.66] b1: [ 0.11 -0.08] b2: 0.18\n",
            "Epoch: 30 loss: 0.0002 W1_grad: [-0.01 -0.01 -0.01 -0.01] W2_grad: [0.03 0.03] b1_grad: [-0.01 -0.01] b2_grad: [0.02] w1: [[0.11 0.31]\n",
            " [0.21 0.42]] w2: [0.46 0.66] b1: [ 0.11 -0.08] b2: 0.18\n",
            "Epoch: 31 loss: 0.0002 W1_grad: [-0.01 -0.01 -0.01 -0.01] W2_grad: [0.03 0.03] b1_grad: [-0.01 -0.01] b2_grad: [0.02] w1: [[0.11 0.31]\n",
            " [0.21 0.42]] w2: [0.46 0.66] b1: [ 0.11 -0.08] b2: 0.18\n",
            "Epoch: 32 loss: 0.0002 W1_grad: [-0.   -0.01 -0.01 -0.01] W2_grad: [0.03 0.03] b1_grad: [-0.01 -0.01] b2_grad: [0.02] w1: [[0.11 0.31]\n",
            " [0.21 0.42]] w2: [0.46 0.66] b1: [ 0.11 -0.08] b2: 0.18\n",
            "Epoch: 33 loss: 0.0001 W1_grad: [-0.   -0.01 -0.01 -0.01] W2_grad: [0.03 0.03] b1_grad: [-0.01 -0.01] b2_grad: [0.02] w1: [[0.11 0.31]\n",
            " [0.21 0.42]] w2: [0.46 0.66] b1: [ 0.11 -0.08] b2: 0.18\n",
            "Epoch: 34 loss: 0.0001 W1_grad: [-0.   -0.01 -0.   -0.01] W2_grad: [0.03 0.02] b1_grad: [-0.01 -0.01] b2_grad: [0.02] w1: [[0.11 0.31]\n",
            " [0.21 0.42]] w2: [0.46 0.66] b1: [ 0.11 -0.08] b2: 0.18\n",
            "Epoch: 35 loss: 0.0001 W1_grad: [-0.   -0.01 -0.   -0.01] W2_grad: [0.02 0.02] b1_grad: [-0.01 -0.01] b2_grad: [0.01] w1: [[0.11 0.31]\n",
            " [0.21 0.42]] w2: [0.46 0.66] b1: [ 0.11 -0.08] b2: 0.18\n",
            "Epoch: 36 loss: 0.0001 W1_grad: [-0.   -0.01 -0.   -0.01] W2_grad: [0.02 0.02] b1_grad: [-0.01 -0.01] b2_grad: [0.01] w1: [[0.11 0.31]\n",
            " [0.21 0.42]] w2: [0.46 0.66] b1: [ 0.11 -0.08] b2: 0.18\n",
            "Epoch: 37 loss: 0.0001 W1_grad: [-0.   -0.01 -0.   -0.01] W2_grad: [0.02 0.02] b1_grad: [-0.01 -0.01] b2_grad: [0.01] w1: [[0.11 0.31]\n",
            " [0.21 0.42]] w2: [0.46 0.66] b1: [ 0.11 -0.08] b2: 0.18\n",
            "Epoch: 38 loss: 0.0001 W1_grad: [-0.   -0.01 -0.   -0.01] W2_grad: [0.02 0.02] b1_grad: [-0.01 -0.01] b2_grad: [0.01] w1: [[0.11 0.31]\n",
            " [0.21 0.42]] w2: [0.46 0.66] b1: [ 0.11 -0.08] b2: 0.18\n",
            "Epoch: 39 loss: 0.0001 W1_grad: [-0.   -0.01 -0.   -0.01] W2_grad: [0.02 0.02] b1_grad: [-0.01 -0.01] b2_grad: [0.01] w1: [[0.11 0.31]\n",
            " [0.21 0.42]] w2: [0.46 0.66] b1: [ 0.11 -0.08] b2: 0.18\n",
            "Epoch: 40 loss: 0.0 W1_grad: [-0.   -0.01 -0.   -0.01] W2_grad: [0.02 0.02] b1_grad: [-0.01 -0.01] b2_grad: [0.01] w1: [[0.11 0.31]\n",
            " [0.21 0.42]] w2: [0.46 0.66] b1: [ 0.11 -0.08] b2: 0.17\n"
          ]
        }
      ]
    }
  ]
}