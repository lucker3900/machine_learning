{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1wsqHqSuVIyiVwCy41V/h"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "eccFwRwFlocS"
      },
      "outputs": [],
      "source": [
        "#考虑一个单隐藏层的神经网络，结构如下：\n",
        "\n",
        "#输入层：2 个神经元。\n",
        "#隐藏层：2 个神经元，使用 Sigmoid 激活函数。\n",
        "#输出层：1 个神经元，使用线性激活函数（即无激活函数）。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "s7l8VllHozfr"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sigmoid激活函数\n",
        "def sigmoid(a):\n",
        "  return 1 / (1 + np.exp(-a))"
      ],
      "metadata": {
        "id": "mzz0GkVzqtgV"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#前向传播, 输出y_pred\n",
        "def forward(W1, W2, b1, b2, X):\n",
        "  Z = np.dot(X, W1) + b1 #ndarray (2,)\n",
        "  H = sigmoid(Z) #ndarray (2,)\n",
        "\n",
        "  y_pred = np.dot(H, W2.T) + b2\n",
        "\n",
        "  return Z, H, y_pred"
      ],
      "metadata": {
        "id": "ARLX6r7WrGY8"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#求损失函数loss值\n",
        "\n",
        "def compute_loss(y_pred, y_true):\n",
        "  loss = (y_pred - y_true)**2 / 2\n",
        "  return loss"
      ],
      "metadata": {
        "id": "9oRsWBGMtX9G"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#反向传播函数,求梯度变化\n",
        "def backward(y_pred, y_true, W2, H, X):\n",
        "  b2_grad = y_pred - y_true\n",
        "  W2_grad = b2_grad * H\n",
        "\n",
        "  bh = b2_grad * W2 # ndarray (2,)\n",
        "  bz = bh * H * (1-H) # ndarray (2,)\n",
        "  b1_grad = bz\n",
        "  W1_grad = np.outer(bz, X)\n",
        "\n",
        "  return W1_grad, W2_grad, b1_grad, b2_grad"
      ],
      "metadata": {
        "id": "edKtKfZCtv29"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#更新参数weight, bias\n",
        "def update_params(W1, W2, b1, b2, W1_grad, W2_grad, b1_grad, b2_grad, learning_rate):\n",
        "  W1 -= learning_rate * W1_grad\n",
        "  W2 -= learning_rate * W2_grad\n",
        "  b1 -= learning_rate * b1_grad\n",
        "  b2 -= learning_rate * b2_grad\n",
        "  return W1, W2, b1, b2"
      ],
      "metadata": {
        "id": "495vhhBvvSde"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#初始化\n",
        "X = np.array([0.5, 1.0]) # ndarray (2,)\n",
        "\n",
        "W1 = np.array([[0.1, 0.3],\n",
        "        [0.2, 0.4]]) # ndarray (2,2)\n",
        "\n",
        "b1 = np.array([0.1, -0.1]) # ndarray (2,)\n",
        "W2 = np.array([0.5, 0.7]) # ndarray (2,)\n",
        "\n",
        "b2 = 0.2\n",
        "\n",
        "y_true = 2.0\n",
        "learning_rate = 0.01\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  Z, H, y_pred = forward(W1, W2, b1, b2, X)\n",
        "  loss = compute_loss(y_pred, y_true)\n",
        "  W1_grad, W2_grad, b1_grad, b2_grad = backward(y_pred, y_true, W2, H, X)\n",
        "  W1, W2, b1, b2 = update_params(W1, W2, b1, b2, W1_grad, W2_grad, b1_grad, b2_grad, learning_rate)\n",
        "\n",
        "  if epoch%10 ==0:\n",
        "    print(\"Epoch:\", epoch + 1,\n",
        "      \"loss:\", np.round(np.mean(loss), 4),\n",
        "      \"W1_grad:\", np.round(W1_grad.flatten(), 2),\n",
        "      \"W2_grad:\", np.round(W2_grad.flatten(), 2),\n",
        "      \"b1_grad:\", np.round(b1_grad.flatten(), 2),\n",
        "      \"b2_grad:\", np.round(b2_grad.flatten(), 2),\n",
        "      \"w1:\", np.round(W1, 2),\n",
        "      \"w2:\", np.round(W2, 2),\n",
        "      \"b1:\", np.round(b1, 2),\n",
        "      \"b2:\", np.round(b2, 2),\n",
        "      )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1nYLJfSqoX0",
        "outputId": "36bae56b-e2f5-4e82-d33e-92fb3f474d37"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 loss: 0.5824 W1_grad: [-0.07 -0.13 -0.09 -0.18] W2_grad: [-0.63 -0.66] b1_grad: [-0.13 -0.18] b2_grad: [-1.08] w1: [[0.1 0.3]\n",
            " [0.2 0.4]] w2: [0.51 0.71] b1: [ 0.1 -0.1] b2: 0.21\n",
            "Epoch: 11 loss: 0.4027 W1_grad: [-0.06 -0.12 -0.08 -0.16] W2_grad: [-0.53 -0.56] b1_grad: [-0.12 -0.16] b2_grad: [-0.9] w1: [[0.11 0.31]\n",
            " [0.21 0.42]] w2: [0.56 0.77] b1: [ 0.11 -0.08] b2: 0.31\n",
            "Epoch: 21 loss: 0.2765 W1_grad: [-0.05 -0.11 -0.07 -0.14] W2_grad: [-0.44 -0.47] b1_grad: [-0.11 -0.14] b2_grad: [-0.74] w1: [[0.11 0.33]\n",
            " [0.22 0.43]] w2: [0.61 0.82] b1: [ 0.13 -0.07] b2: 0.39\n",
            "Epoch: 31 loss: 0.1886 W1_grad: [-0.05 -0.1  -0.06 -0.12] W2_grad: [-0.37 -0.39] b1_grad: [-0.1  -0.12] b2_grad: [-0.61] w1: [[0.12 0.34]\n",
            " [0.22 0.45]] w2: [0.65 0.86] b1: [ 0.14 -0.05] b2: 0.46\n",
            "Epoch: 41 loss: 0.128 W1_grad: [-0.04 -0.08 -0.05 -0.1 ] W2_grad: [-0.31 -0.33] b1_grad: [-0.08 -0.1 ] b2_grad: [-0.51] w1: [[0.12 0.34]\n",
            " [0.23 0.46]] w2: [0.69 0.9 ] b1: [ 0.14 -0.04] b2: 0.51\n",
            "Epoch: 51 loss: 0.0865 W1_grad: [-0.04 -0.07 -0.04 -0.09] W2_grad: [-0.25 -0.27] b1_grad: [-0.07 -0.09] b2_grad: [-0.42] w1: [[0.13 0.35]\n",
            " [0.23 0.47]] w2: [0.71 0.92] b1: [ 0.15 -0.03] b2: 0.56\n",
            "Epoch: 61 loss: 0.0582 W1_grad: [-0.03 -0.06 -0.04 -0.07] W2_grad: [-0.21 -0.22] b1_grad: [-0.06 -0.07] b2_grad: [-0.34] w1: [[0.13 0.36]\n",
            " [0.24 0.48]] w2: [0.74 0.95] b1: [ 0.16 -0.02] b2: 0.59\n",
            "Epoch: 71 loss: 0.0391 W1_grad: [-0.02 -0.05 -0.03 -0.06] W2_grad: [-0.17 -0.18] b1_grad: [-0.05 -0.06] b2_grad: [-0.28] w1: [[0.13 0.36]\n",
            " [0.24 0.48]] w2: [0.76 0.97] b1: [ 0.16 -0.02] b2: 0.63\n",
            "Epoch: 81 loss: 0.0261 W1_grad: [-0.02 -0.04 -0.03 -0.05] W2_grad: [-0.14 -0.15] b1_grad: [-0.04 -0.05] b2_grad: [-0.23] w1: [[0.13 0.37]\n",
            " [0.24 0.49]] w2: [0.77 0.99] b1: [ 0.17 -0.01] b2: 0.65\n",
            "Epoch: 91 loss: 0.0175 W1_grad: [-0.02 -0.03 -0.02 -0.04] W2_grad: [-0.12 -0.12] b1_grad: [-0.03 -0.04] b2_grad: [-0.19] w1: [[0.14 0.37]\n",
            " [0.25 0.49]] w2: [0.78 1.  ] b1: [ 0.17 -0.01] b2: 0.67\n"
          ]
        }
      ]
    }
  ]
}