{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMHL3EpSWG+LD66Vq+VXhWH"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "KFmzouHABFYp"
      },
      "outputs": [],
      "source": [
        "#考虑一个简单的神经网络，用于回归任务。该网络结构如下：\n",
        "\n",
        "#输入层： 1 个神经元\n",
        "#隐藏层： 2 个神经元，使用 Sigmoid 激活函数\n",
        "#输出层： 1 个神经元，线性输出"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "hnXerP55CIgl"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#计算sigmoid激活函数\n",
        "def sigmoid(a):\n",
        "  return 1 / (1 + np.exp(-a))"
      ],
      "metadata": {
        "id": "k3pk27pyCQt1"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#前向传播,求出y_pred的值\n",
        "def forword(W1, W2, b1, b2, X):\n",
        "  Z = np.dot(X, W1.T) + b1.T # shape (2,2)\n",
        "  H = sigmoid(Z) # shape (2,2)\n",
        "\n",
        "  y_pred = np.dot(H, W2.T) + b2\n",
        "\n",
        "  return H, y_pred"
      ],
      "metadata": {
        "id": "eibN_EYZCgvm"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#求损失函数loss值,\n",
        "def compute_loss(y_true, y_pred):\n",
        "  loss = (y_true - y_pred)**2 / 2\n",
        "  return loss"
      ],
      "metadata": {
        "id": "Zncge0t1DtzL"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#求反向传播，梯度计算\n",
        "def backward(y_pred, y_true, H, W2, X):\n",
        "  y_diff = y_pred - y_true # shape(2,1)\n",
        "\n",
        "  b2_grad = np.mean(y_diff).T # shape(1,1)\n",
        "  W2_grad = np.dot(y_diff.T, H)\n",
        "\n",
        "  dh = np.dot(y_diff, W2) # shape(2,2)\n",
        "  dz = dh * (1 - H**2) # shape(2,2)\n",
        "  b1_grad = np.mean(dh, axis=0, keepdims=True).T # shape(2,1)\n",
        "  W1_grad = np.dot(dz, X) # shape(2,1)\n",
        "\n",
        "  return W1_grad, W2_grad, b1_grad, b2_grad"
      ],
      "metadata": {
        "id": "iQMSnWS9EUTf"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#更新参数weight, bias\n",
        "def update_params(W1, W2, b1, b2, W1_grad, W2_grad, b1_grad, b2_grad, learning_rate):\n",
        "  W1 -= learning_rate * W1_grad\n",
        "  W2 -= learning_rate * W2_grad\n",
        "  b1 -= learning_rate * b1_grad\n",
        "  b2 -= learning_rate * b2_grad\n",
        "\n",
        "  return W1, W2, b1, b2"
      ],
      "metadata": {
        "id": "yqWkrcqeH4OB"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([[0.5],\n",
        "        [1.0]]) # ndarray (2,1)\n",
        "\n",
        "y_true = np.array([[0.75],\n",
        "        [1.5]]) # ndarray (2,1)\n",
        "\n",
        "W1 = np.array([[0.2],\n",
        "        [0.3]]) # ndarray (2,1)\n",
        "\n",
        "b1 = np.array([[0.1],\n",
        "        [0.2]]) # ndarray (2,1)\n",
        "\n",
        "W2 = np.array([[0.4, 0.5]]) # ndarray (1,2)\n",
        "b2 = np.array([[0.6]]) # ndarray (1,1)\n",
        "\n",
        "learning_rate = 0.01\n",
        "\n",
        "epochs = 10000\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  H, y_pred = forword(W1, W2, b1, b2, X)\n",
        "  loss = compute_loss(y_true, y_pred)\n",
        "  W1_grad, W2_grad, b1_grad, b2_grad = backward(y_pred, y_true, H, W2, X)\n",
        "  W1, W2, b1, b2 = update_params(W1, W2, b1, b2, W1_grad, W2_grad, b1_grad, b2_grad, learning_rate)\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    print(\"Epoch:\", epoch + 1,\n",
        "      \"loss:\", np.round(np.mean(loss), 4),\n",
        "      \"W1_grad:\", np.round(W1_grad.flatten(), 2),\n",
        "      \"W2_grad:\", np.round(W2_grad.flatten(), 2),\n",
        "      \"b1_grad:\", np.round(b1_grad.flatten(), 2),\n",
        "      \"b2_grad:\", np.round(b2_grad.flatten(), 2)\n",
        "      )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNRObL9JBTBk",
        "outputId": "2d1a1b09-0bd9-4b55-b171-4ce54d0c837f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 loss: 0.0652 W1_grad: [ 0.17 -0.16] W2_grad: [-0.01 -0.01] b1_grad: [0. 0.] b2_grad: [0.]\n",
            "Epoch: 101 loss: 0.0651 W1_grad: [ 0.17 -0.15] W2_grad: [ 0.   -0.01] b1_grad: [0. 0.] b2_grad: [0.01]\n",
            "Epoch: 201 loss: 0.065 W1_grad: [ 0.17 -0.15] W2_grad: [ 0.01 -0.02] b1_grad: [0. 0.] b2_grad: [0.01]\n",
            "Epoch: 301 loss: 0.0648 W1_grad: [ 0.17 -0.15] W2_grad: [ 0.02 -0.02] b1_grad: [0. 0.] b2_grad: [0.01]\n",
            "Epoch: 401 loss: 0.0644 W1_grad: [ 0.17 -0.14] W2_grad: [ 0.03 -0.03] b1_grad: [0. 0.] b2_grad: [0.01]\n",
            "Epoch: 501 loss: 0.0637 W1_grad: [ 0.17 -0.13] W2_grad: [ 0.03 -0.03] b1_grad: [0. 0.] b2_grad: [0.01]\n",
            "Epoch: 601 loss: 0.0627 W1_grad: [ 0.16 -0.12] W2_grad: [ 0.04 -0.03] b1_grad: [0. 0.] b2_grad: [0.01]\n",
            "Epoch: 701 loss: 0.0613 W1_grad: [ 0.16 -0.11] W2_grad: [ 0.04 -0.03] b1_grad: [0.   0.01] b2_grad: [0.01]\n",
            "Epoch: 801 loss: 0.0597 W1_grad: [ 0.15 -0.1 ] W2_grad: [ 0.05 -0.03] b1_grad: [0.   0.01] b2_grad: [0.01]\n",
            "Epoch: 901 loss: 0.0578 W1_grad: [ 0.14 -0.09] W2_grad: [ 0.05 -0.03] b1_grad: [0.   0.01] b2_grad: [0.01]\n",
            "Epoch: 1001 loss: 0.0559 W1_grad: [ 0.14 -0.08] W2_grad: [ 0.05 -0.03] b1_grad: [0.   0.01] b2_grad: [0.01]\n",
            "Epoch: 1101 loss: 0.0538 W1_grad: [ 0.13 -0.07] W2_grad: [ 0.05 -0.03] b1_grad: [0.   0.01] b2_grad: [0.01]\n",
            "Epoch: 1201 loss: 0.0517 W1_grad: [ 0.12 -0.06] W2_grad: [ 0.05 -0.03] b1_grad: [-0.    0.01] b2_grad: [0.01]\n",
            "Epoch: 1301 loss: 0.0496 W1_grad: [ 0.11 -0.05] W2_grad: [ 0.05 -0.02] b1_grad: [-0.    0.01] b2_grad: [0.01]\n",
            "Epoch: 1401 loss: 0.0476 W1_grad: [ 0.11 -0.04] W2_grad: [ 0.05 -0.02] b1_grad: [-0.    0.01] b2_grad: [0.01]\n",
            "Epoch: 1501 loss: 0.0457 W1_grad: [ 0.1  -0.03] W2_grad: [ 0.05 -0.02] b1_grad: [-0.    0.01] b2_grad: [0.01]\n",
            "Epoch: 1601 loss: 0.0438 W1_grad: [ 0.09 -0.03] W2_grad: [ 0.05 -0.02] b1_grad: [-0.    0.01] b2_grad: [0.01]\n",
            "Epoch: 1701 loss: 0.042 W1_grad: [ 0.09 -0.02] W2_grad: [ 0.05 -0.02] b1_grad: [-0.    0.01] b2_grad: [0.01]\n",
            "Epoch: 1801 loss: 0.0403 W1_grad: [ 0.08 -0.01] W2_grad: [ 0.05 -0.02] b1_grad: [-0.    0.01] b2_grad: [0.01]\n",
            "Epoch: 1901 loss: 0.0386 W1_grad: [ 0.07 -0.01] W2_grad: [ 0.05 -0.02] b1_grad: [-0.    0.01] b2_grad: [0.01]\n",
            "Epoch: 2001 loss: 0.0371 W1_grad: [ 0.07 -0.  ] W2_grad: [ 0.05 -0.02] b1_grad: [-0.    0.01] b2_grad: [0.01]\n",
            "Epoch: 2101 loss: 0.0356 W1_grad: [0.07 0.  ] W2_grad: [ 0.05 -0.02] b1_grad: [-0.    0.01] b2_grad: [0.01]\n",
            "Epoch: 2201 loss: 0.0341 W1_grad: [0.06 0.01] W2_grad: [ 0.04 -0.02] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 2301 loss: 0.0328 W1_grad: [0.06 0.01] W2_grad: [ 0.04 -0.02] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 2401 loss: 0.0314 W1_grad: [0.06 0.01] W2_grad: [ 0.04 -0.02] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 2501 loss: 0.0302 W1_grad: [0.05 0.01] W2_grad: [ 0.04 -0.02] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 2601 loss: 0.029 W1_grad: [0.05 0.02] W2_grad: [ 0.04 -0.02] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 2701 loss: 0.0278 W1_grad: [0.05 0.02] W2_grad: [ 0.04 -0.02] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 2801 loss: 0.0268 W1_grad: [0.05 0.02] W2_grad: [ 0.04 -0.02] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 2901 loss: 0.0257 W1_grad: [0.05 0.02] W2_grad: [ 0.04 -0.02] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 3001 loss: 0.0247 W1_grad: [0.05 0.02] W2_grad: [ 0.04 -0.02] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 3101 loss: 0.0238 W1_grad: [0.05 0.02] W2_grad: [ 0.04 -0.02] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 3201 loss: 0.0229 W1_grad: [0.04 0.02] W2_grad: [ 0.03 -0.02] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 3301 loss: 0.022 W1_grad: [0.04 0.02] W2_grad: [ 0.03 -0.02] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 3401 loss: 0.0212 W1_grad: [0.04 0.02] W2_grad: [ 0.03 -0.02] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 3501 loss: 0.0205 W1_grad: [0.04 0.02] W2_grad: [ 0.03 -0.02] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 3601 loss: 0.0197 W1_grad: [0.04 0.02] W2_grad: [ 0.03 -0.02] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 3701 loss: 0.0191 W1_grad: [0.04 0.02] W2_grad: [ 0.03 -0.02] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 3801 loss: 0.0184 W1_grad: [0.04 0.02] W2_grad: [ 0.03 -0.02] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 3901 loss: 0.0178 W1_grad: [0.04 0.02] W2_grad: [ 0.03 -0.02] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 4001 loss: 0.0172 W1_grad: [0.04 0.02] W2_grad: [ 0.03 -0.02] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 4101 loss: 0.0167 W1_grad: [0.04 0.02] W2_grad: [ 0.03 -0.02] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 4201 loss: 0.0161 W1_grad: [0.04 0.02] W2_grad: [ 0.03 -0.02] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 4301 loss: 0.0157 W1_grad: [0.04 0.02] W2_grad: [ 0.03 -0.02] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 4401 loss: 0.0152 W1_grad: [0.04 0.02] W2_grad: [ 0.03 -0.02] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 4501 loss: 0.0148 W1_grad: [0.04 0.02] W2_grad: [ 0.03 -0.02] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 4601 loss: 0.0144 W1_grad: [0.04 0.02] W2_grad: [ 0.03 -0.02] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 4701 loss: 0.014 W1_grad: [0.04 0.01] W2_grad: [ 0.02 -0.02] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 4801 loss: 0.0136 W1_grad: [0.04 0.01] W2_grad: [ 0.02 -0.02] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 4901 loss: 0.0132 W1_grad: [0.04 0.01] W2_grad: [ 0.02 -0.02] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 5001 loss: 0.0129 W1_grad: [0.04 0.01] W2_grad: [ 0.02 -0.02] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 5101 loss: 0.0126 W1_grad: [0.04 0.01] W2_grad: [ 0.02 -0.02] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 5201 loss: 0.0123 W1_grad: [0.04 0.01] W2_grad: [ 0.02 -0.02] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 5301 loss: 0.012 W1_grad: [0.05 0.01] W2_grad: [ 0.02 -0.02] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 5401 loss: 0.0117 W1_grad: [0.05 0.01] W2_grad: [ 0.02 -0.02] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 5501 loss: 0.0115 W1_grad: [0.05 0.01] W2_grad: [ 0.02 -0.02] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 5601 loss: 0.0112 W1_grad: [0.05 0.01] W2_grad: [ 0.02 -0.02] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 5701 loss: 0.011 W1_grad: [0.05 0.01] W2_grad: [ 0.02 -0.01] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 5801 loss: 0.0108 W1_grad: [0.05 0.01] W2_grad: [ 0.02 -0.01] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 5901 loss: 0.0105 W1_grad: [0.05 0.01] W2_grad: [ 0.02 -0.01] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 6001 loss: 0.0103 W1_grad: [0.05 0.01] W2_grad: [ 0.02 -0.01] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 6101 loss: 0.0101 W1_grad: [0.05 0.  ] W2_grad: [ 0.02 -0.01] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 6201 loss: 0.0099 W1_grad: [0.05 0.  ] W2_grad: [ 0.02 -0.01] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 6301 loss: 0.0097 W1_grad: [0.05 0.  ] W2_grad: [ 0.02 -0.01] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 6401 loss: 0.0096 W1_grad: [0.05 0.  ] W2_grad: [ 0.02 -0.01] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 6501 loss: 0.0094 W1_grad: [0.05 0.  ] W2_grad: [ 0.02 -0.01] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 6601 loss: 0.0092 W1_grad: [0.05 0.  ] W2_grad: [ 0.02 -0.01] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 6701 loss: 0.009 W1_grad: [0.05 0.  ] W2_grad: [ 0.02 -0.01] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 6801 loss: 0.0089 W1_grad: [0.05 0.  ] W2_grad: [ 0.02 -0.01] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 6901 loss: 0.0087 W1_grad: [0.05 0.  ] W2_grad: [ 0.02 -0.01] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 7001 loss: 0.0086 W1_grad: [0.05 0.  ] W2_grad: [ 0.02 -0.01] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 7101 loss: 0.0084 W1_grad: [ 0.05 -0.  ] W2_grad: [ 0.02 -0.01] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 7201 loss: 0.0083 W1_grad: [ 0.05 -0.  ] W2_grad: [ 0.02 -0.01] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 7301 loss: 0.0081 W1_grad: [ 0.05 -0.  ] W2_grad: [ 0.01 -0.01] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 7401 loss: 0.008 W1_grad: [ 0.05 -0.  ] W2_grad: [ 0.01 -0.01] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 7501 loss: 0.0078 W1_grad: [ 0.05 -0.  ] W2_grad: [ 0.01 -0.01] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 7601 loss: 0.0077 W1_grad: [ 0.05 -0.  ] W2_grad: [ 0.01 -0.01] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 7701 loss: 0.0075 W1_grad: [ 0.05 -0.  ] W2_grad: [ 0.01 -0.01] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 7801 loss: 0.0074 W1_grad: [ 0.05 -0.  ] W2_grad: [ 0.01 -0.01] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 7901 loss: 0.0073 W1_grad: [ 0.05 -0.  ] W2_grad: [ 0.01 -0.01] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 8001 loss: 0.0071 W1_grad: [ 0.05 -0.  ] W2_grad: [ 0.01 -0.01] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 8101 loss: 0.007 W1_grad: [ 0.05 -0.  ] W2_grad: [ 0.01 -0.01] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 8201 loss: 0.0069 W1_grad: [ 0.05 -0.  ] W2_grad: [ 0.01 -0.01] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 8301 loss: 0.0067 W1_grad: [ 0.06 -0.01] W2_grad: [ 0.01 -0.01] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 8401 loss: 0.0066 W1_grad: [ 0.06 -0.01] W2_grad: [ 0.01 -0.01] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 8501 loss: 0.0065 W1_grad: [ 0.06 -0.01] W2_grad: [ 0.01 -0.01] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 8601 loss: 0.0063 W1_grad: [ 0.06 -0.01] W2_grad: [ 0.01 -0.01] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 8701 loss: 0.0062 W1_grad: [ 0.06 -0.01] W2_grad: [ 0.01 -0.01] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 8801 loss: 0.0061 W1_grad: [ 0.06 -0.01] W2_grad: [ 0.01 -0.01] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 8901 loss: 0.0059 W1_grad: [ 0.06 -0.01] W2_grad: [ 0.01 -0.01] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 9001 loss: 0.0058 W1_grad: [ 0.06 -0.01] W2_grad: [ 0.01 -0.01] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 9101 loss: 0.0057 W1_grad: [ 0.06 -0.01] W2_grad: [ 0.01 -0.01] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 9201 loss: 0.0055 W1_grad: [ 0.06 -0.01] W2_grad: [ 0.01 -0.01] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 9301 loss: 0.0054 W1_grad: [ 0.06 -0.01] W2_grad: [ 0.01 -0.01] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 9401 loss: 0.0053 W1_grad: [ 0.06 -0.01] W2_grad: [ 0.01 -0.01] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 9501 loss: 0.0051 W1_grad: [ 0.06 -0.01] W2_grad: [ 0.01 -0.01] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 9601 loss: 0.005 W1_grad: [ 0.06 -0.01] W2_grad: [ 0.01 -0.01] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 9701 loss: 0.0049 W1_grad: [ 0.06 -0.01] W2_grad: [ 0.01 -0.01] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 9801 loss: 0.0047 W1_grad: [ 0.06 -0.01] W2_grad: [ 0.01 -0.01] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n",
            "Epoch: 9901 loss: 0.0046 W1_grad: [ 0.06 -0.01] W2_grad: [ 0.01 -0.01] b1_grad: [-0.01  0.01] b2_grad: [0.01]\n"
          ]
        }
      ]
    }
  ]
}